2019-02-01 11:43:42,461 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu-HP-Pavilion-dv6-NoteBook-PC/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /opt/hadoop/hadoop-2.7.3/etc/hadoop:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_171
************************************************************/
2019-02-01 11:43:42,492 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-01 11:43:42,511 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-02-01 11:43:43,904 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-01 11:43:44,261 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-02-01 11:43:44,262 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-02-01 11:43:44,296 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2019-02-01 11:43:44,299 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2019-02-01 11:43:45,954 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2019-02-01 11:43:46,561 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-02-01 11:43:46,592 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-01 11:43:46,639 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-02-01 11:43:46,683 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-01 11:43:46,713 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-02-01 11:43:46,713 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-01 11:43:46,714 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-01 11:43:47,748 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-02-01 11:43:47,825 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-02-01 11:43:47,896 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2019-02-01 11:43:47,896 INFO org.mortbay.log: jetty-6.1.26
2019-02-01 11:43:49,044 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2019-02-01 11:43:49,268 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-01 11:43:49,268 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-01 11:43:49,429 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-02-01 11:43:49,430 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-02-01 11:43:49,885 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-02-01 11:43:49,886 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-02-01 11:43:49,890 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-02-01 11:43:49,891 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Feb 01 11:43:49
2019-02-01 11:43:49,897 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-02-01 11:43:49,898 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-01 11:43:49,911 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2019-02-01 11:43:49,911 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-02-01 11:43:49,936 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-02-01 11:43:49,937 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2019-02-01 11:43:49,937 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-02-01 11:43:49,937 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-02-01 11:43:49,937 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-02-01 11:43:49,938 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-02-01 11:43:49,938 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-02-01 11:43:49,938 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-02-01 11:43:49,959 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-02-01 11:43:49,959 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-02-01 11:43:49,959 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2019-02-01 11:43:49,960 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-02-01 11:43:49,969 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-02-01 11:43:50,252 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-02-01 11:43:50,253 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-01 11:43:50,253 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2019-02-01 11:43:50,253 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-02-01 11:43:50,255 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-02-01 11:43:50,255 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-02-01 11:43:50,256 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-02-01 11:43:50,256 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-02-01 11:43:50,301 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-02-01 11:43:50,301 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-01 11:43:50,301 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2019-02-01 11:43:50,302 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2019-02-01 11:43:50,306 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-02-01 11:43:50,306 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-02-01 11:43:50,306 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-02-01 11:43:50,326 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-02-01 11:43:50,326 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-02-01 11:43:50,327 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-02-01 11:43:50,344 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-02-01 11:43:50,345 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-02-01 11:43:50,353 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-02-01 11:43:50,354 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-01 11:43:50,354 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2019-02-01 11:43:50,354 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2019-02-01 11:43:50,563 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/ubuntu/hdata/dfs/name/in_use.lock acquired by nodename 8483@ubuntu-HP-Pavilion-dv6-NoteBook-PC
2019-02-01 11:43:51,626 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/ubuntu/hdata/dfs/name/current
2019-02-01 11:43:51,640 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2019-02-01 11:43:51,641 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-02-01 11:43:52,106 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-02-01 11:43:52,717 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-02-01 11:43:52,717 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000000
2019-02-01 11:43:52,851 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2019-02-01 11:43:52,852 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2019-02-01 11:43:54,055 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-02-01 11:43:54,056 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 3650 msecs
2019-02-01 11:43:56,109 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2019-02-01 11:43:56,183 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2019-02-01 11:43:56,325 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2019-02-01 11:43:56,686 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2019-02-01 11:43:56,769 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2019-02-01 11:43:56,769 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2019-02-01 11:43:56,769 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2019-02-01 11:43:56,770 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 7 secs
2019-02-01 11:43:56,771 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2019-02-01 11:43:56,771 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2019-02-01 11:43:56,846 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-02-01 11:43:56,870 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2019-02-01 11:43:56,885 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2019-02-01 11:43:56,885 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2019-02-01 11:43:56,886 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2019-02-01 11:43:56,886 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2019-02-01 11:43:56,886 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 112 msec
2019-02-01 11:43:57,347 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2019-02-01 11:43:57,348 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2019-02-01 11:43:57,323 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-01 11:43:57,327 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2019-02-01 11:43:57,470 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-02-01 11:44:11,910 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=50bf9075-0a2a-4a3b-a9e7-9154c919bf09, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-44195f4d-20ee-4085-84d3-18859dbea129;nsid=1647206854;c=0) storage 50bf9075-0a2a-4a3b-a9e7-9154c919bf09
2019-02-01 11:44:11,910 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-02-01 11:44:11,928 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2019-02-01 11:44:13,415 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-02-01 11:44:13,415 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1 for DN 127.0.0.1:50010
2019-02-01 11:44:13,762 INFO BlockStateChange: BLOCK* processReport: from storage DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=50bf9075-0a2a-4a3b-a9e7-9154c919bf09, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-44195f4d-20ee-4085-84d3-18859dbea129;nsid=1647206854;c=0), blocks: 0, hasStaleStorage: false, processing time: 7 msecs
2019-02-01 11:45:21,332 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2019-02-01 11:45:21,332 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-02-01 11:45:21,345 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2019-02-01 11:45:21,348 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 69 
2019-02-01 11:45:21,396 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 116 
2019-02-01 11:45:21,401 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/ubuntu/hdata/dfs/name/current/edits_inprogress_0000000000000000001 -> /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000001-0000000000000000002
2019-02-01 11:45:21,422 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2019-02-01 11:45:36,948 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.06s at 0.00 KB/s
2019-02-01 11:45:36,950 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 351 bytes.
2019-02-01 11:45:37,056 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2019-02-01 12:38:22,301 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2019-02-01 12:38:22,304 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu-HP-Pavilion-dv6-NoteBook-PC/127.0.1.1
************************************************************/
2019-02-03 09:49:49,981 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu-HP-Pavilion-dv6-NoteBook-PC/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /opt/hadoop/hadoop-2.7.3/etc/hadoop:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_171
************************************************************/
2019-02-03 09:49:50,022 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-03 09:49:50,026 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-02-03 09:49:50,504 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-03 09:49:50,619 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-02-03 09:49:50,619 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-02-03 09:49:50,621 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2019-02-03 09:49:50,622 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2019-02-03 09:49:50,898 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2019-02-03 09:49:51,000 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-02-03 09:49:51,007 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-03 09:49:51,013 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-02-03 09:49:51,047 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-03 09:49:51,049 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-02-03 09:49:51,050 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-03 09:49:51,050 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-03 09:49:51,182 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-02-03 09:49:51,200 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-02-03 09:49:51,215 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2019-02-03 09:49:51,216 INFO org.mortbay.log: jetty-6.1.26
2019-02-03 09:49:51,457 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2019-02-03 09:49:51,542 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-03 09:49:51,542 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-03 09:49:51,580 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-02-03 09:49:51,580 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-02-03 09:49:51,632 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-02-03 09:49:51,632 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-02-03 09:49:51,633 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-02-03 09:49:51,634 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Feb 03 09:49:51
2019-02-03 09:49:51,635 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-02-03 09:49:51,635 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-03 09:49:51,655 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2019-02-03 09:49:51,655 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-02-03 09:49:51,661 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-02-03 09:49:51,661 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2019-02-03 09:49:51,661 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-02-03 09:49:51,661 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-02-03 09:49:51,661 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-02-03 09:49:51,662 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-02-03 09:49:51,662 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-02-03 09:49:51,662 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-02-03 09:49:51,668 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-02-03 09:49:51,668 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-02-03 09:49:51,668 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2019-02-03 09:49:51,668 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-02-03 09:49:51,669 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-02-03 09:49:51,845 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-02-03 09:49:51,845 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-03 09:49:51,846 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2019-02-03 09:49:51,846 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-02-03 09:49:51,846 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-02-03 09:49:51,846 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-02-03 09:49:51,846 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-02-03 09:49:51,847 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-02-03 09:49:51,855 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-02-03 09:49:51,855 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-03 09:49:51,855 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2019-02-03 09:49:51,855 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2019-02-03 09:49:51,856 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-02-03 09:49:51,856 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-02-03 09:49:51,857 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-02-03 09:49:51,859 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-02-03 09:49:51,859 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-02-03 09:49:51,859 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-02-03 09:49:51,860 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-02-03 09:49:51,860 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-02-03 09:49:51,862 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-02-03 09:49:51,862 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-03 09:49:51,862 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2019-02-03 09:49:51,862 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2019-02-03 09:49:51,986 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/ubuntu/hdata/dfs/name/in_use.lock acquired by nodename 6040@ubuntu-HP-Pavilion-dv6-NoteBook-PC
2019-02-03 09:49:52,154 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/ubuntu/hdata/dfs/name/current
2019-02-03 09:49:52,231 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/ubuntu/hdata/dfs/name/current/edits_inprogress_0000000000000000003 -> /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000003-0000000000000000003
2019-02-03 09:49:52,254 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000002, cpktTxId=0000000000000000002)
2019-02-03 09:49:52,382 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-02-03 09:49:52,407 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-02-03 09:49:52,407 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 2 from /home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000002
2019-02-03 09:49:52,407 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3401a114 expecting start txid #3
2019-02-03 09:49:52,407 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000003-0000000000000000003
2019-02-03 09:49:52,409 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/ubuntu/hdata/dfs/name/current/edits_0000000000000000003-0000000000000000003' to transaction ID 3
2019-02-03 09:49:52,411 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000003-0000000000000000003 of size 1048576 edits # 1 loaded in 0 seconds
2019-02-03 09:49:52,467 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
2019-02-03 09:49:52,467 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
2019-02-03 09:49:52,472 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /home/ubuntu/hdata/dfs/name/current/fsimage.ckpt_0000000000000000003 using no compression
2019-02-03 09:49:52,513 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /home/ubuntu/hdata/dfs/name/current/fsimage.ckpt_0000000000000000003 of size 351 bytes saved in 0 seconds.
2019-02-03 09:49:52,571 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2
2019-02-03 09:49:52,571 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-02-03 09:49:52,678 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 4
2019-02-03 09:49:52,944 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-02-03 09:49:52,945 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 1079 msecs
2019-02-03 09:49:53,203 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2019-02-03 09:49:53,220 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2019-02-03 09:49:53,230 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2019-02-03 09:49:53,299 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2019-02-03 09:49:53,327 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2019-02-03 09:49:53,328 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2019-02-03 09:49:53,328 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2019-02-03 09:49:53,329 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2019-02-03 09:49:53,329 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2019-02-03 09:49:53,330 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2019-02-03 09:49:53,339 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-02-03 09:49:53,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2019-02-03 09:49:53,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2019-02-03 09:49:53,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2019-02-03 09:49:53,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2019-02-03 09:49:53,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2019-02-03 09:49:53,341 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 12 msec
2019-02-03 09:49:53,445 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-03 09:49:53,447 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2019-02-03 09:49:53,453 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2019-02-03 09:49:53,453 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2019-02-03 09:49:53,458 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-02-03 09:49:57,226 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=50bf9075-0a2a-4a3b-a9e7-9154c919bf09, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-44195f4d-20ee-4085-84d3-18859dbea129;nsid=1647206854;c=0) storage 50bf9075-0a2a-4a3b-a9e7-9154c919bf09
2019-02-03 09:49:57,227 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-02-03 09:49:57,227 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2019-02-03 09:49:57,320 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-02-03 09:49:57,320 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1 for DN 127.0.0.1:50010
2019-02-03 09:49:57,361 INFO BlockStateChange: BLOCK* processReport: from storage DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=50bf9075-0a2a-4a3b-a9e7-9154c919bf09, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-44195f4d-20ee-4085-84d3-18859dbea129;nsid=1647206854;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2019-02-03 10:21:01,812 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2019-02-03 10:21:01,813 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-02-03 10:21:01,813 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 4
2019-02-03 10:21:01,814 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 102 
2019-02-03 10:21:01,850 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 137 
2019-02-03 10:21:01,854 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/ubuntu/hdata/dfs/name/current/edits_inprogress_0000000000000000004 -> /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000004-0000000000000000005
2019-02-03 10:21:01,854 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 6
2019-02-03 10:21:03,368 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2019-02-03 10:21:03,368 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000005 size 351 bytes.
2019-02-03 10:21:03,425 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 3
2019-02-03 10:21:03,426 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000002, cpktTxId=0000000000000000002)
2019-02-03 11:24:41,033 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 5556ms
No GCs detected
2019-02-03 11:50:11,981 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2019-02-03 11:50:11,981 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-02-03 11:50:11,981 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 6
2019-02-03 11:50:11,982 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 92 
2019-02-03 11:50:12,037 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 147 
2019-02-03 11:50:12,040 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/ubuntu/hdata/dfs/name/current/edits_inprogress_0000000000000000006 -> /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000006-0000000000000000007
2019-02-03 11:50:12,040 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 8
2019-02-03 11:50:12,361 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2019-02-03 11:50:12,362 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000007 size 351 bytes.
2019-02-03 11:50:12,408 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 5
2019-02-03 11:50:12,408 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000003, cpktTxId=0000000000000000003)
2019-02-03 11:54:08,117 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 3 Total time for transactions(ms): 33 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 97 
2019-02-03 12:00:35,597 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 12 Total time for transactions(ms): 33 Number of transactions batched in Syncs: 0 Number of syncs: 8 SyncTimes(ms): 207 
2019-02-03 12:00:35,862 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /stocks/input/plain-text/NASDAQ/AAPL/stocks.csv._COPYING_
2019-02-03 12:00:36,163 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /stocks/input/plain-text/NASDAQ/AAPL/stocks.csv._COPYING_
2019-02-03 12:00:36,180 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 317543
2019-02-03 12:00:36,618 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /stocks/input/plain-text/NASDAQ/AAPL/stocks.csv._COPYING_ is closed by DFSClient_NONMAPREDUCE_386327010_1
2019-02-03 12:00:36,770 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /stocks/input/plain-text/NASDAQ/INTC/stocks.csv._COPYING_
2019-02-03 12:00:36,784 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:00:36,796 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /stocks/input/plain-text/NASDAQ/INTC/stocks.csv._COPYING_ is closed by DFSClient_NONMAPREDUCE_386327010_1
2019-02-03 12:00:36,857 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /stocks/input/plain-text/NYSE/GE/stocks.csv._COPYING_
2019-02-03 12:00:36,872 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:00:36,885 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /stocks/input/plain-text/NYSE/GE/stocks.csv._COPYING_ is closed by DFSClient_NONMAPREDUCE_386327010_1
2019-02-03 12:00:36,967 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /stocks/input/plain-text/NYSE/IBM/stocks.csv._COPYING_
2019-02-03 12:00:36,983 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:00:36,996 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /stocks/input/plain-text/NYSE/IBM/stocks.csv._COPYING_ is closed by DFSClient_NONMAPREDUCE_386327010_1
2019-02-03 12:03:11,389 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 45 Total time for transactions(ms): 35 Number of transactions batched in Syncs: 1 Number of syncs: 33 SyncTimes(ms): 585 
2019-02-03 12:04:25,806 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 59 Total time for transactions(ms): 35 Number of transactions batched in Syncs: 3 Number of syncs: 45 SyncTimes(ms): 798 
2019-02-03 12:08:45,324 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 66 Total time for transactions(ms): 35 Number of transactions batched in Syncs: 4 Number of syncs: 51 SyncTimes(ms): 875 
2019-02-03 12:10:09,836 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 73 Total time for transactions(ms): 35 Number of transactions batched in Syncs: 5 Number of syncs: 57 SyncTimes(ms): 955 
2019-02-03 12:11:39,638 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 80 Total time for transactions(ms): 35 Number of transactions batched in Syncs: 6 Number of syncs: 63 SyncTimes(ms): 1021 
2019-02-03 12:11:40,491 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-11-39_609_8500457636709532544-1/-mr-10004/404df038-b61c-44d4-b47c-110973ca6be2/map.xml
2019-02-03 12:11:40,593 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:11:40,600 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-11-39_609_8500457636709532544-1/-mr-10004/404df038-b61c-44d4-b47c-110973ca6be2/map.xml is closed by DFSClient_NONMAPREDUCE_-274780454_1
2019-02-03 12:11:40,610 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-11-39_609_8500457636709532544-1/-mr-10004/404df038-b61c-44d4-b47c-110973ca6be2/map.xml
2019-02-03 12:11:40,647 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-11-39_609_8500457636709532544-1/-mr-10004/404df038-b61c-44d4-b47c-110973ca6be2/reduce.xml
2019-02-03 12:11:40,663 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:11:40,678 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-11-39_609_8500457636709532544-1/-mr-10004/404df038-b61c-44d4-b47c-110973ca6be2/reduce.xml is closed by DFSClient_NONMAPREDUCE_-274780454_1
2019-02-03 12:11:40,681 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-11-39_609_8500457636709532544-1/-mr-10004/404df038-b61c-44d4-b47c-110973ca6be2/reduce.xml
2019-02-03 12:11:41,302 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0001/job.jar
2019-02-03 12:11:41,550 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:11:41,578 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0001/job.jar is closed by DFSClient_NONMAPREDUCE_-274780454_1
2019-02-03 12:11:41,579 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0001/job.jar
2019-02-03 12:11:41,723 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0001/job.split
2019-02-03 12:11:41,738 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0001/job.split
2019-02-03 12:11:41,753 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:11:41,755 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0001/job.split is closed by DFSClient_NONMAPREDUCE_-274780454_1
2019-02-03 12:11:41,781 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0001/job.splitmetainfo
2019-02-03 12:11:41,795 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:11:41,800 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0001/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-274780454_1
2019-02-03 12:11:41,852 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0001/job.xml
2019-02-03 12:11:41,880 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:11:41,889 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0001/job.xml is closed by DFSClient_NONMAPREDUCE_-274780454_1
2019-02-03 12:11:53,004 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:35030 Call#7 Retry#0: java.io.FileNotFoundException: File does not exist: /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0001/job_1549167607896_0001_1.jhist
2019-02-03 12:11:55,757 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741829_1005 127.0.0.1:50010 
2019-02-03 12:11:55,768 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741830_1006 127.0.0.1:50010 
2019-02-03 12:11:58,248 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741829_1005, blk_1073741830_1006]
2019-02-03 12:17:04,947 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 143 Total time for transactions(ms): 42 Number of transactions batched in Syncs: 8 Number of syncs: 108 SyncTimes(ms): 1590 
2019-02-03 12:17:05,124 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-17-04_921_2385794882043088775-1/-mr-10004/a8df405f-c8af-4945-ab10-641e5b92ed01/map.xml
2019-02-03 12:17:05,133 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:17:05,139 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-17-04_921_2385794882043088775-1/-mr-10004/a8df405f-c8af-4945-ab10-641e5b92ed01/map.xml is closed by DFSClient_NONMAPREDUCE_-274780454_1
2019-02-03 12:17:05,141 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-17-04_921_2385794882043088775-1/-mr-10004/a8df405f-c8af-4945-ab10-641e5b92ed01/map.xml
2019-02-03 12:17:05,169 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-17-04_921_2385794882043088775-1/-mr-10004/a8df405f-c8af-4945-ab10-641e5b92ed01/reduce.xml
2019-02-03 12:17:05,180 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:17:05,183 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-17-04_921_2385794882043088775-1/-mr-10004/a8df405f-c8af-4945-ab10-641e5b92ed01/reduce.xml is closed by DFSClient_NONMAPREDUCE_-274780454_1
2019-02-03 12:17:05,185 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-17-04_921_2385794882043088775-1/-mr-10004/a8df405f-c8af-4945-ab10-641e5b92ed01/reduce.xml
2019-02-03 12:17:05,299 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0002/job.jar
2019-02-03 12:17:05,425 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:17:05,428 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0002/job.jar is closed by DFSClient_NONMAPREDUCE_-274780454_1
2019-02-03 12:17:05,429 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0002/job.jar
2019-02-03 12:17:05,530 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0002/job.split
2019-02-03 12:17:05,542 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0002/job.split
2019-02-03 12:17:05,563 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:17:05,572 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0002/job.split is closed by DFSClient_NONMAPREDUCE_-274780454_1
2019-02-03 12:17:05,597 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0002/job.splitmetainfo
2019-02-03 12:17:05,618 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:17:05,628 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0002/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-274780454_1
2019-02-03 12:17:05,671 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0002/job.xml
2019-02-03 12:17:05,689 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:17:05,695 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0002/job.xml is closed by DFSClient_NONMAPREDUCE_-274780454_1
2019-02-03 12:17:13,951 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:35134 Call#7 Retry#0: java.io.FileNotFoundException: File does not exist: /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0002/job_1549167607896_0002_1.jhist
2019-02-03 12:17:16,770 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0002/job_1549167607896_0002_2_conf.xml
2019-02-03 12:17:17,296 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741835_1011 127.0.0.1:50010 
2019-02-03 12:17:17,307 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741836_1012 127.0.0.1:50010 
2019-02-03 12:17:19,296 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741835_1011, blk_1073741836_1012]
2019-02-03 12:18:18,059 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 202 Total time for transactions(ms): 45 Number of transactions batched in Syncs: 10 Number of syncs: 151 SyncTimes(ms): 2030 
2019-02-03 12:18:18,184 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getContentSummary from 127.0.0.1:35314 Call#485 Retry#0: java.io.FileNotFoundException: File does not exist: /user/root/-mr-10003besant.stocks{exch=NASDAQ, symbol=AAPL}
2019-02-03 12:18:18,191 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getContentSummary from 127.0.0.1:35314 Call#486 Retry#0: java.io.FileNotFoundException: File does not exist: /user/root/-mr-10004besant.stocks{exch=NASDAQ, symbol=INTC}
2019-02-03 12:18:18,193 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getContentSummary from 127.0.0.1:35314 Call#487 Retry#0: java.io.FileNotFoundException: File does not exist: /user/root/-mr-10005besant.stocks{exch=NYSE, symbol=GE}
2019-02-03 12:18:18,195 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getContentSummary from 127.0.0.1:35314 Call#488 Retry#0: java.io.FileNotFoundException: File does not exist: /user/root/-mr-10006besant.stocks{exch=NYSE, symbol=IBM}
2019-02-03 12:18:18,227 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-18-18_039_1184092446574727904-1/-mr-10007/0/emptyFile
2019-02-03 12:18:18,241 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:18:18,245 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-18-18_039_1184092446574727904-1/-mr-10007/0/emptyFile is closed by DFSClient_NONMAPREDUCE_-274780454_1
2019-02-03 12:18:18,271 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-18-18_039_1184092446574727904-1/-mr-10007/1/emptyFile
2019-02-03 12:18:18,287 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:18:18,301 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-18-18_039_1184092446574727904-1/-mr-10007/1/emptyFile is closed by DFSClient_NONMAPREDUCE_-274780454_1
2019-02-03 12:18:18,326 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-18-18_039_1184092446574727904-1/-mr-10007/2/emptyFile
2019-02-03 12:18:18,345 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:18:18,356 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-18-18_039_1184092446574727904-1/-mr-10007/2/emptyFile is closed by DFSClient_NONMAPREDUCE_-274780454_1
2019-02-03 12:18:18,383 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-18-18_039_1184092446574727904-1/-mr-10007/3/emptyFile
2019-02-03 12:18:18,399 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:18:18,412 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-18-18_039_1184092446574727904-1/-mr-10007/3/emptyFile is closed by DFSClient_NONMAPREDUCE_-274780454_1
2019-02-03 12:18:18,438 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-18-18_039_1184092446574727904-1/-mr-10008/dc1c9c73-145b-4ff8-9cc5-11b3f6fe8dca/map.xml
2019-02-03 12:18:18,452 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:18:18,456 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-18-18_039_1184092446574727904-1/-mr-10008/dc1c9c73-145b-4ff8-9cc5-11b3f6fe8dca/map.xml is closed by DFSClient_NONMAPREDUCE_-274780454_1
2019-02-03 12:18:18,458 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-18-18_039_1184092446574727904-1/-mr-10008/dc1c9c73-145b-4ff8-9cc5-11b3f6fe8dca/map.xml
2019-02-03 12:18:18,487 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-18-18_039_1184092446574727904-1/-mr-10008/dc1c9c73-145b-4ff8-9cc5-11b3f6fe8dca/reduce.xml
2019-02-03 12:18:18,498 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:18:18,501 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-18-18_039_1184092446574727904-1/-mr-10008/dc1c9c73-145b-4ff8-9cc5-11b3f6fe8dca/reduce.xml is closed by DFSClient_NONMAPREDUCE_-274780454_1
2019-02-03 12:18:18,503 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/hive_2019-02-03_12-18-18_039_1184092446574727904-1/-mr-10008/dc1c9c73-145b-4ff8-9cc5-11b3f6fe8dca/reduce.xml
2019-02-03 12:18:18,626 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0003/job.jar
2019-02-03 12:18:18,718 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:18:18,723 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0003/job.jar is closed by DFSClient_NONMAPREDUCE_-274780454_1
2019-02-03 12:18:18,724 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0003/job.jar
2019-02-03 12:18:18,802 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0003/job.split
2019-02-03 12:18:18,815 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0003/job.split
2019-02-03 12:18:18,833 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:18:18,845 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0003/job.split is closed by DFSClient_NONMAPREDUCE_-274780454_1
2019-02-03 12:18:18,870 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0003/job.splitmetainfo
2019-02-03 12:18:18,889 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:18:18,901 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0003/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-274780454_1
2019-02-03 12:18:18,937 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0003/job.xml
2019-02-03 12:18:18,952 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:18:18,956 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0003/job.xml is closed by DFSClient_NONMAPREDUCE_-274780454_1
2019-02-03 12:18:25,257 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0003/job_1549167607896_0003_1_conf.xml
2019-02-03 12:18:25,399 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:18:25,435 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0003/job_1549167607896_0003_1_conf.xml is closed by DFSClient_NONMAPREDUCE_1602473080_1
2019-02-03 12:18:25,734 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0003/job_1549167607896_0003_1.jhist
2019-02-03 12:18:25,778 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:18:25,790 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0003/job_1549167607896_0003_1.jhist is closed by DFSClient_NONMAPREDUCE_1602473080_1
2019-02-03 12:18:32,047 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741846_1022 127.0.0.1:50010 
2019-02-03 12:18:32,058 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741847_1023 127.0.0.1:50010 
2019-02-03 12:18:32,091 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741842_1018 127.0.0.1:50010 
2019-02-03 12:18:32,091 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741843_1019 127.0.0.1:50010 
2019-02-03 12:18:32,091 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741844_1020 127.0.0.1:50010 
2019-02-03 12:18:32,091 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741845_1021 127.0.0.1:50010 
2019-02-03 12:18:34,308 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741842_1018, blk_1073741843_1019, blk_1073741844_1020, blk_1073741845_1021, blk_1073741846_1022, blk_1073741847_1023]
2019-02-03 12:22:50,870 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2019-02-03 12:22:50,872 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu-HP-Pavilion-dv6-NoteBook-PC/127.0.1.1
************************************************************/
2019-02-03 12:23:28,364 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu-HP-Pavilion-dv6-NoteBook-PC/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /opt/hadoop/hadoop-2.7.3/etc/hadoop:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_171
************************************************************/
2019-02-03 12:23:28,375 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-03 12:23:28,379 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-02-03 12:23:28,695 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-03 12:23:28,779 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-02-03 12:23:28,779 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-02-03 12:23:28,782 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2019-02-03 12:23:28,783 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2019-02-03 12:23:29,010 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2019-02-03 12:23:29,067 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-02-03 12:23:29,078 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-03 12:23:29,087 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-02-03 12:23:29,092 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-03 12:23:29,097 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-02-03 12:23:29,097 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-03 12:23:29,097 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-03 12:23:29,253 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-02-03 12:23:29,254 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-02-03 12:23:29,272 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2019-02-03 12:23:29,272 INFO org.mortbay.log: jetty-6.1.26
2019-02-03 12:23:29,424 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2019-02-03 12:23:29,466 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-03 12:23:29,466 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-03 12:23:29,511 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-02-03 12:23:29,512 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-02-03 12:23:29,580 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-02-03 12:23:29,580 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-02-03 12:23:29,582 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-02-03 12:23:29,582 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Feb 03 12:23:29
2019-02-03 12:23:29,585 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-02-03 12:23:29,585 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-03 12:23:29,587 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2019-02-03 12:23:29,587 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-02-03 12:23:29,595 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-02-03 12:23:29,595 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2019-02-03 12:23:29,595 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-02-03 12:23:29,595 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-02-03 12:23:29,595 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-02-03 12:23:29,595 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-02-03 12:23:29,595 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-02-03 12:23:29,596 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-02-03 12:23:29,605 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-02-03 12:23:29,605 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-02-03 12:23:29,605 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2019-02-03 12:23:29,605 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-02-03 12:23:29,606 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-02-03 12:23:29,670 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-02-03 12:23:29,670 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-03 12:23:29,670 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2019-02-03 12:23:29,670 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-02-03 12:23:29,671 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-02-03 12:23:29,671 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-02-03 12:23:29,671 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-02-03 12:23:29,671 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-02-03 12:23:29,678 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-02-03 12:23:29,678 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-03 12:23:29,678 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2019-02-03 12:23:29,678 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2019-02-03 12:23:29,680 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-02-03 12:23:29,680 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-02-03 12:23:29,680 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-02-03 12:23:29,682 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-02-03 12:23:29,682 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-02-03 12:23:29,682 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-02-03 12:23:29,684 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-02-03 12:23:29,684 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-02-03 12:23:29,686 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-02-03 12:23:29,686 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-03 12:23:29,686 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2019-02-03 12:23:29,686 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2019-02-03 12:23:29,759 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/ubuntu/hdata/dfs/name/in_use.lock acquired by nodename 15544@ubuntu-HP-Pavilion-dv6-NoteBook-PC
2019-02-03 12:23:29,826 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/ubuntu/hdata/dfs/name/current
2019-02-03 12:23:29,924 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/ubuntu/hdata/dfs/name/current/edits_inprogress_0000000000000000008 -> /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000008-0000000000000000296
2019-02-03 12:23:29,933 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000007, cpktTxId=0000000000000000007)
2019-02-03 12:23:30,028 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2019-02-03 12:23:30,065 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-02-03 12:23:30,065 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 7 from /home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000007
2019-02-03 12:23:30,065 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@758a34ce expecting start txid #8
2019-02-03 12:23:30,065 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000008-0000000000000000296
2019-02-03 12:23:30,067 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/ubuntu/hdata/dfs/name/current/edits_0000000000000000008-0000000000000000296' to transaction ID 8
2019-02-03 12:23:30,138 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000008-0000000000000000296 of size 1048576 edits # 289 loaded in 0 seconds
2019-02-03 12:23:30,139 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2019-02-03 12:23:30,141 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 297
2019-02-03 12:23:30,383 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-02-03 12:23:30,383 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 694 msecs
2019-02-03 12:23:30,540 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2019-02-03 12:23:30,550 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2019-02-03 12:23:30,562 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2019-02-03 12:23:30,592 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2019-02-03 12:23:30,600 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 1
2019-02-03 12:23:30,600 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 1
2019-02-03 12:23:30,600 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 18 blocks to reach the threshold 0.9990 of total blocks 18.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2019-02-03 12:23:30,606 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-02-03 12:23:30,632 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-03 12:23:30,632 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2019-02-03 12:23:30,639 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2019-02-03 12:23:30,639 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2019-02-03 12:23:30,643 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-02-03 12:23:35,021 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=50bf9075-0a2a-4a3b-a9e7-9154c919bf09, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-44195f4d-20ee-4085-84d3-18859dbea129;nsid=1647206854;c=0) storage 50bf9075-0a2a-4a3b-a9e7-9154c919bf09
2019-02-03 12:23:35,021 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-02-03 12:23:35,022 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2019-02-03 12:23:35,113 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-02-03 12:23:35,113 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1 for DN 127.0.0.1:50010
2019-02-03 12:23:35,171 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 17 has reached the threshold 0.9990 of total blocks 18. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2019-02-03 12:23:35,171 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2019-02-03 12:23:35,173 INFO BlockStateChange: BLOCK* processReport: from storage DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=50bf9075-0a2a-4a3b-a9e7-9154c919bf09, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-44195f4d-20ee-4085-84d3-18859dbea129;nsid=1647206854;c=0), blocks: 18, hasStaleStorage: false, processing time: 8 msecs
2019-02-03 12:23:35,177 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 19
2019-02-03 12:23:35,177 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2019-02-03 12:23:35,177 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 6
2019-02-03 12:23:35,177 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2019-02-03 12:23:35,177 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 1
2019-02-03 12:23:35,178 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 6 msec
2019-02-03 12:23:55,177 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 18 has reached the threshold 0.9990 of total blocks 18. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2019-02-03 12:23:59,564 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.delete from 127.0.0.1:35450 Call#644 Retry#0: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45. Name node is in safe mode.
The reported blocks 18 has reached the threshold 0.9990 of total blocks 18. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 5 seconds.
2019-02-03 12:23:59,609 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.delete from 127.0.0.1:35450 Call#646 Retry#0: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45. Name node is in safe mode.
The reported blocks 18 has reached the threshold 0.9990 of total blocks 18. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 5 seconds.
2019-02-03 12:23:59,620 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.delete from 127.0.0.1:35450 Call#648 Retry#0: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /tmp/hive/root/d4dfec9e-df17-4b8c-953c-1075a25dba45/_tmp_space.db. Name node is in safe mode.
The reported blocks 18 has reached the threshold 0.9990 of total blocks 18. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 5 seconds.
2019-02-03 12:24:05,179 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 35 secs
2019-02-03 12:24:05,180 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2019-02-03 12:24:05,180 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2019-02-03 12:24:05,180 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 6 blocks
2019-02-03 12:24:18,166 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command getfileinfo is: 1
2019-02-03 12:24:18,174 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command * is: 1
2019-02-03 12:24:18,175 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command getfileinfo is: 1
2019-02-03 12:24:18,175 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command * is: 1
2019-02-03 12:24:18,175 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command getfileinfo is: 1
2019-02-03 12:24:18,175 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command * is: 1
2019-02-03 12:24:39,930 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2019-02-03 12:24:39,930 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-02-03 12:24:39,930 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 297
2019-02-03 12:24:39,930 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 0 Number of syncs: 4 SyncTimes(ms): 110 
2019-02-03 12:24:39,969 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 0 Number of syncs: 5 SyncTimes(ms): 149 
2019-02-03 12:24:39,973 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/ubuntu/hdata/dfs/name/current/edits_inprogress_0000000000000000297 -> /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000297-0000000000000000300
2019-02-03 12:24:39,974 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 301
2019-02-03 12:24:41,392 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.07s at 54.79 KB/s
2019-02-03 12:24:41,392 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000300 size 4269 bytes.
2019-02-03 12:24:41,450 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 7
2019-02-03 12:24:41,451 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000005, cpktTxId=0000000000000000005)
2019-02-03 12:25:48,602 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 3 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 122 
2019-02-03 12:27:08,230 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 5 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 5 SyncTimes(ms): 170 
2019-02-03 12:27:08,978 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getContentSummary from 127.0.0.1:35516 Call#27 Retry#0: java.io.FileNotFoundException: File does not exist: /user/root/-mr-10003besant.stocks{exch=NASDAQ, symbol=AAPL}
2019-02-03 12:27:08,987 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getContentSummary from 127.0.0.1:35516 Call#28 Retry#0: java.io.FileNotFoundException: File does not exist: /user/root/-mr-10004besant.stocks{exch=NASDAQ, symbol=INTC}
2019-02-03 12:27:08,989 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getContentSummary from 127.0.0.1:35516 Call#29 Retry#0: java.io.FileNotFoundException: File does not exist: /user/root/-mr-10005besant.stocks{exch=NYSE, symbol=GE}
2019-02-03 12:27:08,992 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getContentSummary from 127.0.0.1:35516 Call#30 Retry#0: java.io.FileNotFoundException: File does not exist: /user/root/-mr-10006besant.stocks{exch=NYSE, symbol=IBM}
2019-02-03 12:27:09,122 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-07_854_4353896192731489829-1/-mr-10007/0/emptyFile
2019-02-03 12:27:09,418 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741854_1030{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-07_854_4353896192731489829-1/-mr-10007/0/emptyFile
2019-02-03 12:27:09,458 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741854_1030{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 6
2019-02-03 12:27:09,834 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-07_854_4353896192731489829-1/-mr-10007/0/emptyFile is closed by DFSClient_NONMAPREDUCE_-1837917950_1
2019-02-03 12:27:09,859 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-07_854_4353896192731489829-1/-mr-10007/1/emptyFile
2019-02-03 12:27:09,876 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:27:09,889 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-07_854_4353896192731489829-1/-mr-10007/1/emptyFile is closed by DFSClient_NONMAPREDUCE_-1837917950_1
2019-02-03 12:27:09,914 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-07_854_4353896192731489829-1/-mr-10007/2/emptyFile
2019-02-03 12:27:09,930 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:27:09,934 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-07_854_4353896192731489829-1/-mr-10007/2/emptyFile is closed by DFSClient_NONMAPREDUCE_-1837917950_1
2019-02-03 12:27:09,959 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-07_854_4353896192731489829-1/-mr-10007/3/emptyFile
2019-02-03 12:27:09,979 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:27:09,989 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-07_854_4353896192731489829-1/-mr-10007/3/emptyFile is closed by DFSClient_NONMAPREDUCE_-1837917950_1
2019-02-03 12:27:10,099 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-07_854_4353896192731489829-1/-mr-10008/f74b5dbb-ddc3-43a7-9154-ee410c9c4d88/map.xml
2019-02-03 12:27:10,119 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:27:10,134 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-07_854_4353896192731489829-1/-mr-10008/f74b5dbb-ddc3-43a7-9154-ee410c9c4d88/map.xml is closed by DFSClient_NONMAPREDUCE_-1837917950_1
2019-02-03 12:27:10,148 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-07_854_4353896192731489829-1/-mr-10008/f74b5dbb-ddc3-43a7-9154-ee410c9c4d88/map.xml
2019-02-03 12:27:10,198 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-07_854_4353896192731489829-1/-mr-10008/f74b5dbb-ddc3-43a7-9154-ee410c9c4d88/reduce.xml
2019-02-03 12:27:10,211 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:27:10,223 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-07_854_4353896192731489829-1/-mr-10008/f74b5dbb-ddc3-43a7-9154-ee410c9c4d88/reduce.xml is closed by DFSClient_NONMAPREDUCE_-1837917950_1
2019-02-03 12:27:10,225 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-07_854_4353896192731489829-1/-mr-10008/f74b5dbb-ddc3-43a7-9154-ee410c9c4d88/reduce.xml
2019-02-03 12:27:10,659 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0001/job.jar
2019-02-03 12:27:10,999 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:27:11,011 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0001/job.jar is closed by DFSClient_NONMAPREDUCE_-1837917950_1
2019-02-03 12:27:11,013 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0001/job.jar
2019-02-03 12:27:11,136 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0001/job.split
2019-02-03 12:27:11,150 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0001/job.split
2019-02-03 12:27:11,171 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:27:11,178 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0001/job.split is closed by DFSClient_NONMAPREDUCE_-1837917950_1
2019-02-03 12:27:11,207 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0001/job.splitmetainfo
2019-02-03 12:27:11,221 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:27:11,234 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0001/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1837917950_1
2019-02-03 12:27:11,295 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0001/job.xml
2019-02-03 12:27:11,317 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:27:11,334 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0001/job.xml is closed by DFSClient_NONMAPREDUCE_-1837917950_1
2019-02-03 12:27:20,210 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0001/job_1549176826207_0001_1_conf.xml
2019-02-03 12:27:20,435 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:27:20,446 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0001/job_1549176826207_0001_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-1711676117_1
2019-02-03 12:27:25,664 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0001/job_1549176826207_0001_1.jhist
2019-02-03 12:27:25,709 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0001/job_1549176826207_0001_1.jhist for DFSClient_NONMAPREDUCE_-1711676117_1
2019-02-03 12:27:32,060 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-07_854_4353896192731489829-1/-mr-10000/.hive-staging_hive_2019-02-03_12-27-07_854_4353896192731489829-1/_task_tmp.-ext-10001/_tmp.000000_0
2019-02-03 12:27:32,283 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:27:32,302 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-07_854_4353896192731489829-1/-mr-10000/.hive-staging_hive_2019-02-03_12-27-07_854_4353896192731489829-1/_task_tmp.-ext-10001/_tmp.000000_0 is closed by DFSClient_attempt_1549176826207_0001_r_000000_0_40346326_1
2019-02-03 12:27:32,558 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0001/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-1711676117_1
2019-02-03 12:27:32,580 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0001/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1711676117_1
2019-02-03 12:27:32,692 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741865_1041{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0001/job_1549176826207_0001_1.jhist
2019-02-03 12:27:32,693 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741865_1041{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 35373
2019-02-03 12:27:33,114 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0001/job_1549176826207_0001_1.jhist is closed by DFSClient_NONMAPREDUCE_-1711676117_1
2019-02-03 12:27:33,129 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549176826207_0001.summary_tmp
2019-02-03 12:27:33,143 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:27:33,147 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549176826207_0001.summary_tmp is closed by DFSClient_NONMAPREDUCE_-1711676117_1
2019-02-03 12:27:33,198 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549176826207_0001-1549177031590-root-SELECT+DISTINCT+symbol+FROM+stocks%28Stage%2D1%29-1549177052584-1-1-SUCCEEDED-default-1549177039058.jhist_tmp
2019-02-03 12:27:33,212 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:27:33,225 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549176826207_0001-1549177031590-root-SELECT+DISTINCT+symbol+FROM+stocks%28Stage%2D1%29-1549177052584-1-1-SUCCEEDED-default-1549177039058.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-1711676117_1
2019-02-03 12:27:33,265 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549176826207_0001_conf.xml_tmp
2019-02-03 12:27:33,281 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:27:33,291 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549176826207_0001_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-1711676117_1
2019-02-03 12:27:34,392 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741860_1036 127.0.0.1:50010 
2019-02-03 12:27:34,393 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741861_1037 127.0.0.1:50010 
2019-02-03 12:27:34,393 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741862_1038 127.0.0.1:50010 
2019-02-03 12:27:34,393 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741863_1039 127.0.0.1:50010 
2019-02-03 12:27:34,393 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741865_1041 127.0.0.1:50010 
2019-02-03 12:27:34,393 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741864_1040 127.0.0.1:50010 
2019-02-03 12:27:35,447 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741858_1034 127.0.0.1:50010 
2019-02-03 12:27:35,470 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741859_1035 127.0.0.1:50010 
2019-02-03 12:27:35,558 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741866_1042 127.0.0.1:50010 
2019-02-03 12:27:35,570 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741854_1030 127.0.0.1:50010 
2019-02-03 12:27:35,570 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741855_1031 127.0.0.1:50010 
2019-02-03 12:27:35,570 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741856_1032 127.0.0.1:50010 
2019-02-03 12:27:35,570 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741857_1033 127.0.0.1:50010 
2019-02-03 12:27:36,659 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741856_1032, blk_1073741857_1033, blk_1073741858_1034, blk_1073741859_1035, blk_1073741860_1036, blk_1073741861_1037, blk_1073741862_1038, blk_1073741863_1039, blk_1073741864_1040, blk_1073741865_1041, blk_1073741866_1042, blk_1073741854_1030, blk_1073741855_1031]
2019-02-03 12:27:59,165 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-58_710_4533973256457699223-1/-mr-10004/ba3e4f80-8b91-4836-b539-77439ee9fe16/map.xml
2019-02-03 12:27:59,177 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:27:59,183 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-58_710_4533973256457699223-1/-mr-10004/ba3e4f80-8b91-4836-b539-77439ee9fe16/map.xml is closed by DFSClient_NONMAPREDUCE_-1837917950_1
2019-02-03 12:27:59,184 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-58_710_4533973256457699223-1/-mr-10004/ba3e4f80-8b91-4836-b539-77439ee9fe16/map.xml
2019-02-03 12:27:59,211 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-58_710_4533973256457699223-1/-mr-10004/ba3e4f80-8b91-4836-b539-77439ee9fe16/reduce.xml
2019-02-03 12:27:59,221 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:27:59,227 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-58_710_4533973256457699223-1/-mr-10004/ba3e4f80-8b91-4836-b539-77439ee9fe16/reduce.xml is closed by DFSClient_NONMAPREDUCE_-1837917950_1
2019-02-03 12:27:59,229 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-58_710_4533973256457699223-1/-mr-10004/ba3e4f80-8b91-4836-b539-77439ee9fe16/reduce.xml
2019-02-03 12:27:59,342 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0002/job.jar
2019-02-03 12:27:59,477 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:27:59,483 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0002/job.jar is closed by DFSClient_NONMAPREDUCE_-1837917950_1
2019-02-03 12:27:59,484 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0002/job.jar
2019-02-03 12:27:59,551 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0002/job.split
2019-02-03 12:27:59,564 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0002/job.split
2019-02-03 12:27:59,581 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741873_1049{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0002/job.split
2019-02-03 12:27:59,582 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741873_1049{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 229
2019-02-03 12:27:59,994 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0002/job.split is closed by DFSClient_NONMAPREDUCE_-1837917950_1
2019-02-03 12:28:00,019 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741874_1050{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0002/job.splitmetainfo
2019-02-03 12:28:00,038 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741874_1050{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:28:00,049 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0002/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1837917950_1
2019-02-03 12:28:00,083 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0002/job.xml
2019-02-03 12:28:00,102 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:28:00,116 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0002/job.xml is closed by DFSClient_NONMAPREDUCE_-1837917950_1
2019-02-03 12:28:06,777 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0002/job_1549176826207_0002_1_conf.xml
2019-02-03 12:28:06,914 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:28:06,928 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0002/job_1549176826207_0002_1_conf.xml is closed by DFSClient_NONMAPREDUCE_846147682_1
2019-02-03 12:28:12,879 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0002/job_1549176826207_0002_1.jhist
2019-02-03 12:28:12,879 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 185 Total time for transactions(ms): 14 Number of transactions batched in Syncs: 4 Number of syncs: 132 SyncTimes(ms): 1923 
2019-02-03 12:28:12,928 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0002/job_1549176826207_0002_1.jhist for DFSClient_NONMAPREDUCE_846147682_1
2019-02-03 12:28:19,041 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-58_710_4533973256457699223-1/-mr-10000/.hive-staging_hive_2019-02-03_12-27-58_710_4533973256457699223-1/_task_tmp.-ext-10001/_tmp.000000_0
2019-02-03 12:28:19,186 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:28:19,207 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-27-58_710_4533973256457699223-1/-mr-10000/.hive-staging_hive_2019-02-03_12-27-58_710_4533973256457699223-1/_task_tmp.-ext-10001/_tmp.000000_0 is closed by DFSClient_attempt_1549176826207_0002_r_000000_0_-7670390_1
2019-02-03 12:28:19,429 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0002/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_846147682_1
2019-02-03 12:28:19,451 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0002/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_846147682_1
2019-02-03 12:28:19,476 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 13629
2019-02-03 12:28:19,484 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0002/job_1549176826207_0002_1.jhist is closed by DFSClient_NONMAPREDUCE_846147682_1
2019-02-03 12:28:19,499 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549176826207_0002.summary_tmp
2019-02-03 12:28:19,516 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:28:19,529 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549176826207_0002.summary_tmp is closed by DFSClient_NONMAPREDUCE_846147682_1
2019-02-03 12:28:19,580 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741880_1056{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549176826207_0002-1549177080137-root-SELECT+count%28*%29+FROM+stocks+WHERE...%27NASDAQ%27%28Stage-1549177099453-1-1-SUCCEEDED-default-1549177086435.jhist_tmp
2019-02-03 12:28:19,593 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741880_1056{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549176826207_0002-1549177080137-root-SELECT+count%28*%29+FROM+stocks+WHERE...%27NASDAQ%27%28Stage-1549177099453-1-1-SUCCEEDED-default-1549177086435.jhist_tmp
2019-02-03 12:28:19,593 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741880_1056{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 35522
2019-02-03 12:28:20,511 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549176826207_0002-1549177080137-root-SELECT+count%28*%29+FROM+stocks+WHERE...%27NASDAQ%27%28Stage-1549177099453-1-1-SUCCEEDED-default-1549177086435.jhist_tmp is closed by DFSClient_NONMAPREDUCE_846147682_1
2019-02-03 12:28:20,629 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741881_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549176826207_0002_conf.xml_tmp
2019-02-03 12:28:20,701 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741881_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:28:20,807 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549176826207_0002_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_846147682_1
2019-02-03 12:28:21,919 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741872_1048 127.0.0.1:50010 
2019-02-03 12:28:21,919 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741873_1049 127.0.0.1:50010 
2019-02-03 12:28:21,919 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741874_1050 127.0.0.1:50010 
2019-02-03 12:28:21,919 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741875_1051 127.0.0.1:50010 
2019-02-03 12:28:21,920 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741877_1053 127.0.0.1:50010 
2019-02-03 12:28:21,920 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741876_1052 127.0.0.1:50010 
2019-02-03 12:28:22,152 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741870_1046 127.0.0.1:50010 
2019-02-03 12:28:22,163 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741871_1047 127.0.0.1:50010 
2019-02-03 12:28:22,207 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741878_1054 127.0.0.1:50010 
2019-02-03 12:28:24,667 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741872_1048, blk_1073741873_1049, blk_1073741874_1050, blk_1073741875_1051, blk_1073741876_1052, blk_1073741877_1053, blk_1073741878_1054, blk_1073741870_1046, blk_1073741871_1047]
2019-02-03 12:29:15,250 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 227 Total time for transactions(ms): 18 Number of transactions batched in Syncs: 6 Number of syncs: 165 SyncTimes(ms): 2936 
2019-02-03 12:29:15,405 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-29-15_219_7554058745246953103-1/-mr-10004/28dde30e-1314-4478-8c55-d7312415a7bd/map.xml
2019-02-03 12:29:15,419 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:29:15,423 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-29-15_219_7554058745246953103-1/-mr-10004/28dde30e-1314-4478-8c55-d7312415a7bd/map.xml is closed by DFSClient_NONMAPREDUCE_-1837917950_1
2019-02-03 12:29:15,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-29-15_219_7554058745246953103-1/-mr-10004/28dde30e-1314-4478-8c55-d7312415a7bd/map.xml
2019-02-03 12:29:15,452 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741883_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-29-15_219_7554058745246953103-1/-mr-10004/28dde30e-1314-4478-8c55-d7312415a7bd/reduce.xml
2019-02-03 12:29:15,462 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741883_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:29:15,467 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-29-15_219_7554058745246953103-1/-mr-10004/28dde30e-1314-4478-8c55-d7312415a7bd/reduce.xml is closed by DFSClient_NONMAPREDUCE_-1837917950_1
2019-02-03 12:29:15,469 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-29-15_219_7554058745246953103-1/-mr-10004/28dde30e-1314-4478-8c55-d7312415a7bd/reduce.xml
2019-02-03 12:29:15,580 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741884_1060{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0003/job.jar
2019-02-03 12:29:15,677 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741884_1060{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0003/job.jar
2019-02-03 12:29:15,677 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741884_1060{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 20599030
2019-02-03 12:29:16,090 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0003/job.jar is closed by DFSClient_NONMAPREDUCE_-1837917950_1
2019-02-03 12:29:16,093 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0003/job.jar
2019-02-03 12:29:16,158 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0003/job.split
2019-02-03 12:29:16,170 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741885_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0003/job.split
2019-02-03 12:29:16,189 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741885_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:29:16,201 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0003/job.split is closed by DFSClient_NONMAPREDUCE_-1837917950_1
2019-02-03 12:29:16,226 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0003/job.splitmetainfo
2019-02-03 12:29:16,239 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:29:16,245 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0003/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1837917950_1
2019-02-03 12:29:16,284 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0003/job.xml
2019-02-03 12:29:16,296 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:29:16,301 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0003/job.xml is closed by DFSClient_NONMAPREDUCE_-1837917950_1
2019-02-03 12:29:22,133 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741888_1064{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0003/job_1549176826207_0003_1_conf.xml
2019-02-03 12:29:22,261 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741888_1064{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:29:22,268 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0003/job_1549176826207_0003_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-690549879_1
2019-02-03 12:29:27,916 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741889_1065{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0003/job_1549176826207_0003_1.jhist
2019-02-03 12:29:27,944 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0003/job_1549176826207_0003_1.jhist for DFSClient_NONMAPREDUCE_-690549879_1
2019-02-03 12:29:33,554 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741890_1066{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-29-15_219_7554058745246953103-1/-mr-10000/.hive-staging_hive_2019-02-03_12-29-15_219_7554058745246953103-1/_task_tmp.-ext-10001/_tmp.000000_0
2019-02-03 12:29:33,679 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741890_1066{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:29:33,691 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/0a1584aa-0fa1-46e9-aa69-105cc2620893/hive_2019-02-03_12-29-15_219_7554058745246953103-1/-mr-10000/.hive-staging_hive_2019-02-03_12-29-15_219_7554058745246953103-1/_task_tmp.-ext-10001/_tmp.000000_0 is closed by DFSClient_attempt_1549176826207_0003_r_000000_0_178397264_1
2019-02-03 12:29:33,902 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0003/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-690549879_1
2019-02-03 12:29:33,925 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0003/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-690549879_1
2019-02-03 12:29:33,947 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741889_1065{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 13636
2019-02-03 12:29:33,958 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549176826207_0003/job_1549176826207_0003_1.jhist is closed by DFSClient_NONMAPREDUCE_-690549879_1
2019-02-03 12:29:33,973 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741891_1067{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549176826207_0003.summary_tmp
2019-02-03 12:29:33,993 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741891_1067{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:29:34,002 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549176826207_0003.summary_tmp is closed by DFSClient_NONMAPREDUCE_-690549879_1
2019-02-03 12:29:34,053 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741892_1068{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549176826207_0003-1549177156319-root-SELECT+avg%28price_close%29+FROM+stoc...%27NASDAQ%27%28Stage-1549177173926-1-1-SUCCEEDED-default-1549177161854.jhist_tmp
2019-02-03 12:29:34,067 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741892_1068{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:29:34,080 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549176826207_0003-1549177156319-root-SELECT+avg%28price_close%29+FROM+stoc...%27NASDAQ%27%28Stage-1549177173926-1-1-SUCCEEDED-default-1549177161854.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-690549879_1
2019-02-03 12:29:34,122 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741893_1069{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549176826207_0003_conf.xml_tmp
2019-02-03 12:29:34,136 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741893_1069{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-03 12:29:34,147 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549176826207_0003_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-690549879_1
2019-02-03 12:29:35,270 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741884_1060 127.0.0.1:50010 
2019-02-03 12:29:35,270 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741885_1061 127.0.0.1:50010 
2019-02-03 12:29:35,270 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741886_1062 127.0.0.1:50010 
2019-02-03 12:29:35,270 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741887_1063 127.0.0.1:50010 
2019-02-03 12:29:35,271 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741889_1065 127.0.0.1:50010 
2019-02-03 12:29:35,271 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741888_1064 127.0.0.1:50010 
2019-02-03 12:29:36,676 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741882_1058 127.0.0.1:50010 
2019-02-03 12:29:36,679 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741888_1064, blk_1073741889_1065, blk_1073741882_1058, blk_1073741884_1060, blk_1073741885_1061, blk_1073741886_1062, blk_1073741887_1063]
2019-02-03 12:29:36,703 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741883_1059 127.0.0.1:50010 
2019-02-03 12:29:36,769 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741890_1066 127.0.0.1:50010 
2019-02-03 12:29:39,679 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741890_1066, blk_1073741883_1059]
2019-02-03 12:31:34,436 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 326 Total time for transactions(ms): 22 Number of transactions batched in Syncs: 8 Number of syncs: 237 SyncTimes(ms): 3965 
2019-02-03 12:31:48,812 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2019-02-03 12:31:48,815 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu-HP-Pavilion-dv6-NoteBook-PC/127.0.1.1
************************************************************/
2019-02-04 16:01:29,236 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu-HP-Pavilion-dv6-NoteBook-PC/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /opt/hadoop/hadoop-2.7.3/etc/hadoop:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_171
************************************************************/
2019-02-04 16:01:29,289 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-04 16:01:29,295 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-02-04 16:01:29,867 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-04 16:01:29,992 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-02-04 16:01:29,992 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-02-04 16:01:29,996 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2019-02-04 16:01:29,998 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2019-02-04 16:01:30,381 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2019-02-04 16:01:30,483 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-02-04 16:01:30,503 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-04 16:01:30,530 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-02-04 16:01:30,535 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-04 16:01:30,538 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-02-04 16:01:30,538 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-04 16:01:30,538 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-04 16:01:30,853 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-02-04 16:01:30,862 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-02-04 16:01:30,878 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2019-02-04 16:01:30,878 INFO org.mortbay.log: jetty-6.1.26
2019-02-04 16:01:31,187 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2019-02-04 16:01:31,330 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-04 16:01:31,330 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-04 16:01:31,444 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-02-04 16:01:31,444 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-02-04 16:01:31,657 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-02-04 16:01:31,657 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-02-04 16:01:31,658 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-02-04 16:01:31,659 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Feb 04 16:01:31
2019-02-04 16:01:31,660 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-02-04 16:01:31,660 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-04 16:01:31,672 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2019-02-04 16:01:31,672 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-02-04 16:01:31,707 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-02-04 16:01:31,707 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2019-02-04 16:01:31,708 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-02-04 16:01:31,708 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-02-04 16:01:31,708 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-02-04 16:01:31,708 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-02-04 16:01:31,708 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-02-04 16:01:31,708 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-02-04 16:01:31,717 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-02-04 16:01:31,717 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-02-04 16:01:31,717 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2019-02-04 16:01:31,718 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-02-04 16:01:31,720 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-02-04 16:01:32,010 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-02-04 16:01:32,010 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-04 16:01:32,011 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2019-02-04 16:01:32,011 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-02-04 16:01:32,011 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-02-04 16:01:32,011 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-02-04 16:01:32,011 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-02-04 16:01:32,012 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-02-04 16:01:32,032 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-02-04 16:01:32,032 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-04 16:01:32,033 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2019-02-04 16:01:32,033 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2019-02-04 16:01:32,035 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-02-04 16:01:32,035 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-02-04 16:01:32,035 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-02-04 16:01:32,047 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-02-04 16:01:32,047 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-02-04 16:01:32,047 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-02-04 16:01:32,050 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-02-04 16:01:32,050 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-02-04 16:01:32,063 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-02-04 16:01:32,063 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-04 16:01:32,063 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2019-02-04 16:01:32,063 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2019-02-04 16:01:32,212 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/ubuntu/hdata/dfs/name/in_use.lock acquired by nodename 8041@ubuntu-HP-Pavilion-dv6-NoteBook-PC
2019-02-04 16:01:32,365 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/ubuntu/hdata/dfs/name/current
2019-02-04 16:01:32,643 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/ubuntu/hdata/dfs/name/current/edits_inprogress_0000000000000000301 -> /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000301-0000000000000000626
2019-02-04 16:01:32,678 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000300, cpktTxId=0000000000000000300)
2019-02-04 16:01:32,837 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 52 INodes.
2019-02-04 16:01:32,906 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-02-04 16:01:32,906 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 300 from /home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000300
2019-02-04 16:01:32,906 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@758a34ce expecting start txid #301
2019-02-04 16:01:32,907 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000301-0000000000000000626
2019-02-04 16:01:32,909 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/ubuntu/hdata/dfs/name/current/edits_0000000000000000301-0000000000000000626' to transaction ID 301
2019-02-04 16:01:33,087 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000301-0000000000000000626 of size 1048576 edits # 326 loaded in 0 seconds
2019-02-04 16:01:33,088 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
2019-02-04 16:01:33,088 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
2019-02-04 16:01:33,094 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /home/ubuntu/hdata/dfs/name/current/fsimage.ckpt_0000000000000000626 using no compression
2019-02-04 16:01:33,155 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /home/ubuntu/hdata/dfs/name/current/fsimage.ckpt_0000000000000000626 of size 5314 bytes saved in 0 seconds.
2019-02-04 16:01:33,205 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 300
2019-02-04 16:01:33,205 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000007, cpktTxId=0000000000000000007)
2019-02-04 16:01:33,360 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 627
2019-02-04 16:01:33,637 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-02-04 16:01:33,637 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 1569 msecs
2019-02-04 16:01:34,067 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2019-02-04 16:01:34,088 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2019-02-04 16:01:34,130 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2019-02-04 16:01:34,204 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2019-02-04 16:01:34,243 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 1
2019-02-04 16:01:34,243 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 1
2019-02-04 16:01:34,244 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 27 blocks to reach the threshold 0.9990 of total blocks 27.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2019-02-04 16:01:34,254 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-02-04 16:01:34,368 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2019-02-04 16:01:34,368 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2019-02-04 16:01:34,357 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-04 16:01:34,359 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2019-02-04 16:01:34,404 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-02-04 16:01:37,632 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=50bf9075-0a2a-4a3b-a9e7-9154c919bf09, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-44195f4d-20ee-4085-84d3-18859dbea129;nsid=1647206854;c=0) storage 50bf9075-0a2a-4a3b-a9e7-9154c919bf09
2019-02-04 16:01:37,632 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-02-04 16:01:37,635 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2019-02-04 16:01:37,815 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-02-04 16:01:37,815 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1 for DN 127.0.0.1:50010
2019-02-04 16:01:37,909 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 26 has reached the threshold 0.9990 of total blocks 27. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2019-02-04 16:01:37,909 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2019-02-04 16:01:37,913 INFO BlockStateChange: BLOCK* processReport: from storage DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=50bf9075-0a2a-4a3b-a9e7-9154c919bf09, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-44195f4d-20ee-4085-84d3-18859dbea129;nsid=1647206854;c=0), blocks: 27, hasStaleStorage: false, processing time: 15 msecs
2019-02-04 16:01:37,919 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 28
2019-02-04 16:01:37,921 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2019-02-04 16:01:37,921 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 6
2019-02-04 16:01:37,921 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2019-02-04 16:01:37,921 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 1
2019-02-04 16:01:37,921 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 10 msec
2019-02-04 16:01:57,920 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 27 has reached the threshold 0.9990 of total blocks 27. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2019-02-04 16:02:07,933 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 36 secs
2019-02-04 16:02:07,933 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2019-02-04 16:02:07,933 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2019-02-04 16:02:07,934 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 6 blocks
2019-02-04 16:02:43,002 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2019-02-04 16:02:43,002 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-02-04 16:02:43,002 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 627
2019-02-04 16:02:43,002 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 103 
2019-02-04 16:02:43,018 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 119 
2019-02-04 16:02:43,022 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/ubuntu/hdata/dfs/name/current/edits_inprogress_0000000000000000627 -> /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000627-0000000000000000628
2019-02-04 16:02:43,023 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 629
2019-02-04 16:02:44,883 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.06s at 80.65 KB/s
2019-02-04 16:02:44,884 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000628 size 5314 bytes.
2019-02-04 16:02:44,942 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 626
2019-02-04 16:02:44,943 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000300, cpktTxId=0000000000000000300)
2019-02-04 16:12:20,450 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 4 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 120 
2019-02-04 17:01:34,768 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: [Lease.  Holder: DFSClient_NONMAPREDUCE_1212752652_1, pendingcreates: 2] has expired hard limit
2019-02-04 17:01:34,769 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering [Lease.  Holder: DFSClient_NONMAPREDUCE_1212752652_1, pendingcreates: 2], src=/tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0002/job_1549167607896_0002_2.jhist
2019-02-04 17:01:34,773 WARN org.apache.hadoop.hdfs.StateChange: BLOCK* internalReleaseLease: All existing blocks are COMPLETE, lease removed, file closed.
2019-02-04 17:01:34,773 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering [Lease.  Holder: DFSClient_NONMAPREDUCE_1212752652_1, pendingcreates: 1], src=/tmp/hadoop-yarn/staging/root/.staging/job_1549167607896_0002/job_1549167607896_0002_2_conf.xml
2019-02-04 17:01:34,775 WARN org.apache.hadoop.hdfs.StateChange: BLOCK* internalReleaseLease: Removed empty last block and closed file.
2019-02-04 17:02:45,417 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2019-02-04 17:02:45,417 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-02-04 17:02:45,418 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 629
2019-02-04 17:02:45,418 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 5 Total time for transactions(ms): 8 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 144 
2019-02-04 17:02:45,460 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 5 Total time for transactions(ms): 8 Number of transactions batched in Syncs: 0 Number of syncs: 4 SyncTimes(ms): 186 
2019-02-04 17:02:45,462 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/ubuntu/hdata/dfs/name/current/edits_inprogress_0000000000000000629 -> /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000629-0000000000000000633
2019-02-04 17:02:45,463 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 634
2019-02-04 17:02:45,832 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 74.07 KB/s
2019-02-04 17:02:45,833 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000633 size 5055 bytes.
2019-02-04 17:02:45,879 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 628
2019-02-04 17:02:45,879 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000626, cpktTxId=0000000000000000626)
2019-02-04 17:08:58,101 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 81 
2019-02-04 18:02:45,919 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2019-02-04 18:02:45,919 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-02-04 18:02:45,919 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 634
2019-02-04 18:02:45,919 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 4 SyncTimes(ms): 137 
2019-02-04 18:02:45,955 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 5 SyncTimes(ms): 173 
2019-02-04 18:02:45,957 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/ubuntu/hdata/dfs/name/current/edits_inprogress_0000000000000000634 -> /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000634-0000000000000000637
2019-02-04 18:02:45,957 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 638
2019-02-04 18:02:46,335 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.06s at 83.33 KB/s
2019-02-04 18:02:46,335 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000637 size 5215 bytes.
2019-02-04 18:02:46,381 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 633
2019-02-04 18:02:46,381 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000628, cpktTxId=0000000000000000628)
2019-02-04 19:02:46,830 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2019-02-04 19:02:46,831 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-02-04 19:02:46,831 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 638
2019-02-04 19:02:46,831 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 99 
2019-02-04 19:02:46,863 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 130 
2019-02-04 19:02:46,865 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/ubuntu/hdata/dfs/name/current/edits_inprogress_0000000000000000638 -> /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000638-0000000000000000639
2019-02-04 19:02:46,865 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 640
2019-02-04 19:02:47,223 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.06s at 79.37 KB/s
2019-02-04 19:02:47,224 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000639 size 5215 bytes.
2019-02-04 19:02:47,281 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 637
2019-02-04 19:02:47,281 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000633, cpktTxId=0000000000000000633)
2019-02-04 20:02:08,200 INFO BlockStateChange: BLOCK* processReport: from storage DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=50bf9075-0a2a-4a3b-a9e7-9154c919bf09, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-44195f4d-20ee-4085-84d3-18859dbea129;nsid=1647206854;c=0), blocks: 27, hasStaleStorage: false, processing time: 3 msecs
2019-02-04 20:02:47,697 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2019-02-04 20:02:47,698 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-02-04 20:02:47,698 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 640
2019-02-04 20:02:47,698 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 80 
2019-02-04 20:02:47,760 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 141 
2019-02-04 20:02:47,761 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/ubuntu/hdata/dfs/name/current/edits_inprogress_0000000000000000640 -> /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000640-0000000000000000641
2019-02-04 20:02:47,762 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 642
2019-02-04 20:02:48,122 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 100.00 KB/s
2019-02-04 20:02:48,122 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000641 size 5215 bytes.
2019-02-04 20:02:48,179 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 639
2019-02-04 20:02:48,180 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000637, cpktTxId=0000000000000000637)
2019-02-04 20:31:01,179 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 82 
2019-02-04 21:02:48,997 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2019-02-04 21:02:48,998 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-02-04 21:02:48,998 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 642
2019-02-04 21:02:48,998 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 3 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 98 
2019-02-04 21:02:49,028 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 3 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 4 SyncTimes(ms): 127 
2019-02-04 21:02:49,030 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/ubuntu/hdata/dfs/name/current/edits_inprogress_0000000000000000642 -> /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000642-0000000000000000644
2019-02-04 21:02:49,030 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 645
2019-02-04 21:02:49,369 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.06s at 64.52 KB/s
2019-02-04 21:02:49,370 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000644 size 5055 bytes.
2019-02-04 21:02:49,426 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 641
2019-02-04 21:02:49,427 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000639, cpktTxId=0000000000000000639)
2019-02-04 22:02:49,798 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2019-02-04 22:02:49,798 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-02-04 22:02:49,799 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 645
2019-02-04 22:02:49,799 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 76 
2019-02-04 22:02:49,831 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 108 
2019-02-04 22:02:49,833 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/ubuntu/hdata/dfs/name/current/edits_inprogress_0000000000000000645 -> /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000645-0000000000000000646
2019-02-04 22:02:49,833 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 647
2019-02-04 22:02:50,167 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 75.47 KB/s
2019-02-04 22:02:50,167 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000646 size 5055 bytes.
2019-02-04 22:02:50,213 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 644
2019-02-04 22:02:50,213 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000641, cpktTxId=0000000000000000641)
2019-02-04 23:02:50,026 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2019-02-04 23:02:50,027 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-02-04 23:02:50,027 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 647
2019-02-04 23:02:50,027 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 68 
2019-02-04 23:02:50,061 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 101 
2019-02-04 23:02:50,062 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/ubuntu/hdata/dfs/name/current/edits_inprogress_0000000000000000647 -> /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000647-0000000000000000648
2019-02-04 23:02:50,062 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 649
2019-02-04 23:02:50,353 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 78.43 KB/s
2019-02-04 23:02:50,354 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000648 size 5055 bytes.
2019-02-04 23:02:50,388 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 646
2019-02-04 23:02:50,388 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000644, cpktTxId=0000000000000000644)
2019-02-04 23:22:05,918 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2019-02-04 23:22:05,962 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu-HP-Pavilion-dv6-NoteBook-PC/127.0.1.1
************************************************************/
2019-02-08 18:13:33,524 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu-HP-Pavilion-dv6-NoteBook-PC/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /opt/hadoop/hadoop-2.7.3/etc/hadoop:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_171
************************************************************/
2019-02-08 18:13:33,583 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-08 18:13:33,590 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-02-08 18:13:34,334 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-08 18:13:34,501 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-02-08 18:13:34,501 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-02-08 18:13:34,506 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2019-02-08 18:13:34,507 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2019-02-08 18:13:35,012 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2019-02-08 18:13:35,125 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-02-08 18:13:35,145 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-08 18:13:35,173 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-02-08 18:13:35,187 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-08 18:13:35,193 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-02-08 18:13:35,193 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-08 18:13:35,193 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-08 18:13:35,569 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-02-08 18:13:35,582 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-02-08 18:13:35,605 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2019-02-08 18:13:35,605 INFO org.mortbay.log: jetty-6.1.26
2019-02-08 18:13:36,066 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2019-02-08 18:13:36,236 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-08 18:13:36,236 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-08 18:13:36,361 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-02-08 18:13:36,361 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-02-08 18:13:36,578 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-02-08 18:13:36,578 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-02-08 18:13:36,580 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-02-08 18:13:36,581 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Feb 08 18:13:36
2019-02-08 18:13:36,584 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-02-08 18:13:36,584 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-08 18:13:36,592 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2019-02-08 18:13:36,592 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-02-08 18:13:36,626 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-02-08 18:13:36,626 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2019-02-08 18:13:36,626 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-02-08 18:13:36,626 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-02-08 18:13:36,627 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-02-08 18:13:36,627 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-02-08 18:13:36,627 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-02-08 18:13:36,627 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-02-08 18:13:36,649 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-02-08 18:13:36,649 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-02-08 18:13:36,649 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2019-02-08 18:13:36,650 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-02-08 18:13:36,655 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-02-08 18:13:37,048 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-02-08 18:13:37,048 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-08 18:13:37,048 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2019-02-08 18:13:37,049 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-02-08 18:13:37,050 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-02-08 18:13:37,050 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-02-08 18:13:37,050 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-02-08 18:13:37,050 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-02-08 18:13:37,076 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-02-08 18:13:37,076 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-08 18:13:37,077 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2019-02-08 18:13:37,077 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2019-02-08 18:13:37,080 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-02-08 18:13:37,080 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-02-08 18:13:37,080 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-02-08 18:13:37,091 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-02-08 18:13:37,091 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-02-08 18:13:37,091 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-02-08 18:13:37,094 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-02-08 18:13:37,094 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-02-08 18:13:37,107 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-02-08 18:13:37,107 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-08 18:13:37,107 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2019-02-08 18:13:37,107 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2019-02-08 18:13:37,243 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/ubuntu/hdata/dfs/name/in_use.lock acquired by nodename 7434@ubuntu-HP-Pavilion-dv6-NoteBook-PC
2019-02-08 18:13:37,533 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/ubuntu/hdata/dfs/name/current
2019-02-08 18:13:37,646 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/ubuntu/hdata/dfs/name/current/edits_inprogress_0000000000000000649 -> /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000649-0000000000000000649
2019-02-08 18:13:37,686 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000648, cpktTxId=0000000000000000648)
2019-02-08 18:13:37,899 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 60 INodes.
2019-02-08 18:13:38,186 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-02-08 18:13:38,187 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 648 from /home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000648
2019-02-08 18:13:38,187 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3401a114 expecting start txid #649
2019-02-08 18:13:38,187 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000649-0000000000000000649
2019-02-08 18:13:38,191 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/ubuntu/hdata/dfs/name/current/edits_0000000000000000649-0000000000000000649' to transaction ID 649
2019-02-08 18:13:38,199 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000649-0000000000000000649 of size 1048576 edits # 1 loaded in 0 seconds
2019-02-08 18:13:38,232 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
2019-02-08 18:13:38,232 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
2019-02-08 18:13:38,253 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /home/ubuntu/hdata/dfs/name/current/fsimage.ckpt_0000000000000000649 using no compression
2019-02-08 18:13:38,369 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /home/ubuntu/hdata/dfs/name/current/fsimage.ckpt_0000000000000000649 of size 5055 bytes saved in 0 seconds.
2019-02-08 18:13:38,426 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 648
2019-02-08 18:13:38,427 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000646, cpktTxId=0000000000000000646)
2019-02-08 18:13:38,560 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 650
2019-02-08 18:13:38,969 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-02-08 18:13:38,969 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 1855 msecs
2019-02-08 18:13:39,569 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2019-02-08 18:13:39,594 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2019-02-08 18:13:39,646 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2019-02-08 18:13:39,814 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2019-02-08 18:13:39,878 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2019-02-08 18:13:39,878 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2019-02-08 18:13:39,879 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 27 blocks to reach the threshold 0.9990 of total blocks 27.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2019-02-08 18:13:39,893 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-02-08 18:13:40,016 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-08 18:13:40,035 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2019-02-08 18:13:40,076 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2019-02-08 18:13:40,076 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2019-02-08 18:13:40,083 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-02-08 18:13:44,459 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=50bf9075-0a2a-4a3b-a9e7-9154c919bf09, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-44195f4d-20ee-4085-84d3-18859dbea129;nsid=1647206854;c=0) storage 50bf9075-0a2a-4a3b-a9e7-9154c919bf09
2019-02-08 18:13:44,461 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-02-08 18:13:44,463 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2019-02-08 18:13:44,708 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-02-08 18:13:44,709 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1 for DN 127.0.0.1:50010
2019-02-08 18:13:44,989 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 26 has reached the threshold 0.9990 of total blocks 27. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2019-02-08 18:13:44,989 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2019-02-08 18:13:44,993 INFO BlockStateChange: BLOCK* processReport: from storage DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=50bf9075-0a2a-4a3b-a9e7-9154c919bf09, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-44195f4d-20ee-4085-84d3-18859dbea129;nsid=1647206854;c=0), blocks: 27, hasStaleStorage: false, processing time: 32 msecs
2019-02-08 18:13:45,016 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 27
2019-02-08 18:13:45,016 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2019-02-08 18:13:45,017 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 6
2019-02-08 18:13:45,018 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2019-02-08 18:13:45,018 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2019-02-08 18:13:45,019 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 11 msec
2019-02-08 18:14:04,998 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 27 has reached the threshold 0.9990 of total blocks 27. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2019-02-08 18:14:15,004 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 38 secs
2019-02-08 18:14:15,004 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2019-02-08 18:14:15,004 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2019-02-08 18:14:15,004 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 6 blocks
2019-02-08 18:14:48,703 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2019-02-08 18:14:48,703 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-02-08 18:14:48,703 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 650
2019-02-08 18:14:48,703 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 124 
2019-02-08 18:14:48,714 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 135 
2019-02-08 18:14:48,716 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/ubuntu/hdata/dfs/name/current/edits_inprogress_0000000000000000650 -> /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000650-0000000000000000651
2019-02-08 18:14:48,717 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 652
2019-02-08 18:14:51,026 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 76.92 KB/s
2019-02-08 18:14:51,027 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000651 size 5055 bytes.
2019-02-08 18:14:51,072 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 649
2019-02-08 18:14:51,072 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000648, cpktTxId=0000000000000000648)
2019-02-08 18:16:09,296 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 137 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 86 
2019-02-08 18:18:01,474 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4 Total time for transactions(ms): 138 Number of transactions batched in Syncs: 0 Number of syncs: 4 SyncTimes(ms): 160 
2019-02-08 18:20:58,880 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 6 Total time for transactions(ms): 139 Number of transactions batched in Syncs: 0 Number of syncs: 5 SyncTimes(ms): 178 
2019-02-08 18:21:04,084 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741894_1070{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /lib/hive/csv-serde-1.1.2-0.11.0-all.jar._COPYING_
2019-02-08 18:21:04,779 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741894_1070{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /lib/hive/csv-serde-1.1.2-0.11.0-all.jar._COPYING_
2019-02-08 18:21:04,779 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2019-02-08 18:21:04,810 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741894_1070{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 25153
2019-02-08 18:21:05,213 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /lib/hive/csv-serde-1.1.2-0.11.0-all.jar._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1773268848_1
2019-02-08 18:22:41,023 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 13 Total time for transactions(ms): 141 Number of transactions batched in Syncs: 0 Number of syncs: 11 SyncTimes(ms): 269 
2019-02-08 18:24:36,919 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 14 Total time for transactions(ms): 141 Number of transactions batched in Syncs: 0 Number of syncs: 12 SyncTimes(ms): 287 
2019-02-08 18:27:01,149 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 16 Total time for transactions(ms): 141 Number of transactions batched in Syncs: 0 Number of syncs: 14 SyncTimes(ms): 361 
2019-02-08 18:28:02,249 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 20 Total time for transactions(ms): 141 Number of transactions batched in Syncs: 0 Number of syncs: 18 SyncTimes(ms): 431 
2019-02-08 18:33:24,209 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 21 Total time for transactions(ms): 141 Number of transactions batched in Syncs: 0 Number of syncs: 19 SyncTimes(ms): 452 
2019-02-08 18:33:24,315 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741895_1071{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/besant.db/sales_all_years/2009/CogsleyServices-SalesData-2009.csv._COPYING_
2019-02-08 18:33:24,799 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741895_1071{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:33:24,832 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/besant.db/sales_all_years/2009/CogsleyServices-SalesData-2009.csv._COPYING_ is closed by DFSClient_NONMAPREDUCE_-590107738_1
2019-02-08 18:34:08,766 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741896_1072{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/besant.db/sales_all_years/2010/CogsleyServices-SalesData-2010.csv._COPYING_
2019-02-08 18:34:09,346 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741896_1072{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:34:09,368 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/besant.db/sales_all_years/2010/CogsleyServices-SalesData-2010.csv._COPYING_ is closed by DFSClient_NONMAPREDUCE_1542148104_1
2019-02-08 18:34:30,497 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 40 Total time for transactions(ms): 147 Number of transactions batched in Syncs: 1 Number of syncs: 33 SyncTimes(ms): 613 
2019-02-08 18:34:30,621 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741897_1073{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/besant.db/sales_all_years/2011/CogsleyServices-SalesData-2011.csv._COPYING_
2019-02-08 18:34:31,261 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741897_1073{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:34:31,303 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/besant.db/sales_all_years/2011/CogsleyServices-SalesData-2011.csv._COPYING_ is closed by DFSClient_NONMAPREDUCE_487963094_1
2019-02-08 18:34:42,399 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741898_1074{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/besant.db/sales_all_years/2012/CogsleyServices-SalesData-2012.csv._COPYING_
2019-02-08 18:34:42,887 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741898_1074{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:34:42,903 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/besant.db/sales_all_years/2012/CogsleyServices-SalesData-2012.csv._COPYING_ is closed by DFSClient_NONMAPREDUCE_1838363307_1
2019-02-08 18:36:16,977 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 59 Total time for transactions(ms): 148 Number of transactions batched in Syncs: 2 Number of syncs: 47 SyncTimes(ms): 761 
2019-02-08 18:36:26,485 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741899_1075{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/_tmp_space.db/Values__Tmp__Table__1/data_file
2019-02-08 18:36:26,668 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741899_1075{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:36:26,687 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/_tmp_space.db/Values__Tmp__Table__1/data_file is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:36:27,526 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741900_1076{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-36-26_381_7772208081699682762-1/-mr-10004/99b51c66-6dfb-46cd-9422-0636bd3ab6e4/map.xml
2019-02-08 18:36:27,557 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741900_1076{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:36:27,565 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-36-26_381_7772208081699682762-1/-mr-10004/99b51c66-6dfb-46cd-9422-0636bd3ab6e4/map.xml is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:36:27,580 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-36-26_381_7772208081699682762-1/-mr-10004/99b51c66-6dfb-46cd-9422-0636bd3ab6e4/map.xml
2019-02-08 18:36:28,236 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:57500 Call#221 Retry#0: java.io.FileNotFoundException: File does not exist: /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-36-26_381_7772208081699682762-1/-mr-10004/99b51c66-6dfb-46cd-9422-0636bd3ab6e4/reduce.xml
2019-02-08 18:36:28,696 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741901_1077{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0001/libjars/csv-serde-1.1.2-0.11.0-all.jar
2019-02-08 18:36:28,737 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741901_1077{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:36:28,743 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0001/libjars/csv-serde-1.1.2-0.11.0-all.jar is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:36:28,747 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0001/libjars/csv-serde-1.1.2-0.11.0-all.jar
2019-02-08 18:36:28,809 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741902_1078{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0001/job.jar
2019-02-08 18:36:29,598 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741902_1078{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:36:29,621 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0001/job.jar is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:36:29,623 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0001/job.jar
2019-02-08 18:36:29,801 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0001/job.split
2019-02-08 18:36:29,817 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741903_1079{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0001/job.split
2019-02-08 18:36:29,842 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741903_1079{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:36:29,854 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0001/job.split is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:36:29,883 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741904_1080{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0001/job.splitmetainfo
2019-02-08 18:36:29,908 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741904_1080{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:36:29,921 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0001/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:36:30,018 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741905_1081{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0001/job.xml
2019-02-08 18:36:30,094 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741905_1081{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:36:30,110 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0001/job.xml is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:36:51,925 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741906_1082{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0001/job_1549629837143_0001_1_conf.xml
2019-02-08 18:36:52,606 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741906_1082{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:36:52,715 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0001/job_1549629837143_0001_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-716965291_1
2019-02-08 18:37:08,115 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741907_1083{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/besant.db/vip_clients/.hive-staging_hive_2019-02-08_18-36-26_381_7772208081699682762-1/_task_tmp.-ext-10002/_tmp.000000_0
2019-02-08 18:37:08,699 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741907_1083{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:37:08,812 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/besant.db/vip_clients/.hive-staging_hive_2019-02-08_18-36-26_381_7772208081699682762-1/_task_tmp.-ext-10002/_tmp.000000_0 is closed by DFSClient_attempt_1549629837143_0001_m_000000_0_-872818546_1
2019-02-08 18:37:08,921 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741908_1084{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/besant.db/vip_clients/.hive-staging_hive_2019-02-08_18-36-26_381_7772208081699682762-1/-ext-10001/tmpstats-0
2019-02-08 18:37:08,973 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741908_1084{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:37:08,990 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/besant.db/vip_clients/.hive-staging_hive_2019-02-08_18-36-26_381_7772208081699682762-1/-ext-10001/tmpstats-0 is closed by DFSClient_attempt_1549629837143_0001_m_000000_0_-872818546_1
2019-02-08 18:37:09,921 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741909_1085{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0001/job_1549629837143_0001_1.jhist
2019-02-08 18:37:10,073 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0001/job_1549629837143_0001_1.jhist for DFSClient_NONMAPREDUCE_-716965291_1
2019-02-08 18:37:10,090 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0001/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-716965291_1
2019-02-08 18:37:10,167 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0001/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-716965291_1
2019-02-08 18:37:10,373 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741909_1085{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 13950
2019-02-08 18:37:10,390 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0001/job_1549629837143_0001_1.jhist is closed by DFSClient_NONMAPREDUCE_-716965291_1
2019-02-08 18:37:10,423 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741910_1086{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0001.summary_tmp
2019-02-08 18:37:10,575 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741910_1086{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:37:10,590 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0001.summary_tmp is closed by DFSClient_NONMAPREDUCE_-716965291_1
2019-02-08 18:37:10,691 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741911_1087{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0001-1549631190763-root-insert+into+vip_clients+values%0A++++...Inc.%27%29%28Stage-1549631230175-1-0-SUCCEEDED-default-1549631211287.jhist_tmp
2019-02-08 18:37:10,723 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741911_1087{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:37:10,734 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0001-1549631190763-root-insert+into+vip_clients+values%0A++++...Inc.%27%29%28Stage-1549631230175-1-0-SUCCEEDED-default-1549631211287.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-716965291_1
2019-02-08 18:37:10,801 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741912_1088{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0001_conf.xml_tmp
2019-02-08 18:37:10,868 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741912_1088{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:37:10,879 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0001_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-716965291_1
2019-02-08 18:37:12,046 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741902_1078 127.0.0.1:50010 
2019-02-08 18:37:12,046 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741903_1079 127.0.0.1:50010 
2019-02-08 18:37:12,046 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741904_1080 127.0.0.1:50010 
2019-02-08 18:37:12,047 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741905_1081 127.0.0.1:50010 
2019-02-08 18:37:12,047 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741909_1085 127.0.0.1:50010 
2019-02-08 18:37:12,047 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741906_1082 127.0.0.1:50010 
2019-02-08 18:37:12,047 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741901_1077 127.0.0.1:50010 
2019-02-08 18:37:13,139 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741904_1080, blk_1073741905_1081, blk_1073741906_1082, blk_1073741909_1085, blk_1073741901_1077, blk_1073741902_1078, blk_1073741903_1079]
2019-02-08 18:37:13,193 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741900_1076 127.0.0.1:50010 
2019-02-08 18:37:13,835 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741908_1084 127.0.0.1:50010 
2019-02-08 18:37:16,140 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741908_1084, blk_1073741900_1076]
2019-02-08 18:39:01,973 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 173 Total time for transactions(ms): 167 Number of transactions batched in Syncs: 4 Number of syncs: 131 SyncTimes(ms): 1678 
2019-02-08 18:39:02,174 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741913_1089{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/besant.db/clients/clients.csv._COPYING_
2019-02-08 18:39:02,878 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741913_1089{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:39:02,918 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/besant.db/clients/clients.csv._COPYING_ is closed by DFSClient_NONMAPREDUCE_-194999534_1
2019-02-08 18:40:40,075 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 193 Total time for transactions(ms): 169 Number of transactions batched in Syncs: 6 Number of syncs: 147 SyncTimes(ms): 2193 
2019-02-08 18:40:40,596 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741914_1090{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-40-39_995_5050944082205006756-1/-mr-10005/a95f3b84-a8fd-45b0-a2cd-9027cf9a9ea3/map.xml
2019-02-08 18:40:40,626 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741914_1090{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:40:40,635 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-40-39_995_5050944082205006756-1/-mr-10005/a95f3b84-a8fd-45b0-a2cd-9027cf9a9ea3/map.xml is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:40:40,638 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-40-39_995_5050944082205006756-1/-mr-10005/a95f3b84-a8fd-45b0-a2cd-9027cf9a9ea3/map.xml
2019-02-08 18:40:40,671 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741915_1091{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-40-39_995_5050944082205006756-1/-mr-10005/a95f3b84-a8fd-45b0-a2cd-9027cf9a9ea3/reduce.xml
2019-02-08 18:40:40,692 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741915_1091{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:40:40,702 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-40-39_995_5050944082205006756-1/-mr-10005/a95f3b84-a8fd-45b0-a2cd-9027cf9a9ea3/reduce.xml is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:40:40,704 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-40-39_995_5050944082205006756-1/-mr-10005/a95f3b84-a8fd-45b0-a2cd-9027cf9a9ea3/reduce.xml
2019-02-08 18:40:40,917 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741916_1092{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0002/libjars/csv-serde-1.1.2-0.11.0-all.jar
2019-02-08 18:40:40,936 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741916_1092{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:40:40,946 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0002/libjars/csv-serde-1.1.2-0.11.0-all.jar is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:40:40,951 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0002/libjars/csv-serde-1.1.2-0.11.0-all.jar
2019-02-08 18:40:40,987 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741917_1093{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0002/job.jar
2019-02-08 18:40:41,395 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741917_1093{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0002/job.jar
2019-02-08 18:40:41,395 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741917_1093{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 20599030
2019-02-08 18:40:41,802 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0002/job.jar is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:40:41,804 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0002/job.jar
2019-02-08 18:40:41,926 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0002/job.split
2019-02-08 18:40:41,948 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741918_1094{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0002/job.split
2019-02-08 18:40:41,974 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741918_1094{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:40:41,980 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0002/job.split is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:40:42,016 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741919_1095{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0002/job.splitmetainfo
2019-02-08 18:40:42,042 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741919_1095{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0002/job.splitmetainfo
2019-02-08 18:40:42,042 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741919_1095{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 51
2019-02-08 18:40:42,502 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0002/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:40:42,590 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741920_1096{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0002/job.xml
2019-02-08 18:40:42,630 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741920_1096{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:40:42,646 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0002/job.xml is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:40:58,636 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741921_1097{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0002/job_1549629837143_0002_1_conf.xml
2019-02-08 18:40:59,213 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741921_1097{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:40:59,260 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0002/job_1549629837143_0002_1_conf.xml is closed by DFSClient_NONMAPREDUCE_518723340_1
2019-02-08 18:41:16,089 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741922_1098{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0002/job_1549629837143_0002_1.jhist
2019-02-08 18:41:16,132 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0002/job_1549629837143_0002_1.jhist for DFSClient_NONMAPREDUCE_518723340_1
2019-02-08 18:41:33,329 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741923_1099{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-40-39_995_5050944082205006756-1/_task_tmp.-mr-10003/_tmp.000000_0
2019-02-08 18:41:34,010 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741923_1099{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:41:34,038 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-40-39_995_5050944082205006756-1/_task_tmp.-mr-10003/_tmp.000000_0 is closed by DFSClient_attempt_1549629837143_0002_r_000000_0_678776963_1
2019-02-08 18:41:34,694 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0002/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_518723340_1
2019-02-08 18:41:34,716 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0002/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_518723340_1
2019-02-08 18:41:34,828 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741922_1098{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 14218
2019-02-08 18:41:34,838 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0002/job_1549629837143_0002_1.jhist is closed by DFSClient_NONMAPREDUCE_518723340_1
2019-02-08 18:41:34,892 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741924_1100{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0002.summary_tmp
2019-02-08 18:41:34,927 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741924_1100{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:41:34,938 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0002.summary_tmp is closed by DFSClient_NONMAPREDUCE_518723340_1
2019-02-08 18:41:35,058 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741925_1101{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0002-1549631442738-root-select%0A++++ordermonthyear+as+OrderMon...desc%28Stage-1549631494719-1-1-SUCCEEDED-default-1549631457655.jhist_tmp
2019-02-08 18:41:35,073 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741925_1101{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:41:35,083 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0002-1549631442738-root-select%0A++++ordermonthyear+as+OrderMon...desc%28Stage-1549631494719-1-1-SUCCEEDED-default-1549631457655.jhist_tmp is closed by DFSClient_NONMAPREDUCE_518723340_1
2019-02-08 18:41:35,128 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741926_1102{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0002_conf.xml_tmp
2019-02-08 18:41:35,168 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741926_1102{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:41:35,183 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0002_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_518723340_1
2019-02-08 18:41:36,316 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741917_1093 127.0.0.1:50010 
2019-02-08 18:41:36,316 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741918_1094 127.0.0.1:50010 
2019-02-08 18:41:36,317 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741919_1095 127.0.0.1:50010 
2019-02-08 18:41:36,317 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741920_1096 127.0.0.1:50010 
2019-02-08 18:41:36,317 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741922_1098 127.0.0.1:50010 
2019-02-08 18:41:36,317 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741921_1097 127.0.0.1:50010 
2019-02-08 18:41:36,317 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741916_1092 127.0.0.1:50010 
2019-02-08 18:41:36,550 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741914_1090 127.0.0.1:50010 
2019-02-08 18:41:36,563 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741915_1091 127.0.0.1:50010 
2019-02-08 18:41:36,672 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741927_1103{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-40-39_995_5050944082205006756-1/-mr-10007/cf17f582-7860-4a1d-b12e-6cfa2edb5319/map.xml
2019-02-08 18:41:36,702 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741927_1103{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-40-39_995_5050944082205006756-1/-mr-10007/cf17f582-7860-4a1d-b12e-6cfa2edb5319/map.xml
2019-02-08 18:41:36,702 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741927_1103{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 2313
2019-02-08 18:41:37,116 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-40-39_995_5050944082205006756-1/-mr-10007/cf17f582-7860-4a1d-b12e-6cfa2edb5319/map.xml is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:41:37,121 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-40-39_995_5050944082205006756-1/-mr-10007/cf17f582-7860-4a1d-b12e-6cfa2edb5319/map.xml
2019-02-08 18:41:37,147 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741928_1104{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-40-39_995_5050944082205006756-1/-mr-10007/cf17f582-7860-4a1d-b12e-6cfa2edb5319/reduce.xml
2019-02-08 18:41:37,171 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741928_1104{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-40-39_995_5050944082205006756-1/-mr-10007/cf17f582-7860-4a1d-b12e-6cfa2edb5319/reduce.xml
2019-02-08 18:41:37,171 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741928_1104{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 3344
2019-02-08 18:41:37,175 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741920_1096, blk_1073741921_1097, blk_1073741922_1098, blk_1073741914_1090, blk_1073741915_1091, blk_1073741916_1092, blk_1073741917_1093, blk_1073741918_1094, blk_1073741919_1095]
2019-02-08 18:41:37,583 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-40-39_995_5050944082205006756-1/-mr-10007/cf17f582-7860-4a1d-b12e-6cfa2edb5319/reduce.xml is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:41:37,585 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-40-39_995_5050944082205006756-1/-mr-10007/cf17f582-7860-4a1d-b12e-6cfa2edb5319/reduce.xml
2019-02-08 18:41:37,776 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741929_1105{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0003/libjars/csv-serde-1.1.2-0.11.0-all.jar
2019-02-08 18:41:37,798 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741929_1105{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:41:37,805 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0003/libjars/csv-serde-1.1.2-0.11.0-all.jar is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:41:37,807 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0003/libjars/csv-serde-1.1.2-0.11.0-all.jar
2019-02-08 18:41:37,830 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741930_1106{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0003/job.jar
2019-02-08 18:41:38,058 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741930_1106{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:41:38,072 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0003/job.jar is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:41:38,074 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0003/job.jar
2019-02-08 18:41:38,230 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0003/job.split
2019-02-08 18:41:38,241 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741931_1107{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0003/job.split
2019-02-08 18:41:38,273 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741931_1107{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:41:38,327 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0003/job.split is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:41:38,363 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741932_1108{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0003/job.splitmetainfo
2019-02-08 18:41:38,403 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741932_1108{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:41:38,427 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0003/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:41:38,496 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741933_1109{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0003/job.xml
2019-02-08 18:41:38,546 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741933_1109{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:41:38,561 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0003/job.xml is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:42:01,565 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 350 Total time for transactions(ms): 182 Number of transactions batched in Syncs: 10 Number of syncs: 260 SyncTimes(ms): 3352 
2019-02-08 18:42:02,133 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741934_1110{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0003/job_1549629837143_0003_1_conf.xml
2019-02-08 18:42:02,861 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741934_1110{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:42:02,873 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0003/job_1549629837143_0003_1_conf.xml is closed by DFSClient_NONMAPREDUCE_552145887_1
2019-02-08 18:42:14,622 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741935_1111{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0003/job_1549629837143_0003_1.jhist
2019-02-08 18:42:14,672 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0003/job_1549629837143_0003_1.jhist for DFSClient_NONMAPREDUCE_552145887_1
2019-02-08 18:42:28,512 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741936_1112{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-40-39_995_5050944082205006756-1/-mr-10000/.hive-staging_hive_2019-02-08_18-40-39_995_5050944082205006756-1/_task_tmp.-ext-10001/_tmp.000000_0
2019-02-08 18:42:28,909 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741936_1112{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:42:28,930 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-40-39_995_5050944082205006756-1/-mr-10000/.hive-staging_hive_2019-02-08_18-40-39_995_5050944082205006756-1/_task_tmp.-ext-10001/_tmp.000000_0 is closed by DFSClient_attempt_1549629837143_0003_r_000000_0_-950210015_1
2019-02-08 18:42:29,564 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0003/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_552145887_1
2019-02-08 18:42:29,586 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0003/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_552145887_1
2019-02-08 18:42:29,656 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741935_1111{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 14242
2019-02-08 18:42:29,664 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0003/job_1549629837143_0003_1.jhist is closed by DFSClient_NONMAPREDUCE_552145887_1
2019-02-08 18:42:29,680 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741937_1113{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0003.summary_tmp
2019-02-08 18:42:29,700 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741937_1113{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:42:29,708 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0003.summary_tmp is closed by DFSClient_NONMAPREDUCE_552145887_1
2019-02-08 18:42:29,794 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741938_1114{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0003-1549631498628-root-select%0A++++ordermonthyear+as+OrderMon...desc%28Stage-1549631549588-1-1-SUCCEEDED-default-1549631521507.jhist_tmp
2019-02-08 18:42:29,808 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741938_1114{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:42:29,830 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0003-1549631498628-root-select%0A++++ordermonthyear+as+OrderMon...desc%28Stage-1549631549588-1-1-SUCCEEDED-default-1549631521507.jhist_tmp is closed by DFSClient_NONMAPREDUCE_552145887_1
2019-02-08 18:42:29,870 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741939_1115{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0003_conf.xml_tmp
2019-02-08 18:42:29,896 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741939_1115{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:42:29,908 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0003_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_552145887_1
2019-02-08 18:42:31,008 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741930_1106 127.0.0.1:50010 
2019-02-08 18:42:31,009 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741931_1107 127.0.0.1:50010 
2019-02-08 18:42:31,009 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741932_1108 127.0.0.1:50010 
2019-02-08 18:42:31,009 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741933_1109 127.0.0.1:50010 
2019-02-08 18:42:31,009 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741935_1111 127.0.0.1:50010 
2019-02-08 18:42:31,009 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741934_1110 127.0.0.1:50010 
2019-02-08 18:42:31,009 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741929_1105 127.0.0.1:50010 
2019-02-08 18:42:31,200 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741929_1105, blk_1073741930_1106, blk_1073741931_1107, blk_1073741932_1108, blk_1073741933_1109, blk_1073741934_1110, blk_1073741935_1111]
2019-02-08 18:42:31,420 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741927_1103 127.0.0.1:50010 
2019-02-08 18:42:31,431 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741928_1104 127.0.0.1:50010 
2019-02-08 18:42:31,564 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741936_1112 127.0.0.1:50010 
2019-02-08 18:42:31,575 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741923_1099 127.0.0.1:50010 
2019-02-08 18:42:34,200 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741936_1112, blk_1073741923_1099, blk_1073741927_1103, blk_1073741928_1104]
2019-02-08 18:43:53,786 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 400 Total time for transactions(ms): 185 Number of transactions batched in Syncs: 11 Number of syncs: 297 SyncTimes(ms): 3827 
2019-02-08 18:44:07,251 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741940_1116{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-43-53_694_6194209807880312917-1/-mr-10005/HashTable-Stage-2/Stage-2.tar.gz
2019-02-08 18:44:07,271 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741940_1116{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:44:07,280 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-43-53_694_6194209807880312917-1/-mr-10005/HashTable-Stage-2/Stage-2.tar.gz is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:44:07,282 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-43-53_694_6194209807880312917-1/-mr-10005/HashTable-Stage-2/Stage-2.tar.gz
2019-02-08 18:44:07,346 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741941_1117{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-43-53_694_6194209807880312917-1/-mr-10009/e2fda910-5e90-48bb-8762-12b2999cdf64/map.xml
2019-02-08 18:44:07,369 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741941_1117{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:44:07,380 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-43-53_694_6194209807880312917-1/-mr-10009/e2fda910-5e90-48bb-8762-12b2999cdf64/map.xml is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:44:07,383 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-43-53_694_6194209807880312917-1/-mr-10009/e2fda910-5e90-48bb-8762-12b2999cdf64/map.xml
2019-02-08 18:44:07,410 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741942_1118{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-43-53_694_6194209807880312917-1/-mr-10009/e2fda910-5e90-48bb-8762-12b2999cdf64/reduce.xml
2019-02-08 18:44:07,424 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741942_1118{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:44:07,436 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-43-53_694_6194209807880312917-1/-mr-10009/e2fda910-5e90-48bb-8762-12b2999cdf64/reduce.xml is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:44:07,439 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-43-53_694_6194209807880312917-1/-mr-10009/e2fda910-5e90-48bb-8762-12b2999cdf64/reduce.xml
2019-02-08 18:44:07,606 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741943_1119{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0004/libjars/csv-serde-1.1.2-0.11.0-all.jar
2019-02-08 18:44:07,623 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741943_1119{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:44:07,636 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0004/libjars/csv-serde-1.1.2-0.11.0-all.jar is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:44:07,637 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0004/libjars/csv-serde-1.1.2-0.11.0-all.jar
2019-02-08 18:44:07,662 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741944_1120{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0004/job.jar
2019-02-08 18:44:07,911 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741944_1120{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:44:07,925 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0004/job.jar is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:44:07,927 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0004/job.jar
2019-02-08 18:44:08,071 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0004/job.split
2019-02-08 18:44:08,082 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741945_1121{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0004/job.split
2019-02-08 18:44:08,101 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741945_1121{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:44:08,114 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0004/job.split is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:44:08,138 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741946_1122{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0004/job.splitmetainfo
2019-02-08 18:44:08,154 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741946_1122{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:44:08,158 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0004/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:44:08,193 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741947_1123{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0004/job.xml
2019-02-08 18:44:08,224 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741947_1123{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0004/job.xml
2019-02-08 18:44:08,224 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741947_1123{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 208176
2019-02-08 18:44:08,636 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0004/job.xml is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:44:22,909 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741948_1124{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0004/job_1549629837143_0004_1_conf.xml
2019-02-08 18:44:23,291 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741948_1124{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:44:23,304 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0004/job_1549629837143_0004_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-465339362_1
2019-02-08 18:44:38,081 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741949_1125{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0004/job_1549629837143_0004_1.jhist
2019-02-08 18:44:38,144 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0004/job_1549629837143_0004_1.jhist for DFSClient_NONMAPREDUCE_-465339362_1
2019-02-08 18:44:50,407 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741950_1126{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-43-53_694_6194209807880312917-1/-mr-10000/.hive-staging_hive_2019-02-08_18-43-53_694_6194209807880312917-1/_task_tmp.-ext-10001/_tmp.000000_0
2019-02-08 18:44:50,892 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741950_1126{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:44:50,927 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-43-53_694_6194209807880312917-1/-mr-10000/.hive-staging_hive_2019-02-08_18-43-53_694_6194209807880312917-1/_task_tmp.-ext-10001/_tmp.000000_0 is closed by DFSClient_attempt_1549629837143_0004_r_000000_0_1993741325_1
2019-02-08 18:44:51,638 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0004/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-465339362_1
2019-02-08 18:44:51,660 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0004/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-465339362_1
2019-02-08 18:44:51,734 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741949_1125{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 13868
2019-02-08 18:44:51,760 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0004/job_1549629837143_0004_1.jhist is closed by DFSClient_NONMAPREDUCE_-465339362_1
2019-02-08 18:44:51,779 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741951_1127{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0004.summary_tmp
2019-02-08 18:44:51,800 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741951_1127{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:44:51,805 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0004.summary_tmp is closed by DFSClient_NONMAPREDUCE_-465339362_1
2019-02-08 18:44:51,877 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741952_1128{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0004-1549631648690-root-select+count%281%29%0Afrom+sales_all_year...c.name%28Stage-1549631691664-1-1-SUCCEEDED-default-1549631662344.jhist_tmp
2019-02-08 18:44:51,899 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741952_1128{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:44:51,927 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0004-1549631648690-root-select+count%281%29%0Afrom+sales_all_year...c.name%28Stage-1549631691664-1-1-SUCCEEDED-default-1549631662344.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-465339362_1
2019-02-08 18:44:51,975 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741953_1129{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0004_conf.xml_tmp
2019-02-08 18:44:52,011 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741953_1129{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:44:52,038 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0004_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-465339362_1
2019-02-08 18:44:53,149 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741944_1120 127.0.0.1:50010 
2019-02-08 18:44:53,150 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741945_1121 127.0.0.1:50010 
2019-02-08 18:44:53,150 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741946_1122 127.0.0.1:50010 
2019-02-08 18:44:53,150 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741947_1123 127.0.0.1:50010 
2019-02-08 18:44:53,150 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741949_1125 127.0.0.1:50010 
2019-02-08 18:44:53,150 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741948_1124 127.0.0.1:50010 
2019-02-08 18:44:53,150 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741943_1119 127.0.0.1:50010 
2019-02-08 18:44:53,616 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741941_1117 127.0.0.1:50010 
2019-02-08 18:44:53,649 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741942_1118 127.0.0.1:50010 
2019-02-08 18:44:53,719 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741950_1126 127.0.0.1:50010 
2019-02-08 18:44:53,727 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741940_1116 127.0.0.1:50010 
2019-02-08 18:44:55,216 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741940_1116, blk_1073741941_1117, blk_1073741942_1118, blk_1073741943_1119, blk_1073741944_1120, blk_1073741945_1121, blk_1073741946_1122, blk_1073741947_1123, blk_1073741948_1124, blk_1073741949_1125, blk_1073741950_1126]
2019-02-08 18:44:56,702 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 515 Total time for transactions(ms): 195 Number of transactions batched in Syncs: 13 Number of syncs: 379 SyncTimes(ms): 4592 
2019-02-08 18:45:13,699 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741954_1130{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-44-56_624_5097965496402863700-1/-mr-10007/HashTable-Stage-2/Stage-2.tar.gz
2019-02-08 18:45:13,722 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741954_1130{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:45:13,728 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-44-56_624_5097965496402863700-1/-mr-10007/HashTable-Stage-2/Stage-2.tar.gz is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:45:13,731 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-44-56_624_5097965496402863700-1/-mr-10007/HashTable-Stage-2/Stage-2.tar.gz
2019-02-08 18:45:13,785 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741955_1131{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-44-56_624_5097965496402863700-1/-mr-10011/32f07ac6-f346-4bd4-994d-2973ec900eef/map.xml
2019-02-08 18:45:13,829 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741955_1131{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:45:13,839 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-44-56_624_5097965496402863700-1/-mr-10011/32f07ac6-f346-4bd4-994d-2973ec900eef/map.xml is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:45:13,843 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-44-56_624_5097965496402863700-1/-mr-10011/32f07ac6-f346-4bd4-994d-2973ec900eef/map.xml
2019-02-08 18:45:13,877 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741956_1132{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-44-56_624_5097965496402863700-1/-mr-10011/32f07ac6-f346-4bd4-994d-2973ec900eef/reduce.xml
2019-02-08 18:45:13,897 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741956_1132{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:45:13,905 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-44-56_624_5097965496402863700-1/-mr-10011/32f07ac6-f346-4bd4-994d-2973ec900eef/reduce.xml is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:45:13,909 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-44-56_624_5097965496402863700-1/-mr-10011/32f07ac6-f346-4bd4-994d-2973ec900eef/reduce.xml
2019-02-08 18:45:14,124 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741957_1133{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0005/libjars/csv-serde-1.1.2-0.11.0-all.jar
2019-02-08 18:45:14,154 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741957_1133{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0005/libjars/csv-serde-1.1.2-0.11.0-all.jar
2019-02-08 18:45:14,155 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741957_1133{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 25153
2019-02-08 18:45:15,012 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0005/libjars/csv-serde-1.1.2-0.11.0-all.jar is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:45:15,015 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0005/libjars/csv-serde-1.1.2-0.11.0-all.jar
2019-02-08 18:45:15,114 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741958_1134{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0005/job.jar
2019-02-08 18:45:15,497 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741958_1134{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:45:15,505 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0005/job.jar is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:45:15,508 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0005/job.jar
2019-02-08 18:45:15,663 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0005/job.split
2019-02-08 18:45:15,675 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741959_1135{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0005/job.split
2019-02-08 18:45:15,706 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741959_1135{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0005/job.split
2019-02-08 18:45:15,708 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741959_1135{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 640
2019-02-08 18:45:16,128 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0005/job.split is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:45:16,153 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741960_1136{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0005/job.splitmetainfo
2019-02-08 18:45:16,180 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741960_1136{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:45:16,196 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0005/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:45:16,241 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741961_1137{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0005/job.xml
2019-02-08 18:45:16,263 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741961_1137{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:45:16,272 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0005/job.xml is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:45:34,497 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741962_1138{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0005/job_1549629837143_0005_1_conf.xml
2019-02-08 18:45:35,480 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741962_1138{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:45:35,506 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0005/job_1549629837143_0005_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-777055992_1
2019-02-08 18:45:50,849 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741963_1139{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0005/job_1549629837143_0005_1.jhist
2019-02-08 18:45:50,920 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0005/job_1549629837143_0005_1.jhist for DFSClient_NONMAPREDUCE_-777055992_1
2019-02-08 18:46:02,531 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 591 Total time for transactions(ms): 203 Number of transactions batched in Syncs: 15 Number of syncs: 430 SyncTimes(ms): 5545 
2019-02-08 18:46:03,576 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741964_1140{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-44-56_624_5097965496402863700-1/_task_tmp.-mr-10005/_tmp.000000_0
2019-02-08 18:46:04,200 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741964_1140{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:46:04,240 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-44-56_624_5097965496402863700-1/_task_tmp.-mr-10005/_tmp.000000_0 is closed by DFSClient_attempt_1549629837143_0005_r_000000_0_-1728010138_1
2019-02-08 18:46:04,774 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0005/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-777055992_1
2019-02-08 18:46:04,840 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0005/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-777055992_1
2019-02-08 18:46:04,964 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741963_1139{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 14242
2019-02-08 18:46:04,973 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0005/job_1549629837143_0005_1.jhist is closed by DFSClient_NONMAPREDUCE_-777055992_1
2019-02-08 18:46:04,990 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741965_1141{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0005.summary_tmp
2019-02-08 18:46:05,015 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741965_1141{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:46:05,062 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0005.summary_tmp is closed by DFSClient_NONMAPREDUCE_-777055992_1
2019-02-08 18:46:05,126 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741966_1142{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0005-1549631716339-root-select+%0A++++c.marketcaplabel%2C+%0A++++c....desc%28Stage-1549631764861-1-1-SUCCEEDED-default-1549631733674.jhist_tmp
2019-02-08 18:46:05,140 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741966_1142{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:46:05,151 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0005-1549631716339-root-select+%0A++++c.marketcaplabel%2C+%0A++++c....desc%28Stage-1549631764861-1-1-SUCCEEDED-default-1549631733674.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-777055992_1
2019-02-08 18:46:05,227 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741967_1143{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0005_conf.xml_tmp
2019-02-08 18:46:05,246 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741967_1143{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:46:05,262 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0005_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-777055992_1
2019-02-08 18:46:06,429 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741958_1134 127.0.0.1:50010 
2019-02-08 18:46:06,430 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741959_1135 127.0.0.1:50010 
2019-02-08 18:46:06,430 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741960_1136 127.0.0.1:50010 
2019-02-08 18:46:06,430 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741961_1137 127.0.0.1:50010 
2019-02-08 18:46:06,430 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741963_1139 127.0.0.1:50010 
2019-02-08 18:46:06,430 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741962_1138 127.0.0.1:50010 
2019-02-08 18:46:06,430 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741957_1133 127.0.0.1:50010 
2019-02-08 18:46:06,707 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741955_1131 127.0.0.1:50010 
2019-02-08 18:46:06,729 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741956_1132 127.0.0.1:50010 
2019-02-08 18:46:06,811 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741968_1144{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-44-56_624_5097965496402863700-1/-mr-10013/226a007f-d0dc-4272-8575-41d59adf96bb/map.xml
2019-02-08 18:46:06,848 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741968_1144{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-44-56_624_5097965496402863700-1/-mr-10013/226a007f-d0dc-4272-8575-41d59adf96bb/map.xml
2019-02-08 18:46:06,848 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741968_1144{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 2444
2019-02-08 18:46:07,233 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741955_1131, blk_1073741956_1132, blk_1073741957_1133, blk_1073741958_1134, blk_1073741959_1135, blk_1073741960_1136, blk_1073741961_1137, blk_1073741962_1138, blk_1073741963_1139]
2019-02-08 18:46:07,263 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-44-56_624_5097965496402863700-1/-mr-10013/226a007f-d0dc-4272-8575-41d59adf96bb/map.xml is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:46:07,265 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-44-56_624_5097965496402863700-1/-mr-10013/226a007f-d0dc-4272-8575-41d59adf96bb/map.xml
2019-02-08 18:46:07,289 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741969_1145{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-44-56_624_5097965496402863700-1/-mr-10013/226a007f-d0dc-4272-8575-41d59adf96bb/reduce.xml
2019-02-08 18:46:07,304 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741969_1145{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-44-56_624_5097965496402863700-1/-mr-10013/226a007f-d0dc-4272-8575-41d59adf96bb/reduce.xml
2019-02-08 18:46:07,305 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741969_1145{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 3534
2019-02-08 18:46:07,718 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-44-56_624_5097965496402863700-1/-mr-10013/226a007f-d0dc-4272-8575-41d59adf96bb/reduce.xml is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:46:07,720 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-44-56_624_5097965496402863700-1/-mr-10013/226a007f-d0dc-4272-8575-41d59adf96bb/reduce.xml
2019-02-08 18:46:07,877 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741970_1146{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0006/libjars/csv-serde-1.1.2-0.11.0-all.jar
2019-02-08 18:46:07,895 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741970_1146{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:46:07,907 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0006/libjars/csv-serde-1.1.2-0.11.0-all.jar is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:46:07,909 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0006/libjars/csv-serde-1.1.2-0.11.0-all.jar
2019-02-08 18:46:07,955 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741971_1147{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0006/job.jar
2019-02-08 18:46:08,151 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741971_1147{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0006/job.jar
2019-02-08 18:46:08,151 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741971_1147{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 20599030
2019-02-08 18:46:08,574 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0006/job.jar is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:46:08,576 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0006/job.jar
2019-02-08 18:46:08,653 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0006/job.split
2019-02-08 18:46:08,665 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741972_1148{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0006/job.split
2019-02-08 18:46:08,684 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741972_1148{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:46:08,696 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0006/job.split is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:46:08,720 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741973_1149{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0006/job.splitmetainfo
2019-02-08 18:46:08,738 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741973_1149{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:46:08,752 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0006/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:46:08,794 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741974_1150{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0006/job.xml
2019-02-08 18:46:08,818 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741974_1150{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:46:08,829 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0006/job.xml is closed by DFSClient_NONMAPREDUCE_582850831_1
2019-02-08 18:46:31,727 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741975_1151{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0006/job_1549629837143_0006_1_conf.xml
2019-02-08 18:46:32,091 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741975_1151{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:46:32,108 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0006/job_1549629837143_0006_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-1534956796_1
2019-02-08 18:46:47,162 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741976_1152{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0006/job_1549629837143_0006_1.jhist
2019-02-08 18:46:47,230 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0006/job_1549629837143_0006_1.jhist for DFSClient_NONMAPREDUCE_-1534956796_1
2019-02-08 18:47:03,270 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 691 Total time for transactions(ms): 214 Number of transactions batched in Syncs: 18 Number of syncs: 502 SyncTimes(ms): 6778 
2019-02-08 18:47:03,530 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741977_1153{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-44-56_624_5097965496402863700-1/-mr-10001/.hive-staging_hive_2019-02-08_18-44-56_624_5097965496402863700-1/_task_tmp.-ext-10002/_tmp.000000_0
2019-02-08 18:47:03,998 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741977_1153{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:47:04,009 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/root/6b858903-342d-476d-936a-68f3d7963ba9/hive_2019-02-08_18-44-56_624_5097965496402863700-1/-mr-10001/.hive-staging_hive_2019-02-08_18-44-56_624_5097965496402863700-1/_task_tmp.-ext-10002/_tmp.000000_0 is closed by DFSClient_attempt_1549629837143_0006_r_000000_0_1246575968_1
2019-02-08 18:47:04,509 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0006/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-1534956796_1
2019-02-08 18:47:04,531 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0006/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1534956796_1
2019-02-08 18:47:04,617 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741976_1152{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0006/job_1549629837143_0006_1.jhist
2019-02-08 18:47:04,617 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741976_1152{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 36234
2019-02-08 18:47:05,042 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1549629837143_0006/job_1549629837143_0006_1.jhist is closed by DFSClient_NONMAPREDUCE_-1534956796_1
2019-02-08 18:47:05,058 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741978_1154{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0006.summary_tmp
2019-02-08 18:47:05,088 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741978_1154{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:47:05,098 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0006.summary_tmp is closed by DFSClient_NONMAPREDUCE_-1534956796_1
2019-02-08 18:47:05,162 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741979_1155{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0006-1549631768901-root-select+%0A++++c.marketcaplabel%2C+%0A++++c....desc%28Stage-1549631824535-1-1-SUCCEEDED-default-1549631790229.jhist_tmp
2019-02-08 18:47:05,217 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741979_1155{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:47:05,242 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0006-1549631768901-root-select+%0A++++c.marketcaplabel%2C+%0A++++c....desc%28Stage-1549631824535-1-1-SUCCEEDED-default-1549631790229.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-1534956796_1
2019-02-08 18:47:05,283 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741980_1156{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0006_conf.xml_tmp
2019-02-08 18:47:05,314 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741980_1156{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2019-02-08 18:47:05,320 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/root/job_1549629837143_0006_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-1534956796_1
2019-02-08 18:47:06,409 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741971_1147 127.0.0.1:50010 
2019-02-08 18:47:06,409 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741972_1148 127.0.0.1:50010 
2019-02-08 18:47:06,409 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741973_1149 127.0.0.1:50010 
2019-02-08 18:47:06,410 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741974_1150 127.0.0.1:50010 
2019-02-08 18:47:06,410 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741976_1152 127.0.0.1:50010 
2019-02-08 18:47:06,410 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741975_1151 127.0.0.1:50010 
2019-02-08 18:47:06,410 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741970_1146 127.0.0.1:50010 
2019-02-08 18:47:07,249 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741970_1146, blk_1073741971_1147, blk_1073741972_1148, blk_1073741973_1149, blk_1073741974_1150, blk_1073741975_1151, blk_1073741976_1152]
2019-02-08 18:47:07,254 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741968_1144 127.0.0.1:50010 
2019-02-08 18:47:07,265 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741969_1145 127.0.0.1:50010 
2019-02-08 18:47:07,376 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741977_1153 127.0.0.1:50010 
2019-02-08 18:47:07,387 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741964_1140 127.0.0.1:50010 
2019-02-08 18:47:07,387 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741954_1130 127.0.0.1:50010 
2019-02-08 18:47:10,250 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741968_1144, blk_1073741969_1145, blk_1073741954_1130, blk_1073741977_1153, blk_1073741964_1140]
2019-02-08 18:58:37,078 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 730 Total time for transactions(ms): 219 Number of transactions batched in Syncs: 20 Number of syncs: 533 SyncTimes(ms): 7070 
2019-02-08 18:58:37,217 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741899_1075 127.0.0.1:50010 
2019-02-08 18:58:37,388 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741899_1075]
2019-02-08 19:11:06,242 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2019-02-08 19:11:06,933 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu-HP-Pavilion-dv6-NoteBook-PC/127.0.1.1
************************************************************/
2019-02-08 20:50:25,858 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu-HP-Pavilion-dv6-NoteBook-PC/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /opt/hadoop/hadoop-2.7.3/etc/hadoop:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_171
************************************************************/
2019-02-08 20:50:25,931 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-08 20:50:25,935 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-02-08 20:50:26,407 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-08 20:50:26,518 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-02-08 20:50:26,518 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-02-08 20:50:26,523 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2019-02-08 20:50:26,524 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2019-02-08 20:50:26,808 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2019-02-08 20:50:26,975 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-02-08 20:50:26,982 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-08 20:50:26,988 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-02-08 20:50:26,994 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-08 20:50:26,996 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-02-08 20:50:26,997 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-08 20:50:26,997 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-08 20:50:27,129 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-02-08 20:50:27,154 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-02-08 20:50:27,170 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2019-02-08 20:50:27,170 INFO org.mortbay.log: jetty-6.1.26
2019-02-08 20:50:27,389 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2019-02-08 20:50:27,475 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-08 20:50:27,475 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-08 20:50:27,513 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-02-08 20:50:27,513 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-02-08 20:50:27,586 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-02-08 20:50:27,586 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-02-08 20:50:27,587 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-02-08 20:50:27,587 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Feb 08 20:50:27
2019-02-08 20:50:27,589 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-02-08 20:50:27,589 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-08 20:50:27,609 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2019-02-08 20:50:27,609 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-02-08 20:50:27,615 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-02-08 20:50:27,615 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2019-02-08 20:50:27,615 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-02-08 20:50:27,615 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-02-08 20:50:27,615 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-02-08 20:50:27,615 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-02-08 20:50:27,615 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-02-08 20:50:27,615 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-02-08 20:50:27,623 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-02-08 20:50:27,623 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-02-08 20:50:27,624 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2019-02-08 20:50:27,624 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-02-08 20:50:27,625 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-02-08 20:50:27,808 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-02-08 20:50:27,808 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-08 20:50:27,808 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2019-02-08 20:50:27,808 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-02-08 20:50:27,809 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-02-08 20:50:27,809 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-02-08 20:50:27,809 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-02-08 20:50:27,809 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-02-08 20:50:27,817 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-02-08 20:50:27,817 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-08 20:50:27,817 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2019-02-08 20:50:27,817 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2019-02-08 20:50:27,818 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-02-08 20:50:27,819 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-02-08 20:50:27,819 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-02-08 20:50:27,821 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-02-08 20:50:27,821 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-02-08 20:50:27,821 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-02-08 20:50:27,823 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-02-08 20:50:27,823 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-02-08 20:50:27,826 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-02-08 20:50:27,826 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-08 20:50:27,826 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2019-02-08 20:50:27,826 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2019-02-08 20:50:27,949 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/ubuntu/hdata/dfs/name/in_use.lock acquired by nodename 4099@ubuntu-HP-Pavilion-dv6-NoteBook-PC
2019-02-08 20:50:28,137 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/ubuntu/hdata/dfs/name/current
2019-02-08 20:50:28,323 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/ubuntu/hdata/dfs/name/current/edits_inprogress_0000000000000000652 -> /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000652-0000000000000001381
2019-02-08 20:50:28,348 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000651, cpktTxId=0000000000000000651)
2019-02-08 20:50:28,488 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 60 INodes.
2019-02-08 20:50:28,537 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-02-08 20:50:28,537 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 651 from /home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000651
2019-02-08 20:50:28,537 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@758a34ce expecting start txid #652
2019-02-08 20:50:28,537 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000652-0000000000000001381
2019-02-08 20:50:28,539 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/ubuntu/hdata/dfs/name/current/edits_0000000000000000652-0000000000000001381' to transaction ID 652
2019-02-08 20:50:28,693 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/ubuntu/hdata/dfs/name/current/edits_0000000000000000652-0000000000000001381 of size 1048576 edits # 730 loaded in 0 seconds
2019-02-08 20:50:28,694 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
2019-02-08 20:50:28,694 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
2019-02-08 20:50:28,700 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /home/ubuntu/hdata/dfs/name/current/fsimage.ckpt_0000000000000001381 using no compression
2019-02-08 20:50:28,756 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /home/ubuntu/hdata/dfs/name/current/fsimage.ckpt_0000000000000001381 of size 8707 bytes saved in 0 seconds.
2019-02-08 20:50:28,808 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 651
2019-02-08 20:50:28,808 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000649, cpktTxId=0000000000000000649)
2019-02-08 20:50:28,890 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1382
2019-02-08 20:50:29,151 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-02-08 20:50:29,151 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 1322 msecs
2019-02-08 20:50:29,516 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2019-02-08 20:50:29,527 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2019-02-08 20:50:29,576 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2019-02-08 20:50:29,628 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2019-02-08 20:50:29,656 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2019-02-08 20:50:29,656 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2019-02-08 20:50:29,657 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 52 blocks to reach the threshold 0.9990 of total blocks 52.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2019-02-08 20:50:29,666 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-02-08 20:50:29,740 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-08 20:50:29,741 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2019-02-08 20:50:29,767 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2019-02-08 20:50:29,767 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2019-02-08 20:50:29,777 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-02-08 20:50:33,184 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=50bf9075-0a2a-4a3b-a9e7-9154c919bf09, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-44195f4d-20ee-4085-84d3-18859dbea129;nsid=1647206854;c=0) storage 50bf9075-0a2a-4a3b-a9e7-9154c919bf09
2019-02-08 20:50:33,184 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-02-08 20:50:33,185 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2019-02-08 20:50:33,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-02-08 20:50:33,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1 for DN 127.0.0.1:50010
2019-02-08 20:50:33,327 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 51 has reached the threshold 0.9990 of total blocks 52. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 30 seconds.
2019-02-08 20:50:33,327 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2019-02-08 20:50:33,328 INFO BlockStateChange: BLOCK* processReport: from storage DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=50bf9075-0a2a-4a3b-a9e7-9154c919bf09, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-44195f4d-20ee-4085-84d3-18859dbea129;nsid=1647206854;c=0), blocks: 52, hasStaleStorage: false, processing time: 9 msecs
2019-02-08 20:50:33,331 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 52
2019-02-08 20:50:33,331 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2019-02-08 20:50:33,331 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 6
2019-02-08 20:50:33,331 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2019-02-08 20:50:33,331 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2019-02-08 20:50:33,331 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 4 msec
2019-02-08 20:50:53,334 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 52 has reached the threshold 0.9990 of total blocks 52. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2019-02-08 20:51:03,336 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 35 secs
2019-02-08 20:51:03,336 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2019-02-08 20:51:03,337 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2019-02-08 20:51:03,337 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 6 blocks
2019-02-08 21:24:35,551 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2019-02-08 21:24:35,553 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu-HP-Pavilion-dv6-NoteBook-PC/127.0.1.1
************************************************************/
2019-02-09 14:45:36,546 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu-HP-Pavilion-dv6-NoteBook-PC/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /opt/hadoop/hadoop-2.7.3/etc/hadoop:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/opt/hadoop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_171
************************************************************/
2019-02-09 14:45:36,590 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-02-09 14:45:36,594 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2019-02-09 14:45:37,001 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-02-09 14:45:37,099 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-02-09 14:45:37,099 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2019-02-09 14:45:37,102 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2019-02-09 14:45:37,102 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2019-02-09 14:45:37,454 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2019-02-09 14:45:37,553 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-02-09 14:45:37,572 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-02-09 14:45:37,607 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2019-02-09 14:45:37,614 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-02-09 14:45:37,616 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-02-09 14:45:37,616 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-02-09 14:45:37,616 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-02-09 14:45:37,934 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-02-09 14:45:37,947 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-02-09 14:45:37,966 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2019-02-09 14:45:37,966 INFO org.mortbay.log: jetty-6.1.26
2019-02-09 14:45:38,165 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2019-02-09 14:45:38,275 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-09 14:45:38,275 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-02-09 14:45:38,330 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-02-09 14:45:38,330 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-02-09 14:45:38,423 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-02-09 14:45:38,423 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-02-09 14:45:38,424 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-02-09 14:45:38,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Feb 09 14:45:38
2019-02-09 14:45:38,426 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-02-09 14:45:38,426 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-09 14:45:38,446 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2019-02-09 14:45:38,446 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-02-09 14:45:38,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-02-09 14:45:38,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2019-02-09 14:45:38,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-02-09 14:45:38,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-02-09 14:45:38,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-02-09 14:45:38,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-02-09 14:45:38,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-02-09 14:45:38,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-02-09 14:45:38,474 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-02-09 14:45:38,474 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-02-09 14:45:38,474 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2019-02-09 14:45:38,474 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-02-09 14:45:38,475 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-02-09 14:45:38,611 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-02-09 14:45:38,611 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-09 14:45:38,611 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2019-02-09 14:45:38,611 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-02-09 14:45:38,612 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-02-09 14:45:38,612 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-02-09 14:45:38,612 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-02-09 14:45:38,612 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-02-09 14:45:38,628 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-02-09 14:45:38,628 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-09 14:45:38,628 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2019-02-09 14:45:38,628 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2019-02-09 14:45:38,630 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-02-09 14:45:38,630 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-02-09 14:45:38,630 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-02-09 14:45:38,643 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-02-09 14:45:38,643 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-02-09 14:45:38,643 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-02-09 14:45:38,644 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2019-02-09 14:45:38,644 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-02-09 14:45:38,660 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2019-02-09 14:45:38,660 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-02-09 14:45:38,660 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2019-02-09 14:45:38,660 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2019-02-09 14:45:38,776 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/ubuntu/hdata/dfs/name/in_use.lock acquired by nodename 10125@ubuntu-HP-Pavilion-dv6-NoteBook-PC
2019-02-09 14:45:38,918 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/ubuntu/hdata/dfs/name/current
2019-02-09 14:45:39,001 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/ubuntu/hdata/dfs/name/current/edits_inprogress_0000000000000001382 -> /home/ubuntu/hdata/dfs/name/current/edits_0000000000000001382-0000000000000001382
2019-02-09 14:45:39,019 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000001381, cpktTxId=0000000000000001381)
2019-02-09 14:45:39,153 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 95 INodes.
2019-02-09 14:45:39,331 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-02-09 14:45:39,331 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 1381 from /home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000001381
2019-02-09 14:45:39,331 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3401a114 expecting start txid #1382
2019-02-09 14:45:39,331 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/ubuntu/hdata/dfs/name/current/edits_0000000000000001382-0000000000000001382
2019-02-09 14:45:39,333 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/ubuntu/hdata/dfs/name/current/edits_0000000000000001382-0000000000000001382' to transaction ID 1382
2019-02-09 14:45:39,335 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/ubuntu/hdata/dfs/name/current/edits_0000000000000001382-0000000000000001382 of size 1048576 edits # 1 loaded in 0 seconds
2019-02-09 14:45:39,360 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
2019-02-09 14:45:39,360 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
2019-02-09 14:45:39,366 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /home/ubuntu/hdata/dfs/name/current/fsimage.ckpt_0000000000000001382 using no compression
2019-02-09 14:45:39,415 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /home/ubuntu/hdata/dfs/name/current/fsimage.ckpt_0000000000000001382 of size 8707 bytes saved in 0 seconds.
2019-02-09 14:45:39,457 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 1381
2019-02-09 14:45:39,458 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000000651, cpktTxId=0000000000000000651)
2019-02-09 14:45:39,518 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1383
2019-02-09 14:45:39,789 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-02-09 14:45:39,790 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 1126 msecs
2019-02-09 14:45:40,195 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2019-02-09 14:45:40,202 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2019-02-09 14:45:40,233 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2019-02-09 14:45:40,312 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2019-02-09 14:45:40,361 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2019-02-09 14:45:40,361 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2019-02-09 14:45:40,362 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 52 blocks to reach the threshold 0.9990 of total blocks 52.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2019-02-09 14:45:40,371 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-02-09 14:45:40,463 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-02-09 14:45:40,465 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2019-02-09 14:45:40,475 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2019-02-09 14:45:40,476 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2019-02-09 14:45:40,486 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-02-09 14:45:43,688 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=50bf9075-0a2a-4a3b-a9e7-9154c919bf09, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-44195f4d-20ee-4085-84d3-18859dbea129;nsid=1647206854;c=0) storage 50bf9075-0a2a-4a3b-a9e7-9154c919bf09
2019-02-09 14:45:43,688 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-02-09 14:45:43,689 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2019-02-09 14:45:43,799 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2019-02-09 14:45:43,799 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1 for DN 127.0.0.1:50010
2019-02-09 14:45:43,875 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 51 has reached the threshold 0.9990 of total blocks 52. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2019-02-09 14:45:43,875 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2019-02-09 14:45:43,877 INFO BlockStateChange: BLOCK* processReport: from storage DS-0c54565c-7ee9-4a7c-9a46-6203df006ed1 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=50bf9075-0a2a-4a3b-a9e7-9154c919bf09, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-44195f4d-20ee-4085-84d3-18859dbea129;nsid=1647206854;c=0), blocks: 52, hasStaleStorage: false, processing time: 14 msecs
2019-02-09 14:45:43,886 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 52
2019-02-09 14:45:43,886 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2019-02-09 14:45:43,886 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 6
2019-02-09 14:45:43,887 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2019-02-09 14:45:43,887 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2019-02-09 14:45:43,887 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 12 msec
2019-02-09 14:46:03,882 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 52 has reached the threshold 0.9990 of total blocks 52. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2019-02-09 14:46:13,885 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 35 secs
2019-02-09 14:46:13,885 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2019-02-09 14:46:13,885 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2019-02-09 14:46:13,885 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 6 blocks
2019-02-09 14:46:48,442 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2019-02-09 14:46:48,442 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2019-02-09 14:46:48,442 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1383
2019-02-09 14:46:48,443 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 127 
2019-02-09 14:46:48,461 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 145 
2019-02-09 14:46:48,463 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/ubuntu/hdata/dfs/name/current/edits_inprogress_0000000000000001383 -> /home/ubuntu/hdata/dfs/name/current/edits_0000000000000001383-0000000000000001384
2019-02-09 14:46:48,463 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1385
2019-02-09 14:46:50,843 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.09s at 89.89 KB/s
2019-02-09 14:46:50,843 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000001384 size 8707 bytes.
2019-02-09 14:46:50,889 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 1382
2019-02-09 14:46:50,889 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/ubuntu/hdata/dfs/name/current/fsimage_0000000000000001381, cpktTxId=0000000000000001381)
2019-02-09 14:50:18,739 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2019-02-09 14:50:18,743 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu-HP-Pavilion-dv6-NoteBook-PC/127.0.1.1
************************************************************/
