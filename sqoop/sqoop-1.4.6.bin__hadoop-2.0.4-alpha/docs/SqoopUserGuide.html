<html><head><meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>Sqoop User Guide (v1.4.6)</title><link rel="stylesheet" href="docbook.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"></head><body><div style="clear:both; margin-bottom: 4px"></div><div align="center"><a href="index.html"><img src="images/home.png" alt="Documentation Home"></a></div><span class="breadcrumbs"><div class="breadcrumbs"><span class="breadcrumb-node">Sqoop User Guide (v1.4.6)</span></div></span><div lang="en" class="article" title="Sqoop User Guide (v1.4.6)"><div class="titlepage"><div><div><h2 class="title"><a name="idp117624"></a>Sqoop User Guide (v1.4.6)</h2></div></div><hr></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#_introduction">1. Introduction</a></span></dt><dt><span class="section"><a href="#_supported_releases">2. Supported Releases</a></span></dt><dt><span class="section"><a href="#_sqoop_releases">3. Sqoop Releases</a></span></dt><dt><span class="section"><a href="#_prerequisites">4. Prerequisites</a></span></dt><dt><span class="section"><a href="#_basic_usage">5. Basic Usage</a></span></dt><dt><span class="section"><a href="#_sqoop_tools">6. Sqoop Tools</a></span></dt><dd><dl><dt><span class="section"><a href="#_using_command_aliases">6.1. Using Command Aliases</a></span></dt><dt><span class="section"><a href="#_controlling_the_hadoop_installation">6.2. Controlling the Hadoop Installation</a></span></dt><dt><span class="section"><a href="#_using_generic_and_specific_arguments">6.3. Using Generic and Specific Arguments</a></span></dt><dt><span class="section"><a href="#_using_options_files_to_pass_arguments">6.4. Using Options Files to Pass Arguments</a></span></dt><dt><span class="section"><a href="#_using_tools">6.5. Using Tools</a></span></dt></dl></dd><dt><span class="section"><a href="#_literal_sqoop_import_literal">7. <code class="literal">sqoop-import</code></a></span></dt><dd><dl><dt><span class="section"><a href="#_purpose">7.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax">7.2. Syntax</a></span></dt><dd><dl><dt><span class="section"><a href="#_connecting_to_a_database_server">7.2.1. Connecting to a Database Server</a></span></dt><dt><span class="section"><a href="#_selecting_the_data_to_import">7.2.2. Selecting the Data to Import</a></span></dt><dt><span class="section"><a href="#_free_form_query_imports">7.2.3. Free-form Query Imports</a></span></dt><dt><span class="section"><a href="#_controlling_parallelism">7.2.4. Controlling Parallelism</a></span></dt><dt><span class="section"><a href="#_controlling_distributed_cache">7.2.5. Controlling Distributed Cache</a></span></dt><dt><span class="section"><a href="#_controlling_the_import_process">7.2.6. Controlling the Import Process</a></span></dt><dt><span class="section"><a href="#_controlling_transaction_isolation">7.2.7. Controlling transaction isolation</a></span></dt><dt><span class="section"><a href="#_controlling_type_mapping">7.2.8. Controlling type mapping</a></span></dt><dt><span class="section"><a href="#_incremental_imports">7.2.9. Incremental Imports</a></span></dt><dt><span class="section"><a href="#_file_formats">7.2.10. File Formats</a></span></dt><dt><span class="section"><a href="#_large_objects">7.2.11. Large Objects</a></span></dt><dt><span class="section"><a href="#_importing_data_into_hive">7.2.12. Importing Data Into Hive</a></span></dt><dt><span class="section"><a href="#_importing_data_into_hbase">7.2.13. Importing Data Into HBase</a></span></dt><dt><span class="section"><a href="#_importing_data_into_accumulo">7.2.14. Importing Data Into Accumulo</a></span></dt><dt><span class="section"><a href="#_additional_import_configuration_properties">7.2.15. Additional Import Configuration Properties</a></span></dt></dl></dd><dt><span class="section"><a href="#_example_invocations">7.3. Example Invocations</a></span></dt></dl></dd><dt><span class="section"><a href="#_literal_sqoop_import_all_tables_literal">8. <code class="literal">sqoop-import-all-tables</code></a></span></dt><dd><dl><dt><span class="section"><a href="#_purpose_2">8.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_2">8.2. Syntax</a></span></dt><dt><span class="section"><a href="#_example_invocations_2">8.3. Example Invocations</a></span></dt></dl></dd><dt><span class="section"><a href="#_literal_sqoop_import_mainframe_literal">9. <code class="literal">sqoop-import-mainframe</code></a></span></dt><dd><dl><dt><span class="section"><a href="#_purpose_3">9.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_3">9.2. Syntax</a></span></dt><dd><dl><dt><span class="section"><a href="#_connecting_to_a_mainframe">9.2.1. Connecting to a Mainframe</a></span></dt><dt><span class="section"><a href="#_selecting_the_files_to_import">9.2.2. Selecting the Files to Import</a></span></dt><dt><span class="section"><a href="#_controlling_parallelism_2">9.2.3. Controlling Parallelism</a></span></dt><dt><span class="section"><a href="#_controlling_distributed_cache_2">9.2.4. Controlling Distributed Cache</a></span></dt><dt><span class="section"><a href="#_controlling_the_import_process_2">9.2.5. Controlling the Import Process</a></span></dt><dt><span class="section"><a href="#_file_formats_2">9.2.6. File Formats</a></span></dt><dt><span class="section"><a href="#_importing_data_into_hive_2">9.2.7. Importing Data Into Hive</a></span></dt><dt><span class="section"><a href="#_importing_data_into_hbase_2">9.2.8. Importing Data Into HBase</a></span></dt><dt><span class="section"><a href="#_importing_data_into_accumulo_2">9.2.9. Importing Data Into Accumulo</a></span></dt><dt><span class="section"><a href="#_additional_import_configuration_properties_2">9.2.10. Additional Import Configuration Properties</a></span></dt></dl></dd><dt><span class="section"><a href="#_example_invocations_3">9.3. Example Invocations</a></span></dt></dl></dd><dt><span class="section"><a href="#_literal_sqoop_export_literal">10. <code class="literal">sqoop-export</code></a></span></dt><dd><dl><dt><span class="section"><a href="#_purpose_4">10.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_4">10.2. Syntax</a></span></dt><dt><span class="section"><a href="#_inserts_vs_updates">10.3. Inserts vs. Updates</a></span></dt><dt><span class="section"><a href="#_exports_and_transactions">10.4. Exports and Transactions</a></span></dt><dt><span class="section"><a href="#_failed_exports">10.5. Failed Exports</a></span></dt><dt><span class="section"><a href="#_example_invocations_4">10.6. Example Invocations</a></span></dt></dl></dd><dt><span class="section"><a href="#validation">11. <code class="literal">validation</code></a></span></dt><dd><dl><dt><span class="section"><a href="#_purpose_5">11.1. Purpose</a></span></dt><dt><span class="section"><a href="#_introduction_2">11.2. Introduction</a></span></dt><dt><span class="section"><a href="#_syntax_5">11.3. Syntax</a></span></dt><dt><span class="section"><a href="#_configuration">11.4. Configuration</a></span></dt><dt><span class="section"><a href="#_limitations">11.5. Limitations</a></span></dt><dt><span class="section"><a href="#_example_invocations_5">11.6. Example Invocations</a></span></dt></dl></dd><dt><span class="section"><a href="#_saved_jobs">12. Saved Jobs</a></span></dt><dt><span class="section"><a href="#_literal_sqoop_job_literal">13. <code class="literal">sqoop-job</code></a></span></dt><dd><dl><dt><span class="section"><a href="#_purpose_6">13.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_6">13.2. Syntax</a></span></dt><dt><span class="section"><a href="#_saved_jobs_and_passwords">13.3. Saved jobs and passwords</a></span></dt><dt><span class="section"><a href="#_saved_jobs_and_incremental_imports">13.4. Saved jobs and incremental imports</a></span></dt></dl></dd><dt><span class="section"><a href="#_literal_sqoop_metastore_literal">14. <code class="literal">sqoop-metastore</code></a></span></dt><dd><dl><dt><span class="section"><a href="#_purpose_7">14.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_7">14.2. Syntax</a></span></dt></dl></dd><dt><span class="section"><a href="#_literal_sqoop_merge_literal">15. <code class="literal">sqoop-merge</code></a></span></dt><dd><dl><dt><span class="section"><a href="#_purpose_8">15.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_8">15.2. Syntax</a></span></dt></dl></dd><dt><span class="section"><a href="#_literal_sqoop_codegen_literal">16. <code class="literal">sqoop-codegen</code></a></span></dt><dd><dl><dt><span class="section"><a href="#_purpose_9">16.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_9">16.2. Syntax</a></span></dt><dt><span class="section"><a href="#_example_invocations_6">16.3. Example Invocations</a></span></dt></dl></dd><dt><span class="section"><a href="#_literal_sqoop_create_hive_table_literal">17. <code class="literal">sqoop-create-hive-table</code></a></span></dt><dd><dl><dt><span class="section"><a href="#_purpose_10">17.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_10">17.2. Syntax</a></span></dt><dt><span class="section"><a href="#_example_invocations_7">17.3. Example Invocations</a></span></dt></dl></dd><dt><span class="section"><a href="#_literal_sqoop_eval_literal">18. <code class="literal">sqoop-eval</code></a></span></dt><dd><dl><dt><span class="section"><a href="#_purpose_11">18.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_11">18.2. Syntax</a></span></dt><dt><span class="section"><a href="#_example_invocations_8">18.3. Example Invocations</a></span></dt></dl></dd><dt><span class="section"><a href="#_literal_sqoop_list_databases_literal">19. <code class="literal">sqoop-list-databases</code></a></span></dt><dd><dl><dt><span class="section"><a href="#_purpose_12">19.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_12">19.2. Syntax</a></span></dt><dt><span class="section"><a href="#_example_invocations_9">19.3. Example Invocations</a></span></dt></dl></dd><dt><span class="section"><a href="#_literal_sqoop_list_tables_literal">20. <code class="literal">sqoop-list-tables</code></a></span></dt><dd><dl><dt><span class="section"><a href="#_purpose_13">20.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_13">20.2. Syntax</a></span></dt><dt><span class="section"><a href="#_example_invocations_10">20.3. Example Invocations</a></span></dt></dl></dd><dt><span class="section"><a href="#_literal_sqoop_help_literal">21. <code class="literal">sqoop-help</code></a></span></dt><dd><dl><dt><span class="section"><a href="#_purpose_14">21.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_14">21.2. Syntax</a></span></dt><dt><span class="section"><a href="#_example_invocations_11">21.3. Example Invocations</a></span></dt></dl></dd><dt><span class="section"><a href="#_literal_sqoop_version_literal">22. <code class="literal">sqoop-version</code></a></span></dt><dd><dl><dt><span class="section"><a href="#_purpose_15">22.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_15">22.2. Syntax</a></span></dt><dt><span class="section"><a href="#_example_invocations_12">22.3. Example Invocations</a></span></dt></dl></dd><dt><span class="section"><a href="#_sqoop_hcatalog_integration">23. Sqoop-HCatalog Integration</a></span></dt><dd><dl><dt><span class="section"><a href="#_hcatalog_background">23.1. HCatalog Background</a></span></dt><dt><span class="section"><a href="#_exposing_hcatalog_tables_to_sqoop">23.2. Exposing HCatalog Tables to Sqoop</a></span></dt><dd><dl><dt><span class="section"><a href="#_new_command_line_options">23.2.1. New Command Line Options</a></span></dt><dt><span class="section"><a href="#_supported_sqoop_hive_options">23.2.2. Supported Sqoop Hive Options</a></span></dt><dt><span class="section"><a href="#_direct_mode_support">23.2.3. Direct Mode support</a></span></dt><dt><span class="section"><a href="#_unsupported_sqoop_options">23.2.4. Unsupported Sqoop Options</a></span></dt><dd><dl><dt><span class="section"><a href="#_unsupported_sqoop_hive_import_options">23.2.4.1. Unsupported Sqoop Hive Import Options</a></span></dt><dt><span class="section"><a href="#_unsupported_sqoop_export_and_import_options">23.2.4.2. Unsupported Sqoop Export and Import Options</a></span></dt></dl></dd><dt><span class="section"><a href="#_ignored_sqoop_options">23.2.5. Ignored Sqoop Options</a></span></dt></dl></dd><dt><span class="section"><a href="#_automatic_table_creation">23.3. Automatic Table Creation</a></span></dt><dt><span class="section"><a href="#_delimited_text_formats_and_field_and_line_delimiter_characters">23.4. Delimited Text Formats and Field and Line Delimiter Characters</a></span></dt><dt><span class="section"><a href="#_hcatalog_table_requirements">23.5. HCatalog Table Requirements</a></span></dt><dt><span class="section"><a href="#_support_for_partitioning">23.6. Support for Partitioning</a></span></dt><dt><span class="section"><a href="#_schema_mapping">23.7. Schema Mapping</a></span></dt><dt><span class="section"><a href="#_support_for_hcatalog_data_types">23.8. Support for HCatalog Data Types</a></span></dt><dt><span class="section"><a href="#_providing_hive_and_hcatalog_libraries_for_the_sqoop_job">23.9. Providing Hive and HCatalog Libraries for the Sqoop Job</a></span></dt><dt><span class="section"><a href="#_examples">23.10. Examples</a></span></dt><dt><span class="section"><a href="#_import">23.11. Import</a></span></dt><dt><span class="section"><a href="#_export">23.12. Export</a></span></dt></dl></dd><dt><span class="section"><a href="#_compatibility_notes">24. Compatibility Notes</a></span></dt><dd><dl><dt><span class="section"><a href="#_supported_databases">24.1. Supported Databases</a></span></dt><dt><span class="section"><a href="#_mysql">24.2. MySQL</a></span></dt><dd><dl><dt><span class="section"><a href="#_zerodatetimebehavior">24.2.1. zeroDateTimeBehavior</a></span></dt><dt><span class="section"><a href="#_literal_unsigned_literal_columns">24.2.2. <code class="literal">UNSIGNED</code> columns</a></span></dt><dt><span class="section"><a href="#_literal_blob_literal_and_literal_clob_literal_columns">24.2.3. <code class="literal">BLOB</code> and <code class="literal">CLOB</code> columns</a></span></dt><dt><span class="section"><a href="#_importing_views_in_direct_mode">24.2.4. Importing views in direct mode</a></span></dt></dl></dd><dt><span class="section"><a href="#_postgresql">24.3. PostgreSQL</a></span></dt><dd><dl><dt><span class="section"><a href="#_importing_views_in_direct_mode_2">24.3.1. Importing views in direct mode</a></span></dt></dl></dd><dt><span class="section"><a href="#_oracle">24.4. Oracle</a></span></dt><dd><dl><dt><span class="section"><a href="#_dates_and_times">24.4.1. Dates and Times</a></span></dt></dl></dd><dt><span class="section"><a href="#_schema_definition_in_hive">24.5. Schema Definition in Hive</a></span></dt><dt><span class="section"><a href="#_cubrid">24.6. CUBRID</a></span></dt></dl></dd><dt><span class="section"><a href="#connectors">25. Notes for specific connectors</a></span></dt><dd><dl><dt><span class="section"><a href="#_mysql_jdbc_connector">25.1. MySQL JDBC Connector</a></span></dt><dd><dl><dt><span class="section"><a href="#_upsert_functionality">25.1.1. Upsert functionality</a></span></dt></dl></dd><dt><span class="section"><a href="#_mysql_direct_connector">25.2. MySQL Direct Connector</a></span></dt><dd><dl><dt><span class="section"><a href="#_requirements">25.2.1. Requirements</a></span></dt><dt><span class="section"><a href="#_limitations_2">25.2.2. Limitations</a></span></dt><dt><span class="section"><a href="#_direct_mode_transactions">25.2.3. Direct-mode Transactions</a></span></dt></dl></dd><dt><span class="section"><a href="#_microsoft_sql_connector">25.3. Microsoft SQL Connector</a></span></dt><dd><dl><dt><span class="section"><a href="#_extra_arguments">25.3.1. Extra arguments</a></span></dt><dt><span class="section"><a href="#_allow_identity_inserts">25.3.2. Allow identity inserts</a></span></dt><dt><span class="section"><a href="#_non_resilient_operations">25.3.3. Non-resilient operations</a></span></dt><dt><span class="section"><a href="#_schema_support">25.3.4. Schema support</a></span></dt><dt><span class="section"><a href="#_table_hints">25.3.5. Table hints</a></span></dt></dl></dd><dt><span class="section"><a href="#_postgresql_connector">25.4. PostgreSQL Connector</a></span></dt><dd><dl><dt><span class="section"><a href="#_extra_arguments_2">25.4.1. Extra arguments</a></span></dt><dt><span class="section"><a href="#_schema_support_2">25.4.2. Schema support</a></span></dt></dl></dd><dt><span class="section"><a href="#_postgresql_direct_connector">25.5. PostgreSQL Direct Connector</a></span></dt><dd><dl><dt><span class="section"><a href="#_requirements_2">25.5.1. Requirements</a></span></dt><dt><span class="section"><a href="#_limitations_3">25.5.2. Limitations</a></span></dt></dl></dd><dt><span class="section"><a href="#_pg_bulkload_connector">25.6. pg_bulkload connector</a></span></dt><dd><dl><dt><span class="section"><a href="#_purpose_16">25.6.1. Purpose</a></span></dt><dt><span class="section"><a href="#_requirements_3">25.6.2. Requirements</a></span></dt><dt><span class="section"><a href="#_syntax_16">25.6.3. Syntax</a></span></dt><dt><span class="section"><a href="#_data_staging">25.6.4. Data Staging</a></span></dt></dl></dd><dt><span class="section"><a href="#_netezza_connector">25.7. Netezza Connector</a></span></dt><dd><dl><dt><span class="section"><a href="#_extra_arguments_3">25.7.1. Extra arguments</a></span></dt><dt><span class="section"><a href="#_direct_mode">25.7.2. Direct Mode</a></span></dt><dt><span class="section"><a href="#_null_string_handling">25.7.3. Null string handling</a></span></dt></dl></dd><dt><span class="section"><a href="#_data_connector_for_oracle_and_hadoop">25.8. Data Connector for Oracle and Hadoop</a></span></dt><dd><dl><dt><span class="section"><a href="#_about">25.8.1. About</a></span></dt><dd><dl><dt><span class="section"><a href="#_jobs">25.8.1.1. Jobs</a></span></dt><dt><span class="section"><a href="#_how_the_standard_oracle_manager_works_for_imports">25.8.1.2. How The Standard Oracle Manager Works for Imports</a></span></dt><dt><span class="section"><a href="#_how_the_data_connector_for_oracle_and_hadoop_works_for_imports">25.8.1.3. How The Data Connector for Oracle and Hadoop Works for Imports</a></span></dt><dt><span class="section"><a href="#_data_connector_for_oracle_and_hadoop_exports">25.8.1.4. Data Connector for Oracle and Hadoop Exports</a></span></dt></dl></dd><dt><span class="section"><a href="#_requirements_4">25.8.2. Requirements</a></span></dt><dd><dl><dt><span class="section"><a href="#_ensure_the_oracle_database_jdbc_driver_is_setup_correctly">25.8.2.1. Ensure The Oracle Database JDBC Driver Is Setup Correctly</a></span></dt><dt><span class="section"><a href="#_oracle_roles_and_privileges">25.8.2.2. Oracle Roles and Privileges</a></span></dt><dt><span class="section"><a href="#_additional_oracle_roles_and_privileges_required_for_export">25.8.2.3. Additional Oracle Roles And Privileges Required for Export</a></span></dt><dt><span class="section"><a href="#_supported_data_types">25.8.2.4. Supported Data Types</a></span></dt></dl></dd><dt><span class="section"><a href="#_execute_sqoop_with_data_connector_for_oracle_and_hadoop">25.8.3. Execute Sqoop With Data Connector for Oracle and Hadoop</a></span></dt><dd><dl><dt><span class="section"><a href="#_connect_to_oracle_oracle_rac">25.8.3.1. Connect to Oracle / Oracle RAC</a></span></dt><dt><span class="section"><a href="#_connect_to_an_oracle_database_instance">25.8.3.2. Connect to An Oracle Database Instance</a></span></dt><dt><span class="section"><a href="#_connect_to_an_oracle_rac">25.8.3.3. Connect to An Oracle RAC</a></span></dt><dt><span class="section"><a href="#_login_to_the_oracle_instance">25.8.3.4. Login to The Oracle Instance</a></span></dt><dt><span class="section"><a href="#_kill_data_connector_for_oracle_and_hadoop_jobs">25.8.3.5. Kill Data Connector for Oracle and Hadoop Jobs</a></span></dt></dl></dd><dt><span class="section"><a href="#_import_data_from_oracle">25.8.4. Import Data from Oracle</a></span></dt><dd><dl><dt><span class="section"><a href="#_match_hadoop_files_to_oracle_table_partitions">25.8.4.1. Match Hadoop Files to Oracle Table Partitions</a></span></dt><dt><span class="section"><a href="#_specify_the_partitions_to_import">25.8.4.2. Specify The Partitions To Import</a></span></dt><dt><span class="section"><a href="#_consistent_read_all_mappers_read_from_the_same_point_in_time">25.8.4.3. Consistent Read: All Mappers Read From The Same Point In Time</a></span></dt></dl></dd><dt><span class="section"><a href="#_export_data_into_oracle">25.8.5. Export Data into Oracle</a></span></dt><dd><dl><dt><span class="section"><a href="#_insert_export">25.8.5.1. Insert-Export</a></span></dt><dt><span class="section"><a href="#_update_export">25.8.5.2. Update-Export</a></span></dt><dt><span class="section"><a href="#_merge_export">25.8.5.3. Merge-Export</a></span></dt><dt><span class="section"><a href="#_create_oracle_tables">25.8.5.4. Create Oracle Tables</a></span></dt><dt><span class="section"><a href="#_nologging">25.8.5.5. NOLOGGING</a></span></dt><dt><span class="section"><a href="#_partitioning">25.8.5.6. Partitioning</a></span></dt><dt><span class="section"><a href="#_match_rows_via_multiple_columns">25.8.5.7. Match Rows Via Multiple Columns</a></span></dt><dt><span class="section"><a href="#_storage_clauses">25.8.5.8. Storage Clauses</a></span></dt></dl></dd><dt><span class="section"><a href="#_manage_date_and_timestamp_data_types">25.8.6. Manage Date And Timestamp Data Types</a></span></dt><dd><dl><dt><span class="section"><a href="#_import_date_and_timestamp_data_types_from_oracle">25.8.6.1. Import Date And Timestamp Data Types from Oracle</a></span></dt><dt><span class="section"><a href="#_the_data_connector_for_oracle_and_hadoop_does_not_apply_a_time_zone_to_date_timestamp_data_types">25.8.6.2. The Data Connector for Oracle and Hadoop Does Not Apply A Time Zone to DATE / TIMESTAMP Data Types</a></span></dt><dt><span class="section"><a href="#_the_data_connector_for_oracle_and_hadoop_retains_time_zone_information_in_timezone_data_types">25.8.6.3. The Data Connector for Oracle and Hadoop Retains Time Zone Information in TIMEZONE Data Types</a></span></dt><dt><span class="section"><a href="#_data_connector_for_oracle_and_hadoop_explicitly_states_time_zone_for_local_timezone_data_types">25.8.6.4. Data Connector for Oracle and Hadoop Explicitly States Time Zone for LOCAL TIMEZONE Data Types</a></span></dt><dt><span class="section"><a href="#_java_sql_timestamp">25.8.6.5. java.sql.Timestamp</a></span></dt><dt><span class="section"><a href="#_export_date_and_timestamp_data_types_into_oracle">25.8.6.6. Export Date And Timestamp Data Types into Oracle</a></span></dt></dl></dd><dt><span class="section"><a href="#_configure_the_data_connector_for_oracle_and_hadoop">25.8.7. Configure The Data Connector for Oracle and Hadoop</a></span></dt><dd><dl><dt><span class="section"><a href="#_oraoop_site_template_xml">25.8.7.1. oraoop-site-template.xml</a></span></dt><dt><span class="section"><a href="#_oraoop_oracle_session_initialization_statements">25.8.7.2. oraoop.oracle.session.initialization.statements</a></span></dt><dt><span class="section"><a href="#_oraoop_table_import_where_clause_location">25.8.7.3. oraoop.table.import.where.clause.location</a></span></dt><dt><span class="section"><a href="#_oracle_row_fetch_size">25.8.7.4. oracle.row.fetch.size</a></span></dt><dt><span class="section"><a href="#_oraoop_import_hint">25.8.7.5. oraoop.import.hint</a></span></dt><dt><span class="section"><a href="#_oraoop_oracle_append_values_hint_usage">25.8.7.6. oraoop.oracle.append.values.hint.usage</a></span></dt><dt><span class="section"><a href="#_mapred_map_tasks_speculative_execution">25.8.7.7. mapred.map.tasks.speculative.execution</a></span></dt><dt><span class="section"><a href="#_oraoop_block_allocation">25.8.7.8. oraoop.block.allocation</a></span></dt><dt><span class="section"><a href="#_oraoop_import_omit_lobs_and_long">25.8.7.9. oraoop.import.omit.lobs.and.long</a></span></dt><dt><span class="section"><a href="#_oraoop_locations">25.8.7.10. oraoop.locations</a></span></dt><dt><span class="section"><a href="#_sqoop_connection_factories">25.8.7.11. sqoop.connection.factories</a></span></dt><dt><span class="section"><a href="#_expressions_in_oraoop_site_xml">25.8.7.12. Expressions in oraoop-site.xml</a></span></dt></dl></dd><dt><span class="section"><a href="#_troubleshooting_the_data_connector_for_oracle_and_hadoop">25.8.8. Troubleshooting The Data Connector for Oracle and Hadoop</a></span></dt><dd><dl><dt><span class="section"><a href="#_quote_oracle_owners_and_tables">25.8.8.1. Quote Oracle Owners And Tables</a></span></dt><dt><span class="section"><a href="#_quote_oracle_columns">25.8.8.2. Quote Oracle Columns</a></span></dt><dt><span class="section"><a href="#_confirm_the_data_connector_for_oracle_and_hadoop_can_initialize_the_oracle_session">25.8.8.3. Confirm The Data Connector for Oracle and Hadoop Can Initialize The Oracle Session</a></span></dt><dt><span class="section"><a href="#_check_the_sqoop_debug_logs_for_error_messages">25.8.8.4. Check The Sqoop Debug Logs for Error Messages</a></span></dt><dt><span class="section"><a href="#_export_check_tables_are_compatible">25.8.8.5. Export: Check Tables Are Compatible</a></span></dt><dt><span class="section"><a href="#_export_parallelization">25.8.8.6. Export: Parallelization</a></span></dt><dt><span class="section"><a href="#_export_check_oraoop_oracle_append_values_hint_usage">25.8.8.7. Export: Check oraoop.oracle.append.values.hint.usage</a></span></dt><dt><span class="section"><a href="#_turn_on_verbose">25.8.8.8. Turn On Verbose</a></span></dt></dl></dd></dl></dd></dl></dd><dt><span class="section"><a href="#_getting_support">26. Getting Support</a></span></dt><dt><span class="section"><a href="#_troubleshooting">27. Troubleshooting</a></span></dt><dd><dl><dt><span class="section"><a href="#_general_troubleshooting_process">27.1. General Troubleshooting Process</a></span></dt><dt><span class="section"><a href="#_specific_troubleshooting_tips">27.2. Specific Troubleshooting Tips</a></span></dt><dd><dl><dt><span class="section"><a href="#_oracle_connection_reset_errors">27.2.1. Oracle: Connection Reset Errors</a></span></dt><dt><span class="section"><a href="#_oracle_case_sensitive_catalog_query_errors">27.2.2. Oracle: Case-Sensitive Catalog Query Errors</a></span></dt><dt><span class="section"><a href="#_mysql_connection_failure">27.2.3. MySQL: Connection Failure</a></span></dt><dt><span class="section"><a href="#_oracle_ora_00933_error_sql_command_not_properly_ended">27.2.4. Oracle: ORA-00933 error (SQL command not properly ended)</a></span></dt><dt><span class="section"><a href="#_mysql_import_of_tinyint_1_from_mysql_behaves_strangely">27.2.5. MySQL: Import of TINYINT(1) from MySQL behaves strangely</a></span></dt></dl></dd></dl></dd></dl></div><pre class="screen">  Licensed to the Apache Software Foundation (ASF) under one
  or more contributor license agreements.  See the NOTICE file
  distributed with this work for additional information
  regarding copyright ownership.  The ASF licenses this file
  to you under the Apache License, Version 2.0 (the
  "License"); you may not use this file except in compliance
  with the License.  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.</pre><div class="section" title="1. Introduction"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_introduction"></a>1. Introduction</h2></div></div></div><p>Sqoop is a tool designed to transfer data between Hadoop and
relational databases or mainframes. You can use Sqoop to import data from a
relational database management system (RDBMS) such as MySQL or Oracle or a
mainframe into the Hadoop Distributed File System (HDFS),
transform the data in Hadoop MapReduce, and then export the data back
into an RDBMS.</p><p>Sqoop automates most of this process, relying on the database to
describe the schema for the data to be imported. Sqoop uses MapReduce
to import and export the data, which provides parallel operation as
well as fault tolerance.</p><p>This document describes how to get started using Sqoop to move data
between databases and Hadoop or mainframe to Hadoop and provides reference
information for the operation of the Sqoop command-line tool suite. This
document is intended for:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
System and application programmers
</li><li class="listitem">
System administrators
</li><li class="listitem">
Database administrators
</li><li class="listitem">
Data analysts
</li><li class="listitem">
Data engineers
</li></ul></div></div><div class="section" title="2. Supported Releases"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_supported_releases"></a>2. Supported Releases</h2></div></div></div><p>This documentation applies to Sqoop v1.4.6.</p></div><div class="section" title="3. Sqoop Releases"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_sqoop_releases"></a>3. Sqoop Releases</h2></div></div></div><p>Sqoop is an open source software product of the Apache Software Foundation.</p><p>Software development for Sqoop occurs at <a class="ulink" href="http://sqoop.apache.org" target="_top">http://sqoop.apache.org</a>
At that site you can obtain:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
New releases of Sqoop as well as its most recent source code
</li><li class="listitem">
An issue tracker
</li><li class="listitem">
A wiki that contains Sqoop documentation
</li></ul></div></div><div class="section" title="4. Prerequisites"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_prerequisites"></a>4. Prerequisites</h2></div></div></div><p>The following prerequisite knowledge is required for this product:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Basic computer technology and terminology
</li><li class="listitem">
Familiarity with command-line interfaces such as <code class="literal">bash</code>
</li><li class="listitem">
Relational database management systems
</li><li class="listitem">
Basic familiarity with the purpose and operation of Hadoop
</li></ul></div><p>Before you can use Sqoop, a release of Hadoop must be installed and
configured. Sqoop is currently supporting 4 major Hadoop releases - 0.20,
0.23, 1.0 and 2.0.</p><p>This document assumes you are using a Linux or Linux-like environment.
If you are using Windows, you may be able to use cygwin to accomplish
most of the following tasks. If you are using Mac OS X, you should see
few (if any) compatibility errors. Sqoop is predominantly operated and
tested on Linux.</p></div><div class="section" title="5. Basic Usage"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_basic_usage"></a>5. Basic Usage</h2></div></div></div><p>With Sqoop, you can <span class="emphasis"><em>import</em></span> data from a relational database system or a
mainframe into HDFS. The input to the import process is either database table
or mainframe datasets. For databases, Sqoop will read the table row-by-row
into HDFS.  For mainframe datasets, Sqoop will read records from each mainframe
dataset into HDFS. The output of this import process is a set of files
containing a copy of the imported table or datasets.
The import process is performed in parallel. For this reason, the
output will be in multiple files. These files may be delimited text
files (for example, with commas or tabs separating each field), or
binary Avro or SequenceFiles containing serialized record data.</p><p>A by-product of the import process is a generated Java class which
can encapsulate one row of the imported table. This class is used
during the import process by Sqoop itself. The Java source code for
this class is also provided to you, for use in subsequent MapReduce
processing of the data. This class can serialize and deserialize data
to and from the SequenceFile format. It can also parse the
delimited-text form of a record. These abilities allow you to quickly
develop MapReduce applications that use the HDFS-stored records in
your processing pipeline. You are also free to parse the delimiteds
record data yourself, using any other tools you prefer.</p><p>After manipulating the imported records (for example, with MapReduce
or Hive) you may have a result data set which you can then <span class="emphasis"><em>export</em></span>
back to the relational database. Sqoop&#8217;s export process will read
a set of delimited text files from HDFS in parallel, parse them into
records, and insert them as new rows in a target database table, for
consumption by external applications or users.</p><p>Sqoop includes some other commands which allow you to inspect the
database you are working with. For example, you can list the available
database schemas (with the <code class="literal">sqoop-list-databases</code> tool) and tables
within a schema (with the <code class="literal">sqoop-list-tables</code> tool). Sqoop also
includes a primitive SQL execution shell (the <code class="literal">sqoop-eval</code> tool).</p><p>Most aspects of the import, code generation, and export processes can
be customized. For databases, you can control the specific row range or
columns imported.  You can specify particular delimiters and escape characters
for the file-based representation of the data, as well as the file format
used.  You can also control the class or package names used in
generated code. Subsequent sections of this document explain how to
specify these and other arguments to Sqoop.</p></div><div class="section" title="6. Sqoop Tools"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_sqoop_tools"></a>6. Sqoop Tools</h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_using_command_aliases">6.1. Using Command Aliases</a></span></dt><dt><span class="section"><a href="#_controlling_the_hadoop_installation">6.2. Controlling the Hadoop Installation</a></span></dt><dt><span class="section"><a href="#_using_generic_and_specific_arguments">6.3. Using Generic and Specific Arguments</a></span></dt><dt><span class="section"><a href="#_using_options_files_to_pass_arguments">6.4. Using Options Files to Pass Arguments</a></span></dt><dt><span class="section"><a href="#_using_tools">6.5. Using Tools</a></span></dt></dl></div><p>Sqoop is a collection of related tools. To use Sqoop, you specify the
tool you want to use and the arguments that control the tool.</p><p>If Sqoop is compiled from its own source, you can run Sqoop without a formal
installation process by running the <code class="literal">bin/sqoop</code> program. Users
of a packaged deployment of Sqoop (such as an RPM shipped with Apache Bigtop)
will see this program installed as <code class="literal">/usr/bin/sqoop</code>. The remainder of this
documentation will refer to this program as <code class="literal">sqoop</code>. For example:</p><pre class="screen">$ sqoop tool-name [tool-arguments]</pre><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>The following examples that begin with a <code class="literal">$</code> character indicate
that the commands must be entered at a terminal prompt (such as
<code class="literal">bash</code>). The <code class="literal">$</code> character represents the prompt itself; you should
not start these commands by typing a <code class="literal">$</code>. You can also enter commands
inline in the text of a paragraph; for example, <code class="literal">sqoop help</code>. These
examples do not show a <code class="literal">$</code> prefix, but you should enter them the same
way.  Don&#8217;t confuse the <code class="literal">$</code> shell prompt in the examples with the <code class="literal">$</code>
that precedes an environment variable name. For example, the string
literal <code class="literal">$HADOOP_HOME</code> includes a "<code class="literal">$</code>".</p></td></tr></table></div><p>Sqoop ships with a help tool. To display a list of all available
tools, type the following command:</p><pre class="screen">$ sqoop help
usage: sqoop COMMAND [ARGS]

Available commands:
  codegen            Generate code to interact with database records
  create-hive-table  Import a table definition into Hive
  eval               Evaluate a SQL statement and display the results
  export             Export an HDFS directory to a database table
  help               List available commands
  import             Import a table from a database to HDFS
  import-all-tables  Import tables from a database to HDFS
  import-mainframe   Import mainframe datasets to HDFS
  list-databases     List available databases on a server
  list-tables        List available tables in a database
  version            Display version information

See 'sqoop help COMMAND' for information on a specific command.</pre><p>You can display help for a specific tool by entering: <code class="literal">sqoop help
(tool-name)</code>; for example, <code class="literal">sqoop help import</code>.</p><p>You can also add the <code class="literal">--help</code> argument to any command: <code class="literal">sqoop import
--help</code>.</p><div class="section" title="6.1. Using Command Aliases"><div class="titlepage"><div><div><h3 class="title"><a name="_using_command_aliases"></a>6.1. Using Command Aliases</h3></div></div></div><p>In addition to typing the <code class="literal">sqoop (toolname)</code> syntax, you can use alias
scripts that specify the <code class="literal">sqoop-(toolname)</code> syntax. For example, the
scripts <code class="literal">sqoop-import</code>, <code class="literal">sqoop-export</code>, etc. each select a specific
tool.</p></div><div class="section" title="6.2. Controlling the Hadoop Installation"><div class="titlepage"><div><div><h3 class="title"><a name="_controlling_the_hadoop_installation"></a>6.2. Controlling the Hadoop Installation</h3></div></div></div><p>You invoke Sqoop through the program launch capability provided by
Hadoop. The <code class="literal">sqoop</code> command-line program is a wrapper which runs the
<code class="literal">bin/hadoop</code> script shipped with Hadoop. If you have multiple
installations of Hadoop present on your machine, you can select the
Hadoop installation by setting the <code class="literal">$HADOOP_COMMON_HOME</code> and
<code class="literal">$HADOOP_MAPRED_HOME</code> environment variables.</p><p>For example:</p><pre class="screen">$ HADOOP_COMMON_HOME=/path/to/some/hadoop \
  HADOOP_MAPRED_HOME=/path/to/some/hadoop-mapreduce \
  sqoop import --arguments...</pre><p>or:</p><pre class="screen">$ export HADOOP_COMMON_HOME=/some/path/to/hadoop
$ export HADOOP_MAPRED_HOME=/some/path/to/hadoop-mapreduce
$ sqoop import --arguments...</pre><p>If either of these variables are not set, Sqoop will fall back to
<code class="literal">$HADOOP_HOME</code>. If it is not set either, Sqoop will use the default
installation locations for Apache Bigtop, <code class="literal">/usr/lib/hadoop</code> and
<code class="literal">/usr/lib/hadoop-mapreduce</code>, respectively.</p><p>The active Hadoop configuration is loaded from <code class="literal">$HADOOP_HOME/conf/</code>,
unless the <code class="literal">$HADOOP_CONF_DIR</code> environment variable is set.</p></div><div class="section" title="6.3. Using Generic and Specific Arguments"><div class="titlepage"><div><div><h3 class="title"><a name="_using_generic_and_specific_arguments"></a>6.3. Using Generic and Specific Arguments</h3></div></div></div><p>To control the operation of each Sqoop tool, you use generic and
specific arguments.</p><p>For example:</p><pre class="screen">$ sqoop help import
usage: sqoop import [GENERIC-ARGS] [TOOL-ARGS]

Common arguments:
   --connect &lt;jdbc-uri&gt;     Specify JDBC connect string
   --connect-manager &lt;class-name&gt;     Specify connection manager class to use
   --driver &lt;class-name&gt;    Manually specify JDBC driver class to use
   --hadoop-mapred-home &lt;dir&gt;      Override $HADOOP_MAPRED_HOME
   --help                   Print usage instructions
   --password-file          Set path for file containing authentication password
   -P                       Read password from console
   --password &lt;password&gt;    Set authentication password
   --username &lt;username&gt;    Set authentication username
   --verbose                Print more information while working
   --hadoop-home &lt;dir&gt;     Deprecated. Override $HADOOP_HOME

[...]

Generic Hadoop command-line arguments:
(must preceed any tool-specific arguments)
Generic options supported are
-conf &lt;configuration file&gt;     specify an application configuration file
-D &lt;property=value&gt;            use value for given property
-fs &lt;local|namenode:port&gt;      specify a namenode
-jt &lt;local|jobtracker:port&gt;    specify a job tracker
-files &lt;comma separated list of files&gt;    specify comma separated files to be copied to the map reduce cluster
-libjars &lt;comma separated list of jars&gt;    specify comma separated jar files to include in the classpath.
-archives &lt;comma separated list of archives&gt;    specify comma separated archives to be unarchived on the compute machines.

The general command line syntax is
bin/hadoop command [genericOptions] [commandOptions]</pre><p>You must supply the generic arguments <code class="literal">-conf</code>, <code class="literal">-D</code>, and so on after the
tool name but <span class="strong"><strong>before</strong></span> any tool-specific arguments (such as
<code class="literal">--connect</code>). Note that generic Hadoop arguments are preceeded by a
single dash character (<code class="literal">-</code>), whereas tool-specific arguments start
with two dashes (<code class="literal">--</code>), unless they are single character arguments such as <code class="literal">-P</code>.</p><p>The <code class="literal">-conf</code>, <code class="literal">-D</code>, <code class="literal">-fs</code> and <code class="literal">-jt</code> arguments control the configuration
and Hadoop server settings. For example, the <code class="literal">-D mapred.job.name=&lt;job_name&gt;</code> can
be used to set the name of the MR job that Sqoop launches, if not specified,
the name defaults to the jar name for the job - which is derived from the used
table name.</p><p>The <code class="literal">-files</code>, <code class="literal">-libjars</code>, and <code class="literal">-archives</code> arguments are not typically used with
Sqoop, but they are included as part of Hadoop&#8217;s internal argument-parsing
system.</p></div><div class="section" title="6.4. Using Options Files to Pass Arguments"><div class="titlepage"><div><div><h3 class="title"><a name="_using_options_files_to_pass_arguments"></a>6.4. Using Options Files to Pass Arguments</h3></div></div></div><p>When using Sqoop, the command line options that do not change from
invocation to invocation can be put in an options file for convenience.
An options file is a text file where each line identifies an option in
the order that it appears otherwise on the command line. Option files
allow specifying a single option on multiple lines by using the
back-slash character at the end of intermediate lines. Also supported
are comments within option files that begin with the hash character.
Comments must be specified on a new line and may not be mixed with
option text. All comments and empty lines are ignored when option
files are expanded. Unless options appear as quoted strings, any
leading or trailing spaces are ignored. Quoted strings if used must
not extend beyond the line on which they are specified.</p><p>Option files can be specified anywhere in the command line as long as
the options within them follow the otherwise prescribed rules of
options ordering. For instance, regardless of where the options are
loaded from, they must follow the ordering such that generic options
appear first, tool specific options next, finally followed by options
that are intended to be passed to child programs.</p><p>To specify an options file, simply create an options file in a
convenient location and pass it to the command line via
<code class="literal">--options-file</code> argument.</p><p>Whenever an options file is specified, it is expanded on the
command line before the tool is invoked. You can specify more than
one option files within the same invocation if needed.</p><p>For example, the following Sqoop invocation for import can
be specified alternatively as shown below:</p><pre class="screen">$ sqoop import --connect jdbc:mysql://localhost/db --username foo --table TEST

$ sqoop --options-file /users/homer/work/import.txt --table TEST</pre><p>where the options file <code class="literal">/users/homer/work/import.txt</code> contains the following:</p><pre class="screen">import
--connect
jdbc:mysql://localhost/db
--username
foo</pre><p>The options file can have empty lines and comments for readability purposes.
So the above example would work exactly the same if the options file
<code class="literal">/users/homer/work/import.txt</code> contained the following:</p><pre class="screen">#
# Options file for Sqoop import
#

# Specifies the tool being invoked
import

# Connect parameter and value
--connect
jdbc:mysql://localhost/db

# Username parameter and value
--username
foo

#
# Remaining options should be specified in the command line.
#</pre></div><div class="section" title="6.5. Using Tools"><div class="titlepage"><div><div><h3 class="title"><a name="_using_tools"></a>6.5. Using Tools</h3></div></div></div><p>The following sections will describe each tool&#8217;s operation. The
tools are listed in the most likely order you will find them useful.</p></div></div><div class="section" title="7. sqoop-import"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_literal_sqoop_import_literal"></a>7. <code class="literal">sqoop-import</code></h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_purpose">7.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax">7.2. Syntax</a></span></dt><dd><dl><dt><span class="section"><a href="#_connecting_to_a_database_server">7.2.1. Connecting to a Database Server</a></span></dt><dt><span class="section"><a href="#_selecting_the_data_to_import">7.2.2. Selecting the Data to Import</a></span></dt><dt><span class="section"><a href="#_free_form_query_imports">7.2.3. Free-form Query Imports</a></span></dt><dt><span class="section"><a href="#_controlling_parallelism">7.2.4. Controlling Parallelism</a></span></dt><dt><span class="section"><a href="#_controlling_distributed_cache">7.2.5. Controlling Distributed Cache</a></span></dt><dt><span class="section"><a href="#_controlling_the_import_process">7.2.6. Controlling the Import Process</a></span></dt><dt><span class="section"><a href="#_controlling_transaction_isolation">7.2.7. Controlling transaction isolation</a></span></dt><dt><span class="section"><a href="#_controlling_type_mapping">7.2.8. Controlling type mapping</a></span></dt><dt><span class="section"><a href="#_incremental_imports">7.2.9. Incremental Imports</a></span></dt><dt><span class="section"><a href="#_file_formats">7.2.10. File Formats</a></span></dt><dt><span class="section"><a href="#_large_objects">7.2.11. Large Objects</a></span></dt><dt><span class="section"><a href="#_importing_data_into_hive">7.2.12. Importing Data Into Hive</a></span></dt><dt><span class="section"><a href="#_importing_data_into_hbase">7.2.13. Importing Data Into HBase</a></span></dt><dt><span class="section"><a href="#_importing_data_into_accumulo">7.2.14. Importing Data Into Accumulo</a></span></dt><dt><span class="section"><a href="#_additional_import_configuration_properties">7.2.15. Additional Import Configuration Properties</a></span></dt></dl></dd><dt><span class="section"><a href="#_example_invocations">7.3. Example Invocations</a></span></dt></dl></div><div class="section" title="7.1. Purpose"><div class="titlepage"><div><div><h3 class="title"><a name="_purpose"></a>7.1. Purpose</h3></div></div></div><p>The <code class="literal">import</code> tool imports an individual table from an RDBMS to HDFS.
Each row from a table is represented as a separate record in HDFS.
Records can be stored as text files (one record per line), or in
binary representation as Avro or SequenceFiles.</p></div><div class="section" title="7.2. Syntax"><div class="titlepage"><div><div><h3 class="title"><a name="_syntax"></a>7.2. Syntax</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_connecting_to_a_database_server">7.2.1. Connecting to a Database Server</a></span></dt><dt><span class="section"><a href="#_selecting_the_data_to_import">7.2.2. Selecting the Data to Import</a></span></dt><dt><span class="section"><a href="#_free_form_query_imports">7.2.3. Free-form Query Imports</a></span></dt><dt><span class="section"><a href="#_controlling_parallelism">7.2.4. Controlling Parallelism</a></span></dt><dt><span class="section"><a href="#_controlling_distributed_cache">7.2.5. Controlling Distributed Cache</a></span></dt><dt><span class="section"><a href="#_controlling_the_import_process">7.2.6. Controlling the Import Process</a></span></dt><dt><span class="section"><a href="#_controlling_transaction_isolation">7.2.7. Controlling transaction isolation</a></span></dt><dt><span class="section"><a href="#_controlling_type_mapping">7.2.8. Controlling type mapping</a></span></dt><dt><span class="section"><a href="#_incremental_imports">7.2.9. Incremental Imports</a></span></dt><dt><span class="section"><a href="#_file_formats">7.2.10. File Formats</a></span></dt><dt><span class="section"><a href="#_large_objects">7.2.11. Large Objects</a></span></dt><dt><span class="section"><a href="#_importing_data_into_hive">7.2.12. Importing Data Into Hive</a></span></dt><dt><span class="section"><a href="#_importing_data_into_hbase">7.2.13. Importing Data Into HBase</a></span></dt><dt><span class="section"><a href="#_importing_data_into_accumulo">7.2.14. Importing Data Into Accumulo</a></span></dt><dt><span class="section"><a href="#_additional_import_configuration_properties">7.2.15. Additional Import Configuration Properties</a></span></dt></dl></div><pre class="screen">$ sqoop import (generic-args) (import-args)
$ sqoop-import (generic-args) (import-args)</pre><p>While the Hadoop generic arguments must precede any import arguments,
you can type the import arguments in any order with respect to one
another.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>In this document, arguments are grouped into collections
organized by function. Some collections are present in several tools
(for example, the "common" arguments). An extended description of their
functionality is given only on the first presentation in this
document.</p></td></tr></table></div><div class="table"><a name="idp34104"></a><p class="title"><b>Table 1. Common arguments</b></p><div class="table-contents"><table summary="Common arguments" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connect &lt;jdbc-uri&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify JDBC connect string
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connection-manager &lt;class-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify connection manager class to                                          use
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--driver &lt;class-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Manually specify JDBC driver class                                          to use
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hadoop-mapred-home &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Override $HADOOP_MAPRED_HOME
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--help</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Print usage instructions
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--password-file</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set path for a file containing the                                          authentication password
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">-P</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Read password from console
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--password &lt;password&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set authentication password
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--username &lt;username&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set authentication username
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--verbose</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Print more information while working
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connection-param-file &lt;filename&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Optional properties file that                                          provides connection parameters
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--relaxed-isolation</code>
    </td><td style="" align="left">
    Set connection transaction isolation                                          to read uncommitted for the mappers.
    </td></tr></tbody></table></div></div><br class="table-break"><div class="section" title="7.2.1. Connecting to a Database Server"><div class="titlepage"><div><div><h4 class="title"><a name="_connecting_to_a_database_server"></a>7.2.1. Connecting to a Database Server</h4></div></div></div><p>Sqoop is designed to import tables from a database into HDFS. To do
so, you must specify a <span class="emphasis"><em>connect string</em></span> that describes how to connect to the
database. The <span class="emphasis"><em>connect string</em></span> is similar to a URL, and is communicated to
Sqoop with the <code class="literal">--connect</code> argument. This describes the server and
database to connect to; it may also specify the port. For example:</p><pre class="screen">$ sqoop import --connect jdbc:mysql://database.example.com/employees</pre><p>This string will connect to a MySQL database named <code class="literal">employees</code> on the
host <code class="literal">database.example.com</code>. It&#8217;s important that you <span class="strong"><strong>do not</strong></span> use the URL
<code class="literal">localhost</code> if you intend to use Sqoop with a distributed Hadoop
cluster. The connect string you supply will be used on TaskTracker nodes
throughout your MapReduce cluster; if you specify the
literal name <code class="literal">localhost</code>, each node will connect to a different
database (or more likely, no database at all). Instead, you should use
the full hostname or IP address of the database host that can be seen
by all your remote nodes.</p><p>You might need to authenticate against the database before you can
access it. You can use the <code class="literal">--username</code> to supply a username to the database.
Sqoop provides couple of different ways to supply a password,
secure and non-secure, to the database which is detailed below.</p><p title="Secure way of supplying password to the database"><b>Secure way of supplying password to the database. </b>You should save the password in a file on the users home directory with 400
permissions and specify the path to that file using the <span class="strong"><strong><code class="literal">--password-file</code></strong></span>
argument, and is the preferred method of entering credentials. Sqoop will
then read the password from the file and pass it to the MapReduce cluster
using secure means with out exposing the password in the job configuration.
The file containing the password can either be on the Local FS or HDFS.
For example:</p><pre class="screen">$ sqoop import --connect jdbc:mysql://database.example.com/employees \
    --username venkatesh --password-file ${user.home}/.password</pre><div class="warning" title="Warning" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Warning"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Warning]" src="images/warning.png"></td><th align="left">Warning</th></tr><tr><td align="left" valign="top"><p>Sqoop will read entire content of the password file and use it as
a password. This will include any trailing white space characters such as
new line characters that are added by default by most of the text editors.
You need to make sure that your password file contains only characters
that belongs to your password. On the command line you can use command
<code class="literal">echo</code> with switch <code class="literal">-n</code> to store password without any trailing white space
characters. For example to store password <code class="literal">secret</code> you would call
<code class="literal">echo -n "secret" &gt; password.file</code>.</p></td></tr></table></div><p>Another way of supplying passwords is using the <code class="literal">-P</code> argument which will
read a password from a console prompt.</p><p title="Protecting password from preying eyes"><b>Protecting password from preying eyes. </b>Hadoop 2.6.0 provides an API to separate password storage from applications.
This API is called the credential provided API and there is a new
<code class="literal">credential</code>  command line tool to manage passwords and their aliases.
The passwords are stored with their aliases in a keystore that is password
protected.   The keystore password can be the provided to a password prompt
on the command line, via an environment variable or defaulted to a software
defined constant.   Please check the Hadoop documentation on the usage
of this facility.</p><p>Once the password is stored using the Credential Provider facility and
the Hadoop configuration has been suitably updated, all applications can
optionally use the alias in place of the actual password and at runtime
resolve the alias for the password to use.</p><p>Since the keystore or similar technology used for storing the credential
provider is shared across components, passwords for various applications,
various database and other passwords can be securely stored in them and only
the alias needs to be exposed in configuration files, protecting the password
from being visible.</p><p>Sqoop has been enhanced to allow usage of this funcionality if it is
available in the underlying Hadoop version being used.   One new option
has been introduced to provide the alias on the command line instead of the
actual password (--password-alias).  The argument value this option is
the alias on the storage associated with the actual password.
Example usage is as follows:</p><pre class="screen">$ sqoop import --connect jdbc:mysql://database.example.com/employees \
    --username dbuser --password-alias mydb.password.alias</pre><p>Similarly, if the command line option is not preferred, the alias can be saved
in the file provided with --password-file option.  Along with this, the
Sqoop configuration parameter org.apache.sqoop.credentials.loader.class
should be set to the classname that provides the alias resolution:
<code class="literal">org.apache.sqoop.util.password.CredentialProviderPasswordLoader</code></p><p>Example usage is as follows (assuming .password.alias has the alias for
the real password) :</p><pre class="screen">$ sqoop import --connect jdbc:mysql://database.example.com/employees \
    --username dbuser --password-file ${user.home}/.password-alias</pre><div class="warning" title="Warning" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Warning"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Warning]" src="images/warning.png"></td><th align="left">Warning</th></tr><tr><td align="left" valign="top"><p>The <code class="literal">--password</code> parameter is insecure, as other users may
be able to read your password from the command-line arguments via
the output of programs such as <code class="literal">ps</code>. The <span class="strong"><strong><code class="literal">-P</code></strong></span> argument is the preferred
method over using the <code class="literal">--password</code> argument. Credentials may still be
transferred between nodes of the MapReduce cluster using insecure means.
For example:</p></td></tr></table></div><pre class="screen">$ sqoop import --connect jdbc:mysql://database.example.com/employees \
    --username aaron --password 12345</pre><p>Sqoop automatically supports several databases, including MySQL.  Connect
strings beginning with <code class="literal">jdbc:mysql://</code> are handled automatically in Sqoop.  (A
full list of databases with built-in support is provided in the "Supported
Databases" section. For some, you may need to install the JDBC driver
yourself.)</p><p>You can use Sqoop with any other
JDBC-compliant database. First, download the appropriate JDBC
driver for the type of database you want to import, and install the .jar
file in the <code class="literal">$SQOOP_HOME/lib</code> directory on your client machine. (This will
be <code class="literal">/usr/lib/sqoop/lib</code> if you installed from an RPM or Debian package.)
Each driver <code class="literal">.jar</code> file also has a specific driver class which defines
the entry-point to the driver. For example, MySQL&#8217;s Connector/J library has
a driver class of <code class="literal">com.mysql.jdbc.Driver</code>. Refer to your database
vendor-specific documentation to determine the main driver class.
This class must be provided as an argument to Sqoop with <code class="literal">--driver</code>.</p><p>For example, to connect to a SQLServer database, first download the driver from
microsoft.com and install it in your Sqoop lib path.</p><p>Then run Sqoop. For example:</p><pre class="screen">$ sqoop import --driver com.microsoft.jdbc.sqlserver.SQLServerDriver \
    --connect &lt;connect-string&gt; ...</pre><p>When connecting to a database using JDBC, you can optionally specify extra
JDBC parameters via a property file using the option
<code class="literal">--connection-param-file</code>. The contents of this file are parsed as standard
Java properties and passed into the driver while creating a connection.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>The parameters specified via the optional property file are only
applicable to JDBC connections. Any fastpath connectors that use connections
other than JDBC will ignore these parameters.</p></td></tr></table></div><div class="table"><a name="idp3494776"></a><p class="title"><b>Table 2. Validation arguments <a class="link" href="#validation" title="11. validation">More Details</a></b></p><div class="table-contents"><table summary="Validation arguments More Details" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--validate</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Enable validation of data copied,                                             supports single table copy only.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--validator &lt;class-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify validator class to use.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--validation-threshold &lt;class-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify validation threshold class                                             to use.
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--validation-failurehandler &lt;class-name&gt;</code>
    </td><td style="" align="left">
    Specify validation failure                                             handler class to use.
    </td></tr></tbody></table></div></div><br class="table-break"><div class="table"><a name="idp3508656"></a><p class="title"><b>Table 3. Import control arguments:</b></p><div class="table-contents"><table summary="Import control arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--append</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Append data to an existing dataset                                  in HDFS
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--as-avrodatafile</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Imports data to Avro Data Files
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--as-sequencefile</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Imports data to SequenceFiles
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--as-textfile</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Imports data as plain text (default)
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--as-parquetfile</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Imports data to Parquet Files
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--boundary-query &lt;statement&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Boundary query to use for creating splits
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--columns &lt;col,col,col&#8230;&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Columns to import from table
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--delete-target-dir</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Delete the import target directory                                  if it exists
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--direct</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Use direct connector if exists for the database
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--fetch-size &lt;n&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Number of entries to read from database                                  at once.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--inline-lob-limit &lt;n&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set the maximum size for an inline LOB
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">-m,--num-mappers &lt;n&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Use <span class="emphasis"><em>n</em></span> map tasks to import in parallel
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">-e,--query &lt;statement&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Import the results of <span class="emphasis"><em><code class="literal">statement</code></em></span>.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--split-by &lt;column-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Column of the table used to split work                                  units.  Cannot be used with                                  <code class="literal">--autoreset-to-one-mapper</code> option.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--autoreset-to-one-mapper</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Import should use one mapper if a table                                  has no primary key and no split-by column                                  is provided.  Cannot be used with                                  <code class="literal">--split-by &lt;col&gt;</code> option.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--table &lt;table-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Table to read
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--target-dir &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    HDFS destination dir
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--warehouse-dir &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    HDFS parent for table destination
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--where &lt;where clause&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    WHERE clause to use during import
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">-z,--compress</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Enable compression
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--compression-codec &lt;c&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Use Hadoop codec (default gzip)
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--null-string &lt;null-string&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    The string to be written for a null                                  value for string columns
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--null-non-string &lt;null-string&gt;</code>
    </td><td style="" align="left">
    The string to be written for a null                                  value for non-string columns
    </td></tr></tbody></table></div></div><br class="table-break"><p>The <code class="literal">--null-string</code> and <code class="literal">--null-non-string</code> arguments are optional.\
If not specified, then the string "null" will be used.</p></div><div class="section" title="7.2.2. Selecting the Data to Import"><div class="titlepage"><div><div><h4 class="title"><a name="_selecting_the_data_to_import"></a>7.2.2. Selecting the Data to Import</h4></div></div></div><p>Sqoop typically imports data in a table-centric fashion. Use the
<code class="literal">--table</code> argument to select the table to import. For example, <code class="literal">--table
employees</code>. This argument can also identify a <code class="literal">VIEW</code> or other table-like
entity in a database.</p><p>By default, all columns within a table are selected for import.
Imported data is written to HDFS in its "natural order;" that is, a
table containing columns A, B, and C result in an import of data such
as:</p><pre class="screen">A1,B1,C1
A2,B2,C2
...</pre><p>You can select a subset of columns and control their ordering by using
the <code class="literal">--columns</code> argument. This should include a comma-delimited list
of columns to import. For example: <code class="literal">--columns "name,employee_id,jobtitle"</code>.</p><p>You can control which rows are imported by adding a SQL <code class="literal">WHERE</code> clause
to the import statement. By default, Sqoop generates statements of the
form <code class="literal">SELECT &lt;column list&gt; FROM &lt;table name&gt;</code>. You can append a
<code class="literal">WHERE</code> clause to this with the <code class="literal">--where</code> argument. For example: <code class="literal">--where
"id &gt; 400"</code>. Only rows where the <code class="literal">id</code> column has a value greater than
400 will be imported.</p><p>By default sqoop will use query <code class="literal">select min(&lt;split-by&gt;), max(&lt;split-by&gt;) from
&lt;table name&gt;</code> to find out boundaries for creating splits. In some cases this query
is not the most optimal so you can specify any arbitrary query returning two
numeric columns using <code class="literal">--boundary-query</code> argument.</p></div><div class="section" title="7.2.3. Free-form Query Imports"><div class="titlepage"><div><div><h4 class="title"><a name="_free_form_query_imports"></a>7.2.3. Free-form Query Imports</h4></div></div></div><p>Sqoop can also import the result set of an arbitrary SQL query. Instead of
using the <code class="literal">--table</code>, <code class="literal">--columns</code> and <code class="literal">--where</code> arguments, you can specify
a SQL statement with the <code class="literal">--query</code> argument.</p><p>When importing a free-form query, you must specify a destination directory
with <code class="literal">--target-dir</code>.</p><p>If you want to import the results of a query in parallel, then each map task
will need to execute a copy of the query, with results partitioned by bounding
conditions inferred by Sqoop. Your query must include the token <code class="literal">$CONDITIONS</code>
which each Sqoop process will replace with a unique condition expression.
You must also select a splitting column with <code class="literal">--split-by</code>.</p><p>For example:</p><pre class="screen">$ sqoop import \
  --query 'SELECT a.*, b.* FROM a JOIN b on (a.id == b.id) WHERE $CONDITIONS' \
  --split-by a.id --target-dir /user/foo/joinresults</pre><p>Alternately, the query can be executed once and imported serially, by
specifying a single map task with <code class="literal">-m 1</code>:</p><pre class="screen">$ sqoop import \
  --query 'SELECT a.*, b.* FROM a JOIN b on (a.id == b.id) WHERE $CONDITIONS' \
  -m 1 --target-dir /user/foo/joinresults</pre><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>If you are issuing the query wrapped with double quotes ("),
you will have to use <code class="literal">\$CONDITIONS</code> instead of just <code class="literal">$CONDITIONS</code>
to disallow your shell from treating it as a shell variable.
For example, a double quoted query may look like:
<code class="literal">"SELECT * FROM x WHERE a='foo' AND \$CONDITIONS"</code></p></td></tr></table></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>The facility of using free-form query in the current version of Sqoop
is limited to simple queries where there are no ambiguous projections and
no <code class="literal">OR</code> conditions in the <code class="literal">WHERE</code> clause. Use of complex queries such as
queries that have sub-queries or joins leading to ambiguous projections can
lead to unexpected results.</p></td></tr></table></div></div><div class="section" title="7.2.4. Controlling Parallelism"><div class="titlepage"><div><div><h4 class="title"><a name="_controlling_parallelism"></a>7.2.4. Controlling Parallelism</h4></div></div></div><p>Sqoop imports data in parallel from most database sources. You can
specify the number
of map tasks (parallel processes) to use to perform the import by
using the <code class="literal">-m</code> or <code class="literal">--num-mappers</code> argument. Each of these arguments
takes an integer value which corresponds to the degree of parallelism
to employ. By default, four tasks are used. Some databases may see
improved performance by increasing this value to 8 or 16. Do not
increase the degree of parallelism greater than that available within
your MapReduce cluster; tasks will run serially and will likely
increase the amount of time required to perform the import. Likewise,
do not increase the degree of parallism higher than that which your
database can reasonably support. Connecting 100 concurrent clients to
your database may increase the load on the database server to a point
where performance suffers as a result.</p><p>When performing parallel imports, Sqoop needs a criterion by which it
can split the workload. Sqoop uses a <span class="emphasis"><em>splitting column</em></span> to split the
workload. By default, Sqoop will identify the primary key column (if
present) in a table and use it as the splitting column. The low and
high values for the splitting column are retrieved from the database,
and the map tasks operate on evenly-sized components of the total
range. For example, if you had a table with a primary key column of
<code class="literal">id</code> whose minimum value was 0 and maximum value was 1000, and Sqoop
was directed to use 4 tasks, Sqoop would run four processes which each
execute SQL statements of the form <code class="literal">SELECT * FROM sometable WHERE id
&gt;= lo AND id &lt; hi</code>, with <code class="literal">(lo, hi)</code> set to (0, 250), (250, 500),
(500, 750), and (750, 1001) in the different tasks.</p><p>If the actual values for the primary key are not uniformly distributed
across its range, then this can result in unbalanced tasks. You should
explicitly choose a different column with the <code class="literal">--split-by</code> argument.
For example, <code class="literal">--split-by employee_id</code>. Sqoop cannot currently split on
multi-column indices. If your table has no index column, or has a
multi-column key, then you must also manually choose a splitting
column.</p><p>If a table does not have a primary key defined and the <code class="literal">--split-by &lt;col&gt;</code>
is not provided, then import will fail unless the number
of mappers is explicitly set to one with the <code class="literal">--num-mappers 1</code> option
or the <code class="literal">--autoreset-to-one-mapper</code> option is used.  The option
<code class="literal">--autoreset-to-one-mapper</code> is typically used with the import-all-tables
tool to automatically handle tables without a primary key in a schema.</p></div><div class="section" title="7.2.5. Controlling Distributed Cache"><div class="titlepage"><div><div><h4 class="title"><a name="_controlling_distributed_cache"></a>7.2.5. Controlling Distributed Cache</h4></div></div></div><p>Sqoop will copy the jars in $SQOOP_HOME/lib folder to job cache every
time when start a Sqoop job. When launched by Oozie this is unnecessary
since Oozie use its own Sqoop share lib which keeps Sqoop dependencies
in the distributed cache. Oozie will do the localization on each
worker node for the Sqoop dependencies only once during the first Sqoop
job and reuse the jars on worker node for subsquencial jobs. Using
option <code class="literal">--skip-dist-cache</code> in Sqoop command when launched by Oozie will
skip the step which Sqoop copies its dependencies to job cache and save
massive I/O.</p></div><div class="section" title="7.2.6. Controlling the Import Process"><div class="titlepage"><div><div><h4 class="title"><a name="_controlling_the_import_process"></a>7.2.6. Controlling the Import Process</h4></div></div></div><p>By default, the import process will use JDBC which provides a
reasonable cross-vendor import channel. Some databases can perform
imports in a more high-performance fashion by using database-specific
data movement tools. For example, MySQL provides the <code class="literal">mysqldump</code> tool
which can export data from MySQL to other systems very quickly. By
supplying the <code class="literal">--direct</code> argument, you are specifying that Sqoop
should attempt the direct import channel. This channel may be
higher performance than using JDBC.</p><p>Details about use of direct mode with each specific RDBMS, installation requirements, available
options and limitations can be found in <a class="xref" href="#connectors" title="25. Notes for specific connectors">Section 25, &#8220;Notes for specific connectors&#8221;</a>.</p><p>By default, Sqoop will import a table named <code class="literal">foo</code> to a directory named
<code class="literal">foo</code> inside your home directory in HDFS. For example, if your
username is <code class="literal">someuser</code>, then the import tool will write to
<code class="literal">/user/someuser/foo/(files)</code>. You can adjust the parent directory of
the import with the <code class="literal">--warehouse-dir</code> argument. For example:</p><pre class="screen">$ sqoop import --connnect &lt;connect-str&gt; --table foo --warehouse-dir /shared \
    ...</pre><p>This command would write to a set of files in the <code class="literal">/shared/foo/</code> directory.</p><p>You can also explicitly choose the target directory, like so:</p><pre class="screen">$ sqoop import --connnect &lt;connect-str&gt; --table foo --target-dir /dest \
    ...</pre><p>This will import the files into the <code class="literal">/dest</code> directory. <code class="literal">--target-dir</code> is
incompatible with <code class="literal">--warehouse-dir</code>.</p><p>When using direct mode, you can specify additional arguments which
should be passed to the underlying tool. If the argument
<code class="literal">--</code> is given on the command-line, then subsequent arguments are sent
directly to the underlying tool. For example, the following adjusts
the character set used by <code class="literal">mysqldump</code>:</p><pre class="screen">$ sqoop import --connect jdbc:mysql://server.foo.com/db --table bar \
    --direct -- --default-character-set=latin1</pre><p>By default, imports go to a new target location. If the destination directory
already exists in HDFS, Sqoop will refuse to import and overwrite that
directory&#8217;s contents. If you use the <code class="literal">--append</code> argument, Sqoop will import
data to a temporary directory and then rename the files into the normal
target directory in a manner that does not conflict with existing filenames
in that directory.</p></div><div class="section" title="7.2.7. Controlling transaction isolation"><div class="titlepage"><div><div><h4 class="title"><a name="_controlling_transaction_isolation"></a>7.2.7. Controlling transaction isolation</h4></div></div></div><p>By default, Sqoop uses the read committed transaction isolation in the mappers
to import data.  This may not be the ideal in all ETL workflows and it may
desired to reduce the isolation guarantees.   The <code class="literal">--relaxed-isolation</code> option
can be used to instruct Sqoop to use read uncommitted isolation level.</p><p>The <code class="literal">read-uncommitted</code> isolation level is not supported on all databases
(for example, Oracle), so specifying the option <code class="literal">--relaxed-isolation</code>
may not be supported on all databases.</p></div><div class="section" title="7.2.8. Controlling type mapping"><div class="titlepage"><div><div><h4 class="title"><a name="_controlling_type_mapping"></a>7.2.8. Controlling type mapping</h4></div></div></div><p>Sqoop is preconfigured to map most SQL types to appropriate Java or Hive
representatives. However the default mapping might not be suitable for
everyone and might be overridden by <code class="literal">--map-column-java</code> (for changing
mapping to Java) or <code class="literal">--map-column-hive</code> (for changing Hive mapping).</p><div class="table"><a name="idp3580872"></a><p class="title"><b>Table 4. Parameters for overriding mapping</b></p><div class="table-contents"><table summary="Parameters for overriding mapping" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--map-column-java &lt;mapping&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Override mapping from SQL to Java type                                  for configured columns.
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--map-column-hive &lt;mapping&gt;</code>
    </td><td style="" align="left">
    Override mapping from SQL to Hive type                                  for configured columns.
    </td></tr></tbody></table></div></div><br class="table-break"><p>Sqoop is expecting comma separated list of mapping in form &lt;name of column&gt;=&lt;new type&gt;. For example:</p><pre class="screen">$ sqoop import ... --map-column-java id=String,value=Integer</pre><p>Sqoop will rise exception in case that some configured mapping will not be used.</p></div><div class="section" title="7.2.9. Incremental Imports"><div class="titlepage"><div><div><h4 class="title"><a name="_incremental_imports"></a>7.2.9. Incremental Imports</h4></div></div></div><p>Sqoop provides an incremental import mode which can be used to retrieve
only rows newer than some previously-imported set of rows.</p><p>The following arguments control incremental imports:</p><div class="table"><a name="idp3588392"></a><p class="title"><b>Table 5. Incremental import arguments:</b></p><div class="table-contents"><table summary="Incremental import arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--check-column (col)</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specifies the column to be examined                               when determining which rows to import.                              (the column should not be of type                               CHAR/NCHAR/VARCHAR/VARNCHAR/                              LONGVARCHAR/LONGNVARCHAR)
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--incremental (mode)</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specifies how Sqoop determines which                               rows are new. Legal values for <code class="literal">mode</code>                              include <code class="literal">append</code> and <code class="literal">lastmodified</code>.
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--last-value (value)</code>
    </td><td style="" align="left">
    Specifies the maximum value of the                               check column from the previous import.
    </td></tr></tbody></table></div></div><br class="table-break"><p>Sqoop supports two types of incremental imports: <code class="literal">append</code> and <code class="literal">lastmodified</code>.
You can use the <code class="literal">--incremental</code> argument to specify the type of incremental
import to perform.</p><p>You should specify <code class="literal">append</code> mode when importing a table where new rows are
continually being added with increasing row id values. You specify the column
containing the row&#8217;s id with <code class="literal">--check-column</code>. Sqoop imports rows where the
check column has a value greater than the one specified with <code class="literal">--last-value</code>.</p><p>An alternate table update strategy supported by Sqoop is called <code class="literal">lastmodified</code>
mode. You should use this when rows of the source table may be updated, and
each such update will set the value of a last-modified column to the current
timestamp.  Rows where the check column holds a timestamp more recent than the
timestamp specified with <code class="literal">--last-value</code> are imported.</p><p>At the end of an incremental import, the value which should be specified as
<code class="literal">--last-value</code> for a subsequent import is printed to the screen. When running
a subsequent import, you should specify <code class="literal">--last-value</code> in this way to ensure
you import only the new or updated data. This is handled automatically by
creating an incremental import as a saved job, which is the preferred
mechanism for performing a recurring incremental import. See the section on
saved jobs later in this document for more information.</p></div><div class="section" title="7.2.10. File Formats"><div class="titlepage"><div><div><h4 class="title"><a name="_file_formats"></a>7.2.10. File Formats</h4></div></div></div><p>You can import data in one of two file formats: delimited text or
SequenceFiles.</p><p>Delimited text is the default import format. You can also specify it
explicitly by using the <code class="literal">--as-textfile</code> argument. This argument will write
string-based representations of each record to the output files, with
delimiter characters between individual columns and rows. These
delimiters may be commas, tabs, or other characters. (The delimiters
can be selected; see "Output line formatting arguments.") The
following is the results of an example text-based import:</p><pre class="screen">1,here is a message,2010-05-01
2,happy new year!,2010-01-01
3,another message,2009-11-12</pre><p>Delimited text is appropriate for most non-binary data types. It also
readily supports further manipulation by other tools, such as Hive.</p><p>SequenceFiles are a binary format that store individual records in
custom record-specific data types. These data types are manifested as
Java classes. Sqoop will automatically generate these data types for
you. This format supports exact storage of all data in binary
representations, and is appropriate for storing binary data
(for example, <code class="literal">VARBINARY</code> columns), or data that will be principly
manipulated by custom MapReduce programs (reading from SequenceFiles
is higher-performance than reading from text files, as records do not
need to be parsed).</p><p>Avro data files are a compact, efficient binary format that provides
interoperability with applications written in other programming
languages.  Avro also supports versioning, so that when, e.g., columns
are added or removed from a table, previously imported data files can
be processed along with new ones.</p><p>By default, data is not compressed. You can compress your data by
using the deflate (gzip) algorithm with the <code class="literal">-z</code> or <code class="literal">--compress</code>
argument, or specify any Hadoop compression codec using the
<code class="literal">--compression-codec</code> argument. This applies to SequenceFile, text,
and Avro files.</p></div><div class="section" title="7.2.11. Large Objects"><div class="titlepage"><div><div><h4 class="title"><a name="_large_objects"></a>7.2.11. Large Objects</h4></div></div></div><p>Sqoop handles large objects (<code class="literal">BLOB</code> and <code class="literal">CLOB</code> columns) in particular
ways. If this data is truly large, then these columns should not be
fully materialized in memory for manipulation, as most columns are.
Instead, their data is handled in a streaming fashion. Large objects
can be stored inline with the rest of the data, in which case they are
fully materialized in memory on every access, or they can be stored in
a secondary storage file linked to the primary data storage. By
default, large objects less than 16 MB in size are stored inline with
the rest of the data. At a larger size, they are stored in files in
the <code class="literal">_lobs</code> subdirectory of the import target directory. These files
are stored in a separate format optimized for large record storage,
which can accomodate records of up to 2^63 bytes each. The size at
which lobs spill into separate files is controlled by the
<code class="literal">--inline-lob-limit</code> argument, which takes a parameter specifying the
largest lob size to keep inline, in bytes. If you set the inline LOB
limit to 0, all large objects will be placed in external
storage.</p><div class="table"><a name="idp3612448"></a><p class="title"><b>Table 6. Output line formatting arguments:</b></p><div class="table-contents"><table summary="Output line formatting arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--enclosed-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets a required field enclosing                                    character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--escaped-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the escape character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--fields-terminated-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the field separator character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--lines-terminated-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the end-of-line character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--mysql-delimiters</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Uses MySQL&#8217;s default delimiter set:                                   fields: <code class="literal">,</code>  lines: <code class="literal">\n</code>                                    escaped-by: <code class="literal">\</code>                                    optionally-enclosed-by: <code class="literal">'</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--optionally-enclosed-by &lt;char&gt;</code>
    </td><td style="" align="left">
    Sets a field enclosing character
    </td></tr></tbody></table></div></div><br class="table-break"><p>When importing to delimited files, the choice of delimiter is
important. Delimiters which appear inside string-based fields may
cause ambiguous parsing of the imported data by subsequent analysis
passes. For example, the string <code class="literal">"Hello, pleased to meet you"</code> should
not be imported with the end-of-field delimiter set to a comma.</p><p>Delimiters may be specified as:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
a character (<code class="literal">--fields-terminated-by X</code>)
</li><li class="listitem"><p class="simpara">
an escape character (<code class="literal">--fields-terminated-by \t</code>). Supported escape
  characters are:
</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
<code class="literal">\b</code> (backspace)
</li><li class="listitem">
<code class="literal">\n</code> (newline)
</li><li class="listitem">
<code class="literal">\r</code> (carriage return)
</li><li class="listitem">
<code class="literal">\t</code> (tab)
</li><li class="listitem">
<code class="literal">\"</code> (double-quote)
</li><li class="listitem">
<code class="literal">\\'</code> (single-quote)
</li><li class="listitem">
<code class="literal">\\</code> (backslash)
</li><li class="listitem">
<code class="literal">\0</code> (NUL) - This will insert NUL characters between fields or lines,
  or will disable enclosing/escaping if used for one of the <code class="literal">--enclosed-by</code>,
  <code class="literal">--optionally-enclosed-by</code>, or <code class="literal">--escaped-by</code> arguments.
</li></ul></div></li><li class="listitem">
The octal representation of a UTF-8 character&#8217;s code point. This
  should be of the form <code class="literal">\0ooo</code>, where <span class="emphasis"><em>ooo</em></span> is the octal value.
  For example, <code class="literal">--fields-terminated-by \001</code> would yield the <code class="literal">^A</code> character.
</li><li class="listitem">
The hexadecimal representation of a UTF-8 character&#8217;s code point. This
  should be of the form <code class="literal">\0xhhh</code>, where <span class="emphasis"><em>hhh</em></span> is the hex value.
  For example, <code class="literal">--fields-terminated-by \0x10</code> would yield the carriage
  return character.
</li></ul></div><p>The default delimiters are a comma (<code class="literal">,</code>) for fields, a newline (<code class="literal">\n</code>) for records, no quote
character, and no escape character. Note that this can lead to
ambiguous/unparsible records if you import database records containing
commas or newlines in the field data. For unambiguous parsing, both must
be enabled. For example, via <code class="literal">--mysql-delimiters</code>.</p><p>If unambiguous delimiters cannot be presented, then use <span class="emphasis"><em>enclosing</em></span> and
<span class="emphasis"><em>escaping</em></span> characters. The combination of (optional)
enclosing and escaping characters will allow unambiguous parsing of
lines. For example, suppose one column of a dataset contained the
following values:</p><pre class="screen">Some string, with a comma.
Another "string with quotes"</pre><p>The following arguments would provide delimiters which can be
unambiguously parsed:</p><pre class="screen">$ sqoop import --fields-terminated-by , --escaped-by \\ --enclosed-by '\"' ...</pre><p>(Note that to prevent the shell from mangling the enclosing character,
we have enclosed that argument itself in single-quotes.)</p><p>The result of the above arguments applied to the above dataset would
be:</p><pre class="screen">"Some string, with a comma.","1","2","3"...
"Another \"string with quotes\"","4","5","6"...</pre><p>Here the imported strings are shown in the context of additional
columns (<code class="literal">"1","2","3"</code>, etc.) to demonstrate the full effect of enclosing
and escaping. The enclosing character is only strictly necessary when
delimiter characters appear in the imported text. The enclosing
character can therefore be specified as optional:</p><pre class="screen">$ sqoop import --optionally-enclosed-by '\"' (the rest as above)...</pre><p>Which would result in the following import:</p><pre class="screen">"Some string, with a comma.",1,2,3...
"Another \"string with quotes\"",4,5,6...</pre><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>Even though Hive supports escaping characters, it does not
handle escaping of new-line character. Also, it does not support
the notion of enclosing characters that may include field delimiters
in the enclosed string.  It is therefore recommended that you choose
unambiguous field and record-terminating delimiters without the help
of escaping and enclosing characters when working with Hive; this is
due to limitations of Hive&#8217;s input parsing abilities.</p></td></tr></table></div><p>The <code class="literal">--mysql-delimiters</code> argument is a shorthand argument which uses
the default delimiters for the <code class="literal">mysqldump</code> program.
If you use the <code class="literal">mysqldump</code> delimiters in conjunction with a
direct-mode import (with <code class="literal">--direct</code>), very fast imports can be
achieved.</p><p>While the choice of delimiters is most important for a text-mode
import, it is still relevant if you import to SequenceFiles with
<code class="literal">--as-sequencefile</code>. The generated class' <code class="literal">toString()</code> method
will use the delimiters you specify, so subsequent formatting of
the output data will rely on the delimiters you choose.</p><div class="table"><a name="idp3647416"></a><p class="title"><b>Table 7. Input parsing arguments:</b></p><div class="table-contents"><table summary="Input parsing arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--input-enclosed-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets a required field encloser
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--input-escaped-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the input escape                                          character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--input-fields-terminated-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the input field separator
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--input-lines-terminated-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the input end-of-line                                          character
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--input-optionally-enclosed-by &lt;char&gt;</code>
    </td><td style="" align="left">
    Sets a field enclosing                                          character
    </td></tr></tbody></table></div></div><br class="table-break"><p>When Sqoop imports data to HDFS, it generates a Java class which can
reinterpret the text files that it creates when doing a
delimited-format import. The delimiters are chosen with arguments such
as <code class="literal">--fields-terminated-by</code>; this controls both how the data is
written to disk, and how the generated <code class="literal">parse()</code> method reinterprets
this data. The delimiters used by the <code class="literal">parse()</code> method can be chosen
independently of the output arguments, by using
<code class="literal">--input-fields-terminated-by</code>, and so on. This is useful, for example, to
generate classes which can parse records created with one set of
delimiters, and emit the records to a different set of files using a
separate set of delimiters.</p><div class="table"><a name="idp3658232"></a><p class="title"><b>Table 8. Hive arguments:</b></p><div class="table-contents"><table summary="Hive arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-home &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Override <code class="literal">$HIVE_HOME</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-import</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Import tables into Hive (Uses Hive&#8217;s                               default delimiters if none are set.)
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-overwrite</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Overwrite existing data in the Hive table.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--create-hive-table</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    If set, then the job will fail if the target hive
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    table exits. By default this property is false.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-table &lt;table-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the table name to use when importing                              to Hive.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-drop-import-delims</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Drops <span class="emphasis"><em>\n</em></span>, <span class="emphasis"><em>\r</em></span>, and <span class="emphasis"><em>\01</em></span> from string                              fields when importing to Hive.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-delims-replacement</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Replace <span class="emphasis"><em>\n</em></span>, <span class="emphasis"><em>\r</em></span>, and <span class="emphasis"><em>\01</em></span> from string                              fields with user defined string when importing to Hive.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-partition-key</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Name of a hive field to partition are                               sharded on
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-partition-value &lt;v&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    String-value that serves as partition key                              for this imported into hive in this job.
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--map-column-hive &lt;map&gt;</code>
    </td><td style="" align="left">
    Override default mapping from SQL type to                              Hive type for configured columns.
    </td></tr></tbody></table></div></div><br class="table-break"></div><div class="section" title="7.2.12. Importing Data Into Hive"><div class="titlepage"><div><div><h4 class="title"><a name="_importing_data_into_hive"></a>7.2.12. Importing Data Into Hive</h4></div></div></div><p>Sqoop&#8217;s import tool&#8217;s main function is to upload your data into files
in HDFS. If you have a Hive metastore associated with your HDFS
cluster, Sqoop can also import the data into Hive by generating and
executing a <code class="literal">CREATE TABLE</code> statement to define the data&#8217;s layout in
Hive. Importing data into Hive is as simple as adding the
<span class="strong"><strong><code class="literal">--hive-import</code></strong></span> option to your Sqoop command line.</p><p>If the Hive table already exists, you can specify the
<span class="strong"><strong><code class="literal">--hive-overwrite</code></strong></span> option to indicate that existing table in hive must
be replaced. After your data is imported into HDFS or this step is
omitted, Sqoop will generate a Hive script containing a <code class="literal">CREATE TABLE</code>
operation defining your columns using Hive&#8217;s types, and a <code class="literal">LOAD DATA INPATH</code>
statement to move the data files into Hive&#8217;s warehouse directory.</p><p>The script will be executed by calling
the installed copy of hive on the machine where Sqoop is run. If you have
multiple Hive installations, or <code class="literal">hive</code> is not in your <code class="literal">$PATH</code>, use the
<span class="strong"><strong><code class="literal">--hive-home</code></strong></span> option to identify the Hive installation directory.
Sqoop will use <code class="literal">$HIVE_HOME/bin/hive</code> from here.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>This function is incompatible with <code class="literal">--as-avrodatafile</code> and
<code class="literal">--as-sequencefile</code>.</p></td></tr></table></div><p>Even though Hive supports escaping characters, it does not
handle escaping of new-line character. Also, it does not support
the notion of enclosing characters that may include field delimiters
in the enclosed string.  It is therefore recommended that you choose
unambiguous field and record-terminating delimiters without the help
of escaping and enclosing characters when working with Hive; this is
due to limitations of Hive&#8217;s input parsing abilities. If you do use
<code class="literal">--escaped-by</code>, <code class="literal">--enclosed-by</code>, or <code class="literal">--optionally-enclosed-by</code> when
importing data into Hive, Sqoop will print a warning message.</p><p>Hive will have problems using Sqoop-imported data if your database&#8217;s
rows contain string fields that have Hive&#8217;s default row delimiters
(<code class="literal">\n</code> and <code class="literal">\r</code> characters) or column delimiters (<code class="literal">\01</code> characters)
present in them.  You can use the <code class="literal">--hive-drop-import-delims</code> option
to drop those characters on import to give Hive-compatible text data.
Alternatively, you can use the <code class="literal">--hive-delims-replacement</code> option
to replace those characters with a user-defined string on import to give
Hive-compatible text data.  These options should only be used if you use
Hive&#8217;s default delimiters and should not be used if different delimiters
are specified.</p><p>Sqoop will pass the field and record delimiters through to Hive. If you do
not set any delimiters and do use <code class="literal">--hive-import</code>, the field delimiter will
be set to <code class="literal">^A</code> and the record delimiter will be set to <code class="literal">\n</code> to be consistent
with Hive&#8217;s defaults.</p><p>Sqoop will by default import NULL values as string <code class="literal">null</code>. Hive is however
using string <code class="literal">\N</code> to denote <code class="literal">NULL</code> values and therefore predicates dealing
with <code class="literal">NULL</code> (like <code class="literal">IS NULL</code>) will not work correctly. You should append
parameters <code class="literal">--null-string</code> and <code class="literal">--null-non-string</code> in case of import job or
<code class="literal">--input-null-string</code> and <code class="literal">--input-null-non-string</code> in case of an export job if
you wish to properly preserve <code class="literal">NULL</code> values. Because sqoop is using those
parameters in generated code, you need to properly escape value <code class="literal">\N</code> to <code class="literal">\\N</code>:</p><pre class="screen">$ sqoop import  ... --null-string '\\N' --null-non-string '\\N'</pre><p>The table name used in Hive is, by default, the same as that of the
source table. You can control the output table name with the <code class="literal">--hive-table</code>
option.</p><p>Hive can put data into partitions for more efficient query
performance.  You can tell a Sqoop job to import data for Hive into a
particular partition by specifying the <code class="literal">--hive-partition-key</code> and
<code class="literal">--hive-partition-value</code> arguments.  The partition value must be a
string.  Please see the Hive documentation for more details on
partitioning.</p><p>You can import compressed tables into Hive using the <code class="literal">--compress</code> and
<code class="literal">--compression-codec</code> options. One downside to compressing tables imported
into Hive is that many codecs cannot be split for processing by parallel map
tasks. The lzop codec, however, does support splitting. When importing tables
with this codec, Sqoop will automatically index the files for splitting and
configuring a new Hive table with the correct InputFormat. This feature
currently requires that all partitions of a table be compressed with the lzop
codec.</p><div class="table"><a name="idp3695976"></a><p class="title"><b>Table 9. HBase arguments:</b></p><div class="table-contents"><table summary="HBase arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--column-family &lt;family&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the target column family for the import
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hbase-create-table</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    If specified, create missing HBase tables
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hbase-row-key &lt;col&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specifies which input column to use as the                              row key
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    In case, if input table contains composite
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    key, then &lt;col&gt; must be in the form of a
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    comma-separated list of composite key
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    attributes
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hbase-table &lt;table-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specifies an HBase table to use as the                               target instead of HDFS
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--hbase-bulkload</code>
    </td><td style="" align="left">
    Enables bulk loading
    </td></tr></tbody></table></div></div><br class="table-break"></div><div class="section" title="7.2.13. Importing Data Into HBase"><div class="titlepage"><div><div><h4 class="title"><a name="_importing_data_into_hbase"></a>7.2.13. Importing Data Into HBase</h4></div></div></div><p>Sqoop supports additional import targets beyond HDFS and Hive. Sqoop
can also import records into a table in HBase.</p><p>By specifying <code class="literal">--hbase-table</code>, you instruct Sqoop to import
to a table in HBase rather than a directory in HDFS. Sqoop will
import data to the table specified as the argument to <code class="literal">--hbase-table</code>.
Each row of the input table will be transformed into an HBase
<code class="literal">Put</code> operation to a row of the output table. The key for each row is
taken from a column of the input. By default Sqoop will use the split-by
column as the row key column. If that is not specified, it will try to
identify the primary key column, if any, of the source table. You can
manually specify the row key column with <code class="literal">--hbase-row-key</code>. Each output
column will be placed in the same column family, which must be specified
with <code class="literal">--column-family</code>.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>This function is incompatible with direct import (parameter
<code class="literal">--direct</code>).</p></td></tr></table></div><p>If the input table has composite key, the <code class="literal">--hbase-row-key</code> must be
in the form of a comma-separated list of composite key attributes.
In this case, the row key for HBase row will be generated by combining
values of composite key attributes using underscore as a separator.
NOTE: Sqoop import for a table with composite key will work only if
parameter <code class="literal">--hbase-row-key</code> has been specified.</p><p>If the target table and column family do not exist, the Sqoop job will
exit with an error. You should create the target table and column family
before running an import. If you specify <code class="literal">--hbase-create-table</code>, Sqoop
will create the target table and column family if they do not exist,
using the default parameters from your HBase configuration.</p><p>Sqoop currently serializes all values to HBase by converting each field
to its string representation (as if you were importing to HDFS in text
mode), and then inserts the UTF-8 bytes of this string in the target
cell. Sqoop will skip all rows containing null values in all columns
except the row key column.</p><p>To decrease the load on hbase, Sqoop can do bulk loading as opposed to
direct writes. To use bulk loading, enable it using <code class="literal">--hbase-bulkload</code>.</p><div class="table"><a name="idp3715352"></a><p class="title"><b>Table 10. Accumulo arguments:</b></p><div class="table-contents"><table summary="Accumulo arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--accumulo-table &lt;table-nam&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specifies an Accumulo table to use                                      as the target instead of HDFS
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--accumulo-column-family &lt;family&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the target column family for                                      the import
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--accumulo-create-table</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    If specified, create missing                                      Accumulo tables
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--accumulo-row-key &lt;col&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specifies which input column to use                                      as the row key
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--accumulo-visibility &lt;vis&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    (Optional) Specifies a visibility                                      token to apply to all rows inserted                                      into Accumulo.  Default is the                                      empty string.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--accumulo-batch-size &lt;size&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    (Optional) Sets the size in bytes                                      of Accumulo&#8217;s write buffer. Default                                      is 4MB.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--accumulo-max-latency &lt;ms&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    (Optional) Sets the max latency in                                      milliseconds for the Accumulo                                      batch writer.  Default is 0.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--accumulo-zookeepers &lt;host:port&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Comma-separated list of Zookeeper                                      servers used by the Accumulo instance
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--accumulo-instance &lt;table-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Name of the target Accumulo instance
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--accumulo-user &lt;username&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Name of the Accumulo user to import as
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--accumulo-password &lt;password&gt;</code>
    </td><td style="" align="left">
    Password for the Accumulo user
    </td></tr></tbody></table></div></div><br class="table-break"></div><div class="section" title="7.2.14. Importing Data Into Accumulo"><div class="titlepage"><div><div><h4 class="title"><a name="_importing_data_into_accumulo"></a>7.2.14. Importing Data Into Accumulo</h4></div></div></div><p>Sqoop supports importing records into a table in Accumulo</p><p>By specifying <code class="literal">--accumulo-table</code>, you instruct Sqoop to import
to a table in Accumulo rather than a directory in HDFS. Sqoop will
import data to the table specified as the argument to <code class="literal">--accumulo-table</code>.
Each row of the input table will be transformed into an Accumulo
<code class="literal">Mutation</code> operation to a row of the output table. The key for each row is
taken from a column of the input. By default Sqoop will use the split-by
column as the row key column. If that is not specified, it will try to
identify the primary key column, if any, of the source table. You can
manually specify the row key column with <code class="literal">--accumulo-row-key</code>. Each output
column will be placed in the same column family, which must be specified
with <code class="literal">--accumulo-column-family</code>.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>This function is incompatible with direct import (parameter
<code class="literal">--direct</code>), and cannot be used in the same operation as an HBase import.</p></td></tr></table></div><p>If the target table does not exist, the Sqoop job will
exit with an error, unless the <code class="literal">--accumulo-create-table</code> parameter is
specified. Otherwise, you should create the target table before running
an import.</p><p>Sqoop currently serializes all values to Accumulo by converting each field
to its string representation (as if you were importing to HDFS in text
mode), and then inserts the UTF-8 bytes of this string in the target
cell.</p><p>By default, no visibility is applied to the resulting cells in Accumulo,
so the data will be visible to any Accumulo user. Use the
<code class="literal">--accumulo-visibility</code> parameter to specify a visibility token to
apply to all rows in the import job.</p><p>For performance tuning, use the optional <code class="literal">--accumulo-buffer-size\</code> and
<code class="literal">--accumulo-max-latency</code> parameters. See Accumulo&#8217;s documentation for
an explanation of the effects of these parameters.</p><p>In order to connect to an Accumulo instance, you must specify the location
of a Zookeeper ensemble using the <code class="literal">--accumulo-zookeepers</code> parameter,
the name of the Accumulo instance (<code class="literal">--accumulo-instance</code>), and the
username and password to connect with (<code class="literal">--accumulo-user</code> and
<code class="literal">--accumulo-password</code> respectively).</p><div class="table"><a name="idp3740888"></a><p class="title"><b>Table 11. Code generation arguments:</b></p><div class="table-contents"><table summary="Code generation arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--bindir &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Output directory for compiled objects
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--class-name &lt;name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the generated class name. This overrides                         <code class="literal">--package-name</code>. When combined with                          <code class="literal">--jar-file</code>, sets the input class.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--jar-file &lt;file&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Disable code generation; use specified jar
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--outdir &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Output directory for generated code
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--package-name &lt;name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Put auto-generated classes in this package
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--map-column-java &lt;m&gt;</code>
    </td><td style="" align="left">
    Override default mapping from SQL type to                         Java type for configured columns.
    </td></tr></tbody></table></div></div><br class="table-break"><p>As mentioned earlier, a byproduct of importing a table to HDFS is a
class which can manipulate the imported data. If the data is stored in
SequenceFiles, this class will be used for the data&#8217;s serialization
container. Therefore, you should use this class in your subsequent
MapReduce processing of the data.</p><p>The class is typically named after the table; a table named <code class="literal">foo</code> will
generate a class named <code class="literal">foo</code>. You may want to override this class
name. For example, if your table is named <code class="literal">EMPLOYEES</code>, you may want to
specify <code class="literal">--class-name Employee</code> instead. Similarly, you can specify
just the package name with <code class="literal">--package-name</code>. The following import
generates a class named <code class="literal">com.foocorp.SomeTable</code>:</p><pre class="screen">$ sqoop import --connect &lt;connect-str&gt; --table SomeTable --package-name com.foocorp</pre><p>The <code class="literal">.java</code> source file for your class will be written to the current
working directory when you run <code class="literal">sqoop</code>. You can control the output
directory with <code class="literal">--outdir</code>. For example, <code class="literal">--outdir src/generated/</code>.</p><p>The import process compiles the source into <code class="literal">.class</code> and <code class="literal">.jar</code> files;
these are ordinarily stored under <code class="literal">/tmp</code>. You can select an alternate
target directory with <code class="literal">--bindir</code>. For example, <code class="literal">--bindir /scratch</code>.</p><p>If you already have a compiled class that can be used to perform the
import and want to suppress the code-generation aspect of the import
process, you can use an existing jar and class by
providing the <code class="literal">--jar-file</code> and <code class="literal">--class-name</code> options. For example:</p><pre class="screen">$ sqoop import --table SomeTable --jar-file mydatatypes.jar \
    --class-name SomeTableType</pre><p>This command will load the <code class="literal">SomeTableType</code> class out of <code class="literal">mydatatypes.jar</code>.</p></div><div class="section" title="7.2.15. Additional Import Configuration Properties"><div class="titlepage"><div><div><h4 class="title"><a name="_additional_import_configuration_properties"></a>7.2.15. Additional Import Configuration Properties</h4></div></div></div><p>There are some additional properties which can be configured by modifying
<code class="literal">conf/sqoop-site.xml</code>. Properties can be specified the same as in Hadoop
configuration files, for example:</p><pre class="screen">  &lt;property&gt;
    &lt;name&gt;property.name&lt;/name&gt;
    &lt;value&gt;property.value&lt;/value&gt;
  &lt;/property&gt;</pre><p>They can also be specified on the command line in the generic arguments, for
example:</p><pre class="screen">sqoop import -D property.name=property.value ...</pre><div class="table"><a name="idp3763704"></a><p class="title"><b>Table 12. Additional import configuration properties:</b></p><div class="table-contents"><table summary="Additional import configuration properties:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">sqoop.bigdecimal.format.string</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Controls how BigDecimal columns will                                          formatted when stored as a String. A                                          value of <code class="literal">true</code> (default) will use                                            toPlainString to store them without an                                        exponent component (0.0000001); while                                         a value of <code class="literal">false</code> will use toString                                          which may include an exponent (1E-7)
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">sqoop.hbase.add.row.key</code>
    </td><td style="" align="left">
    When set to <code class="literal">false</code> (default), Sqoop                                          will not add the column used as a row                                         key into the row data in HBase. When                                          set to <code class="literal">true</code>, the column used as a                                           row key will be added to the row data                                         in HBase.
    </td></tr></tbody></table></div></div><br class="table-break"></div></div><div class="section" title="7.3. Example Invocations"><div class="titlepage"><div><div><h3 class="title"><a name="_example_invocations"></a>7.3. Example Invocations</h3></div></div></div><p>The following examples illustrate how to use the import tool in a variety
of situations.</p><p>A basic import of a table named <code class="literal">EMPLOYEES</code> in the <code class="literal">corp</code> database:</p><pre class="screen">$ sqoop import --connect jdbc:mysql://db.foo.com/corp --table EMPLOYEES</pre><p>A basic import requiring a login:</p><pre class="screen">$ sqoop import --connect jdbc:mysql://db.foo.com/corp --table EMPLOYEES \
    --username SomeUser -P
Enter password: (hidden)</pre><p>Selecting specific columns from the <code class="literal">EMPLOYEES</code> table:</p><pre class="screen">$ sqoop import --connect jdbc:mysql://db.foo.com/corp --table EMPLOYEES \
    --columns "employee_id,first_name,last_name,job_title"</pre><p>Controlling the import parallelism (using 8 parallel tasks):</p><pre class="screen">$ sqoop import --connect jdbc:mysql://db.foo.com/corp --table EMPLOYEES \
    -m 8</pre><p>Storing data in SequenceFiles, and setting the generated class name to
<code class="literal">com.foocorp.Employee</code>:</p><pre class="screen">$ sqoop import --connect jdbc:mysql://db.foo.com/corp --table EMPLOYEES \
    --class-name com.foocorp.Employee --as-sequencefile</pre><p>Specifying the delimiters to use in a text-mode import:</p><pre class="screen">$ sqoop import --connect jdbc:mysql://db.foo.com/corp --table EMPLOYEES \
    --fields-terminated-by '\t' --lines-terminated-by '\n' \
    --optionally-enclosed-by '\"'</pre><p>Importing the data to Hive:</p><pre class="screen">$ sqoop import --connect jdbc:mysql://db.foo.com/corp --table EMPLOYEES \
    --hive-import</pre><p>Importing only new employees:</p><pre class="screen">$ sqoop import --connect jdbc:mysql://db.foo.com/corp --table EMPLOYEES \
    --where "start_date &gt; '2010-01-01'"</pre><p>Changing the splitting column from the default:</p><pre class="screen">$ sqoop import --connect jdbc:mysql://db.foo.com/corp --table EMPLOYEES \
    --split-by dept_id</pre><p>Verifying that an import was successful:</p><pre class="screen">$ hadoop fs -ls EMPLOYEES
Found 5 items
drwxr-xr-x   - someuser somegrp          0 2010-04-27 16:40 /user/someuser/EMPLOYEES/_logs
-rw-r--r--   1 someuser somegrp    2913511 2010-04-27 16:40 /user/someuser/EMPLOYEES/part-m-00000
-rw-r--r--   1 someuser somegrp    1683938 2010-04-27 16:40 /user/someuser/EMPLOYEES/part-m-00001
-rw-r--r--   1 someuser somegrp    7245839 2010-04-27 16:40 /user/someuser/EMPLOYEES/part-m-00002
-rw-r--r--   1 someuser somegrp    7842523 2010-04-27 16:40 /user/someuser/EMPLOYEES/part-m-00003

$ hadoop fs -cat EMPLOYEES/part-m-00000 | head -n 10
0,joe,smith,engineering
1,jane,doe,marketing
...</pre><p>Performing an incremental import of new data, after having already
imported the first 100,000 rows of a table:</p><pre class="screen">$ sqoop import --connect jdbc:mysql://db.foo.com/somedb --table sometable \
    --where "id &gt; 100000" --target-dir /incremental_dataset --append</pre><p>An import of a table named <code class="literal">EMPLOYEES</code> in the <code class="literal">corp</code> database that uses
validation to validate the import using the table row count and number of
rows copied into HDFS:
<a class="link" href="#validation" title="11. validation">More Details</a></p><pre class="screen">$ sqoop import --connect jdbc:mysql://db.foo.com/corp \
    --table EMPLOYEES --validate</pre></div></div><div class="section" title="8. sqoop-import-all-tables"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_literal_sqoop_import_all_tables_literal"></a>8. <code class="literal">sqoop-import-all-tables</code></h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_purpose_2">8.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_2">8.2. Syntax</a></span></dt><dt><span class="section"><a href="#_example_invocations_2">8.3. Example Invocations</a></span></dt></dl></div><div class="section" title="8.1. Purpose"><div class="titlepage"><div><div><h3 class="title"><a name="_purpose_2"></a>8.1. Purpose</h3></div></div></div><p>The <code class="literal">import-all-tables</code> tool imports a set of tables from an RDBMS to HDFS.
Data from each table is stored in a separate directory in HDFS.</p><p>For the <code class="literal">import-all-tables</code> tool to be useful, the following conditions
must be met:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Each table must have a single-column primary key or <code class="literal">--autoreset-to-one-mapper</code> option must be used.
</li><li class="listitem">
You must intend to import all columns of each table.
</li><li class="listitem">
You must not intend to use non-default splitting column, nor impose
  any conditions via a <code class="literal">WHERE</code> clause.
</li></ul></div></div><div class="section" title="8.2. Syntax"><div class="titlepage"><div><div><h3 class="title"><a name="_syntax_2"></a>8.2. Syntax</h3></div></div></div><pre class="screen">$ sqoop import-all-tables (generic-args) (import-args)
$ sqoop-import-all-tables (generic-args) (import-args)</pre><p>Although the Hadoop generic arguments must preceed any import arguments,
the import arguments can be entered in any order with respect to one
another.</p><div class="table"><a name="idp3790824"></a><p class="title"><b>Table 13. Common arguments</b></p><div class="table-contents"><table summary="Common arguments" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connect &lt;jdbc-uri&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify JDBC connect string
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connection-manager &lt;class-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify connection manager class to                                          use
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--driver &lt;class-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Manually specify JDBC driver class                                          to use
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hadoop-mapred-home &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Override $HADOOP_MAPRED_HOME
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--help</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Print usage instructions
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--password-file</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set path for a file containing the                                          authentication password
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">-P</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Read password from console
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--password &lt;password&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set authentication password
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--username &lt;username&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set authentication username
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--verbose</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Print more information while working
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connection-param-file &lt;filename&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Optional properties file that                                          provides connection parameters
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--relaxed-isolation</code>
    </td><td style="" align="left">
    Set connection transaction isolation                                          to read uncommitted for the mappers.
    </td></tr></tbody></table></div></div><br class="table-break"><div class="table"><a name="idp3806256"></a><p class="title"><b>Table 14. Import control arguments:</b></p><div class="table-contents"><table summary="Import control arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--as-avrodatafile</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Imports data to Avro Data Files
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--as-sequencefile</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Imports data to SequenceFiles
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--as-textfile</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Imports data as plain text (default)
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--as-parquetfile</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Imports data to Parquet Files
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--direct</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Use direct import fast path
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--inline-lob-limit &lt;n&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set the maximum size for an inline LOB
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">-m,--num-mappers &lt;n&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Use <span class="emphasis"><em>n</em></span> map tasks to import in parallel
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--warehouse-dir &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    HDFS parent for table destination
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">-z,--compress</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Enable compression
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--compression-codec &lt;c&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Use Hadoop codec (default gzip)
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--exclude-tables &lt;tables&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Comma separated list of tables to exclude                             from import process
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--autoreset-to-one-mapper</code>
    </td><td style="" align="left">
    Import should use one mapper if a table                             with no primary key is encountered
    </td></tr></tbody></table></div></div><br class="table-break"><p>These arguments behave in the same manner as they do when used for the
<code class="literal">sqoop-import</code> tool, but the <code class="literal">--table</code>, <code class="literal">--split-by</code>, <code class="literal">--columns</code>,
and <code class="literal">--where</code> arguments are invalid for <code class="literal">sqoop-import-all-tables</code>.
The <code class="literal">--exclude-tables argument is for +sqoop-import-all-tables</code> only.</p><div class="table"><a name="idp3824216"></a><p class="title"><b>Table 15. Output line formatting arguments:</b></p><div class="table-contents"><table summary="Output line formatting arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--enclosed-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets a required field enclosing                                    character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--escaped-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the escape character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--fields-terminated-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the field separator character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--lines-terminated-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the end-of-line character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--mysql-delimiters</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Uses MySQL&#8217;s default delimiter set:                                   fields: <code class="literal">,</code>  lines: <code class="literal">\n</code>                                    escaped-by: <code class="literal">\</code>                                    optionally-enclosed-by: <code class="literal">'</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--optionally-enclosed-by &lt;char&gt;</code>
    </td><td style="" align="left">
    Sets a field enclosing character
    </td></tr></tbody></table></div></div><br class="table-break"><div class="table"><a name="idp3834944"></a><p class="title"><b>Table 16. Input parsing arguments:</b></p><div class="table-contents"><table summary="Input parsing arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--input-enclosed-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets a required field encloser
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--input-escaped-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the input escape                                          character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--input-fields-terminated-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the input field separator
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--input-lines-terminated-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the input end-of-line                                          character
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--input-optionally-enclosed-by &lt;char&gt;</code>
    </td><td style="" align="left">
    Sets a field enclosing                                          character
    </td></tr></tbody></table></div></div><br class="table-break"><div class="table"><a name="idp3843368"></a><p class="title"><b>Table 17. Hive arguments:</b></p><div class="table-contents"><table summary="Hive arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-home &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Override <code class="literal">$HIVE_HOME</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-import</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Import tables into Hive (Uses Hive&#8217;s                               default delimiters if none are set.)
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-overwrite</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Overwrite existing data in the Hive table.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--create-hive-table</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    If set, then the job will fail if the target hive
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    table exits. By default this property is false.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-table &lt;table-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the table name to use when importing                              to Hive.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-drop-import-delims</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Drops <span class="emphasis"><em>\n</em></span>, <span class="emphasis"><em>\r</em></span>, and <span class="emphasis"><em>\01</em></span> from string                              fields when importing to Hive.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-delims-replacement</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Replace <span class="emphasis"><em>\n</em></span>, <span class="emphasis"><em>\r</em></span>, and <span class="emphasis"><em>\01</em></span> from string                              fields with user defined string when importing to Hive.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-partition-key</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Name of a hive field to partition are                               sharded on
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-partition-value &lt;v&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    String-value that serves as partition key                              for this imported into hive in this job.
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--map-column-hive &lt;map&gt;</code>
    </td><td style="" align="left">
    Override default mapping from SQL type to                              Hive type for configured columns.
    </td></tr></tbody></table></div></div><br class="table-break"><div class="table"><a name="idp3859304"></a><p class="title"><b>Table 18. Code generation arguments:</b></p><div class="table-contents"><table summary="Code generation arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--bindir &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Output directory for compiled objects
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--jar-file &lt;file&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Disable code generation; use specified jar
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--outdir &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Output directory for generated code
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--package-name &lt;name&gt;</code>
    </td><td style="" align="left">
    Put auto-generated classes in this package
    </td></tr></tbody></table></div></div><br class="table-break"><p>The <code class="literal">import-all-tables</code> tool does not support the <code class="literal">--class-name</code> argument.
You may, however, specify a package with <code class="literal">--package-name</code> in which all
generated classes will be placed.</p></div><div class="section" title="8.3. Example Invocations"><div class="titlepage"><div><div><h3 class="title"><a name="_example_invocations_2"></a>8.3. Example Invocations</h3></div></div></div><p>Import all tables from the <code class="literal">corp</code> database:</p><pre class="screen">$ sqoop import-all-tables --connect jdbc:mysql://db.foo.com/corp</pre><p>Verifying that it worked:</p><pre class="screen">$ hadoop fs -ls
Found 4 items
drwxr-xr-x   - someuser somegrp       0 2010-04-27 17:15 /user/someuser/EMPLOYEES
drwxr-xr-x   - someuser somegrp       0 2010-04-27 17:15 /user/someuser/PAYCHECKS
drwxr-xr-x   - someuser somegrp       0 2010-04-27 17:15 /user/someuser/DEPARTMENTS
drwxr-xr-x   - someuser somegrp       0 2010-04-27 17:15 /user/someuser/OFFICE_SUPPLIES</pre></div></div><div class="section" title="9. sqoop-import-mainframe"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_literal_sqoop_import_mainframe_literal"></a>9. <code class="literal">sqoop-import-mainframe</code></h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_purpose_3">9.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_3">9.2. Syntax</a></span></dt><dd><dl><dt><span class="section"><a href="#_connecting_to_a_mainframe">9.2.1. Connecting to a Mainframe</a></span></dt><dt><span class="section"><a href="#_selecting_the_files_to_import">9.2.2. Selecting the Files to Import</a></span></dt><dt><span class="section"><a href="#_controlling_parallelism_2">9.2.3. Controlling Parallelism</a></span></dt><dt><span class="section"><a href="#_controlling_distributed_cache_2">9.2.4. Controlling Distributed Cache</a></span></dt><dt><span class="section"><a href="#_controlling_the_import_process_2">9.2.5. Controlling the Import Process</a></span></dt><dt><span class="section"><a href="#_file_formats_2">9.2.6. File Formats</a></span></dt><dt><span class="section"><a href="#_importing_data_into_hive_2">9.2.7. Importing Data Into Hive</a></span></dt><dt><span class="section"><a href="#_importing_data_into_hbase_2">9.2.8. Importing Data Into HBase</a></span></dt><dt><span class="section"><a href="#_importing_data_into_accumulo_2">9.2.9. Importing Data Into Accumulo</a></span></dt><dt><span class="section"><a href="#_additional_import_configuration_properties_2">9.2.10. Additional Import Configuration Properties</a></span></dt></dl></dd><dt><span class="section"><a href="#_example_invocations_3">9.3. Example Invocations</a></span></dt></dl></div><div class="section" title="9.1. Purpose"><div class="titlepage"><div><div><h3 class="title"><a name="_purpose_3"></a>9.1. Purpose</h3></div></div></div><p>The <code class="literal">import-mainframe</code> tool imports all sequential datasets
in a partitioned dataset(PDS) on a mainframe to HDFS.  A PDS is
akin to a directory on the open systems.
The records in a dataset can contain only character data.
Records will be stored with the entire record as a single text field.</p></div><div class="section" title="9.2. Syntax"><div class="titlepage"><div><div><h3 class="title"><a name="_syntax_3"></a>9.2. Syntax</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_connecting_to_a_mainframe">9.2.1. Connecting to a Mainframe</a></span></dt><dt><span class="section"><a href="#_selecting_the_files_to_import">9.2.2. Selecting the Files to Import</a></span></dt><dt><span class="section"><a href="#_controlling_parallelism_2">9.2.3. Controlling Parallelism</a></span></dt><dt><span class="section"><a href="#_controlling_distributed_cache_2">9.2.4. Controlling Distributed Cache</a></span></dt><dt><span class="section"><a href="#_controlling_the_import_process_2">9.2.5. Controlling the Import Process</a></span></dt><dt><span class="section"><a href="#_file_formats_2">9.2.6. File Formats</a></span></dt><dt><span class="section"><a href="#_importing_data_into_hive_2">9.2.7. Importing Data Into Hive</a></span></dt><dt><span class="section"><a href="#_importing_data_into_hbase_2">9.2.8. Importing Data Into HBase</a></span></dt><dt><span class="section"><a href="#_importing_data_into_accumulo_2">9.2.9. Importing Data Into Accumulo</a></span></dt><dt><span class="section"><a href="#_additional_import_configuration_properties_2">9.2.10. Additional Import Configuration Properties</a></span></dt></dl></div><pre class="screen">$ sqoop import-mainframe (generic-args) (import-args)
$ sqoop-import-mainframe (generic-args) (import-args)</pre><p>While the Hadoop generic arguments must precede any import arguments,
you can type the import arguments in any order with respect to one
another.</p><div class="table"><a name="idp3874184"></a><p class="title"><b>Table 19. Common arguments</b></p><div class="table-contents"><table summary="Common arguments" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connect &lt;hostname&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify mainframe host to connect
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connection-manager &lt;class-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify connection manager class to                                          use
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hadoop-mapred-home &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Override $HADOOP_MAPRED_HOME
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--help</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Print usage instructions
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--password-file</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set path for a file containing the                                          authentication password
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">-P</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Read password from console
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--password &lt;password&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set authentication password
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--username &lt;username&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set authentication username
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--verbose</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Print more information while working
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--connection-param-file &lt;filename&gt;</code>
    </td><td style="" align="left">
    Optional properties file that                                          provides connection parameters
    </td></tr></tbody></table></div></div><br class="table-break"><div class="section" title="9.2.1. Connecting to a Mainframe"><div class="titlepage"><div><div><h4 class="title"><a name="_connecting_to_a_mainframe"></a>9.2.1. Connecting to a Mainframe</h4></div></div></div><p>Sqoop is designed to import mainframe datasets into HDFS. To do
so, you must specify a mainframe host name in the Sqoop <code class="literal">--connect</code> argument.</p><pre class="screen">$ sqoop import-mainframe --connect z390</pre><p>This will connect to the mainframe host z390 via ftp.</p><p>You might need to authenticate against the mainframe host to
access it. You can use the <code class="literal">--username</code> to supply a username to the mainframe.
Sqoop provides couple of different ways to supply a password,
secure and non-secure, to the mainframe which is detailed below.</p><p title="Secure way of supplying password to the mainframe"><b>Secure way of supplying password to the mainframe. </b>You should save the password in a file on the users home directory with 400
permissions and specify the path to that file using the <span class="strong"><strong><code class="literal">--password-file</code></strong></span>
argument, and is the preferred method of entering credentials. Sqoop will
then read the password from the file and pass it to the MapReduce cluster
using secure means with out exposing the password in the job configuration.
The file containing the password can either be on the Local FS or HDFS.</p><p>Example:</p><pre class="screen">$ sqoop import-mainframe --connect z390 \
    --username david --password-file ${user.home}/.password</pre><p>Another way of supplying passwords is using the <code class="literal">-P</code> argument which will
read a password from a console prompt.</p><div class="warning" title="Warning" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Warning"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Warning]" src="images/warning.png"></td><th align="left">Warning</th></tr><tr><td align="left" valign="top"><p>The <code class="literal">--password</code> parameter is insecure, as other users may
be able to read your password from the command-line arguments via
the output of programs such as <code class="literal">ps</code>. The <span class="strong"><strong><code class="literal">-P</code></strong></span> argument is the preferred
method over using the <code class="literal">--password</code> argument. Credentials may still be
transferred between nodes of the MapReduce cluster using insecure means.</p></td></tr></table></div><p>Example:</p><pre class="screen">$ sqoop import-mainframe --connect z390 --username david --password 12345</pre><div class="table"><a name="idp3895848"></a><p class="title"><b>Table 20. Import control arguments:</b></p><div class="table-contents"><table summary="Import control arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--as-avrodatafile</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Imports data to Avro Data Files
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--as-sequencefile</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Imports data to SequenceFiles
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--as-textfile</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Imports data as plain text (default)
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--as-parquetfile</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Imports data to Parquet Files
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--delete-target-dir</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Delete the import target directory                                  if it exists
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">-m,--num-mappers &lt;n&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Use <span class="emphasis"><em>n</em></span> map tasks to import in parallel
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--target-dir &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    HDFS destination dir
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--warehouse-dir &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    HDFS parent for table destination
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">-z,--compress</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Enable compression
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--compression-codec &lt;c&gt;</code>
    </td><td style="" align="left">
    Use Hadoop codec (default gzip)
    </td></tr></tbody></table></div></div><br class="table-break"></div><div class="section" title="9.2.2. Selecting the Files to Import"><div class="titlepage"><div><div><h4 class="title"><a name="_selecting_the_files_to_import"></a>9.2.2. Selecting the Files to Import</h4></div></div></div><p>You can use the <code class="literal">--dataset</code> argument to specify a partitioned dataset name.
All sequential datasets in the partitioned dataset will be imported.</p></div><div class="section" title="9.2.3. Controlling Parallelism"><div class="titlepage"><div><div><h4 class="title"><a name="_controlling_parallelism_2"></a>9.2.3. Controlling Parallelism</h4></div></div></div><p>Sqoop imports data in parallel by making multiple ftp connections to the
mainframe to transfer multiple files simultaneously.  You can specify the
number of map tasks (parallel processes) to use to perform the import by
using the <code class="literal">-m</code> or <code class="literal">--num-mappers</code> argument. Each of these arguments
takes an integer value which corresponds to the degree of parallelism
to employ. By default, four tasks are used. You can adjust this value to
maximize the data transfer rate from the mainframe.</p></div><div class="section" title="9.2.4. Controlling Distributed Cache"><div class="titlepage"><div><div><h4 class="title"><a name="_controlling_distributed_cache_2"></a>9.2.4. Controlling Distributed Cache</h4></div></div></div><p>Sqoop will copy the jars in $SQOOP_HOME/lib folder to job cache every
time when start a Sqoop job. When launched by Oozie this is unnecessary
since Oozie use its own Sqoop share lib which keeps Sqoop dependencies
in the distributed cache. Oozie will do the localization on each
worker node for the Sqoop dependencies only once during the first Sqoop
job and reuse the jars on worker node for subsquencial jobs. Using
option <code class="literal">--skip-dist-cache</code> in Sqoop command when launched by Oozie will
skip the step which Sqoop copies its dependencies to job cache and save
massive I/O.</p></div><div class="section" title="9.2.5. Controlling the Import Process"><div class="titlepage"><div><div><h4 class="title"><a name="_controlling_the_import_process_2"></a>9.2.5. Controlling the Import Process</h4></div></div></div><p>By default, Sqoop will import all sequential files in a partitioned dataset
<code class="literal">pds</code> to a directory named <code class="literal">pds</code> inside your home directory in HDFS. For
example, if your username is <code class="literal">someuser</code>, then the import tool will write to
<code class="literal">/user/someuser/pds/(files)</code>. You can adjust the parent directory of
the import with the <code class="literal">--warehouse-dir</code> argument. For example:</p><pre class="screen">$ sqoop import-mainframe --connnect &lt;host&gt; --dataset foo --warehouse-dir /shared \
    ...</pre><p>This command would write to a set of files in the <code class="literal">/shared/pds/</code> directory.</p><p>You can also explicitly choose the target directory, like so:</p><pre class="screen">$ sqoop import-mainframe --connnect &lt;host&gt; --dataset foo --target-dir /dest \
    ...</pre><p>This will import the files into the <code class="literal">/dest</code> directory. <code class="literal">--target-dir</code> is
incompatible with <code class="literal">--warehouse-dir</code>.</p><p>By default, imports go to a new target location. If the destination directory
already exists in HDFS, Sqoop will refuse to import and overwrite that
directory&#8217;s contents.</p></div><div class="section" title="9.2.6. File Formats"><div class="titlepage"><div><div><h4 class="title"><a name="_file_formats_2"></a>9.2.6. File Formats</h4></div></div></div><p>By default, each record in a dataset is stored
as a text record with a newline at the end.  Each record is assumed to contain
a single text field with the name DEFAULT_COLUMN.
When Sqoop imports data to HDFS, it generates a Java class which can
reinterpret the text files that it creates.</p><p>You can also import mainframe records to Sequence, Avro, or Parquet files.</p><p>By default, data is not compressed. You can compress your data by
using the deflate (gzip) algorithm with the <code class="literal">-z</code> or <code class="literal">--compress</code>
argument, or specify any Hadoop compression codec using the
<code class="literal">--compression-codec</code> argument.</p><div class="table"><a name="idp3924232"></a><p class="title"><b>Table 21. Output line formatting arguments:</b></p><div class="table-contents"><table summary="Output line formatting arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--enclosed-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets a required field enclosing                                    character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--escaped-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the escape character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--fields-terminated-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the field separator character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--lines-terminated-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the end-of-line character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--mysql-delimiters</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Uses MySQL&#8217;s default delimiter set:                                   fields: <code class="literal">,</code>  lines: <code class="literal">\n</code>                                    escaped-by: <code class="literal">\</code>                                    optionally-enclosed-by: <code class="literal">'</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--optionally-enclosed-by &lt;char&gt;</code>
    </td><td style="" align="left">
    Sets a field enclosing character
    </td></tr></tbody></table></div></div><br class="table-break"><p>Since mainframe record contains only one field, importing to delimited files
will not contain any field delimiter.  However, the field may be enclosed with
enclosing character or escaped by an escaping character.</p><div class="table"><a name="idp3935472"></a><p class="title"><b>Table 22. Input parsing arguments:</b></p><div class="table-contents"><table summary="Input parsing arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--input-enclosed-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets a required field encloser
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--input-escaped-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the input escape                                          character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--input-fields-terminated-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the input field separator
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--input-lines-terminated-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the input end-of-line                                          character
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--input-optionally-enclosed-by &lt;char&gt;</code>
    </td><td style="" align="left">
    Sets a field enclosing                                          character
    </td></tr></tbody></table></div></div><br class="table-break"><p>When Sqoop imports data to HDFS, it generates a Java class which can
reinterpret the text files that it creates when doing a
delimited-format import. The delimiters are chosen with arguments such
as <code class="literal">--fields-terminated-by</code>; this controls both how the data is
written to disk, and how the generated <code class="literal">parse()</code> method reinterprets
this data. The delimiters used by the <code class="literal">parse()</code> method can be chosen
independently of the output arguments, by using
<code class="literal">--input-fields-terminated-by</code>, and so on. This is useful, for example, to
generate classes which can parse records created with one set of
delimiters, and emit the records to a different set of files using a
separate set of delimiters.</p><div class="table"><a name="idp3946288"></a><p class="title"><b>Table 23. Hive arguments:</b></p><div class="table-contents"><table summary="Hive arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-home &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Override <code class="literal">$HIVE_HOME</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-import</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Import tables into Hive (Uses Hive&#8217;s                               default delimiters if none are set.)
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-overwrite</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Overwrite existing data in the Hive table.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--create-hive-table</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    If set, then the job will fail if the target hive
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    table exits. By default this property is false.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-table &lt;table-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the table name to use when importing                              to Hive.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-drop-import-delims</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Drops <span class="emphasis"><em>\n</em></span>, <span class="emphasis"><em>\r</em></span>, and <span class="emphasis"><em>\01</em></span> from string                              fields when importing to Hive.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-delims-replacement</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Replace <span class="emphasis"><em>\n</em></span>, <span class="emphasis"><em>\r</em></span>, and <span class="emphasis"><em>\01</em></span> from string                              fields with user defined string when importing to Hive.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-partition-key</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Name of a hive field to partition are                               sharded on
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-partition-value &lt;v&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    String-value that serves as partition key                              for this imported into hive in this job.
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--map-column-hive &lt;map&gt;</code>
    </td><td style="" align="left">
    Override default mapping from SQL type to                              Hive type for configured columns.
    </td></tr></tbody></table></div></div><br class="table-break"></div><div class="section" title="9.2.7. Importing Data Into Hive"><div class="titlepage"><div><div><h4 class="title"><a name="_importing_data_into_hive_2"></a>9.2.7. Importing Data Into Hive</h4></div></div></div><p>Sqoop&#8217;s import tool&#8217;s main function is to upload your data into files
in HDFS. If you have a Hive metastore associated with your HDFS
cluster, Sqoop can also import the data into Hive by generating and
executing a <code class="literal">CREATE TABLE</code> statement to define the data&#8217;s layout in
Hive. Importing data into Hive is as simple as adding the
<span class="strong"><strong><code class="literal">--hive-import</code></strong></span> option to your Sqoop command line.</p><p>If the Hive table already exists, you can specify the
<span class="strong"><strong><code class="literal">--hive-overwrite</code></strong></span> option to indicate that existing table in hive must
be replaced. After your data is imported into HDFS or this step is
omitted, Sqoop will generate a Hive script containing a <code class="literal">CREATE TABLE</code>
operation defining your columns using Hive&#8217;s types, and a <code class="literal">LOAD DATA INPATH</code>
statement to move the data files into Hive&#8217;s warehouse directory.</p><p>The script will be executed by calling
the installed copy of hive on the machine where Sqoop is run. If you have
multiple Hive installations, or <code class="literal">hive</code> is not in your <code class="literal">$PATH</code>, use the
<span class="strong"><strong><code class="literal">--hive-home</code></strong></span> option to identify the Hive installation directory.
Sqoop will use <code class="literal">$HIVE_HOME/bin/hive</code> from here.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>This function is incompatible with <code class="literal">--as-avrodatafile</code> and
<code class="literal">--as-sequencefile</code>.</p></td></tr></table></div><p>Even though Hive supports escaping characters, it does not
handle escaping of new-line character. Also, it does not support
the notion of enclosing characters that may include field delimiters
in the enclosed string.  It is therefore recommended that you choose
unambiguous field and record-terminating delimiters without the help
of escaping and enclosing characters when working with Hive; this is
due to limitations of Hive&#8217;s input parsing abilities. If you do use
<code class="literal">--escaped-by</code>, <code class="literal">--enclosed-by</code>, or <code class="literal">--optionally-enclosed-by</code> when
importing data into Hive, Sqoop will print a warning message.</p><p>Hive will have problems using Sqoop-imported data if your database&#8217;s
rows contain string fields that have Hive&#8217;s default row delimiters
(<code class="literal">\n</code> and <code class="literal">\r</code> characters) or column delimiters (<code class="literal">\01</code> characters)
present in them.  You can use the <code class="literal">--hive-drop-import-delims</code> option
to drop those characters on import to give Hive-compatible text data.
Alternatively, you can use the <code class="literal">--hive-delims-replacement</code> option
to replace those characters with a user-defined string on import to give
Hive-compatible text data.  These options should only be used if you use
Hive&#8217;s default delimiters and should not be used if different delimiters
are specified.</p><p>Sqoop will pass the field and record delimiters through to Hive. If you do
not set any delimiters and do use <code class="literal">--hive-import</code>, the field delimiter will
be set to <code class="literal">^A</code> and the record delimiter will be set to <code class="literal">\n</code> to be consistent
with Hive&#8217;s defaults.</p><p>Sqoop will by default import NULL values as string <code class="literal">null</code>. Hive is however
using string <code class="literal">\N</code> to denote <code class="literal">NULL</code> values and therefore predicates dealing
with <code class="literal">NULL</code> (like <code class="literal">IS NULL</code>) will not work correctly. You should append
parameters <code class="literal">--null-string</code> and <code class="literal">--null-non-string</code> in case of import job or
<code class="literal">--input-null-string</code> and <code class="literal">--input-null-non-string</code> in case of an export job if
you wish to properly preserve <code class="literal">NULL</code> values. Because sqoop is using those
parameters in generated code, you need to properly escape value <code class="literal">\N</code> to <code class="literal">\\N</code>:</p><pre class="screen">$ sqoop import  ... --null-string '\\N' --null-non-string '\\N'</pre><p>The table name used in Hive is, by default, the same as that of the
source table. You can control the output table name with the <code class="literal">--hive-table</code>
option.</p><p>Hive can put data into partitions for more efficient query
performance.  You can tell a Sqoop job to import data for Hive into a
particular partition by specifying the <code class="literal">--hive-partition-key</code> and
<code class="literal">--hive-partition-value</code> arguments.  The partition value must be a
string.  Please see the Hive documentation for more details on
partitioning.</p><p>You can import compressed tables into Hive using the <code class="literal">--compress</code> and
<code class="literal">--compression-codec</code> options. One downside to compressing tables imported
into Hive is that many codecs cannot be split for processing by parallel map
tasks. The lzop codec, however, does support splitting. When importing tables
with this codec, Sqoop will automatically index the files for splitting and
configuring a new Hive table with the correct InputFormat. This feature
currently requires that all partitions of a table be compressed with the lzop
codec.</p><div class="table"><a name="idp3984032"></a><p class="title"><b>Table 24. HBase arguments:</b></p><div class="table-contents"><table summary="HBase arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--column-family &lt;family&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the target column family for the import
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hbase-create-table</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    If specified, create missing HBase tables
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hbase-row-key &lt;col&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specifies which input column to use as the                              row key
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    In case, if input table contains composite
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    key, then &lt;col&gt; must be in the form of a
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    comma-separated list of composite key
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    attributes
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hbase-table &lt;table-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specifies an HBase table to use as the                               target instead of HDFS
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--hbase-bulkload</code>
    </td><td style="" align="left">
    Enables bulk loading
    </td></tr></tbody></table></div></div><br class="table-break"></div><div class="section" title="9.2.8. Importing Data Into HBase"><div class="titlepage"><div><div><h4 class="title"><a name="_importing_data_into_hbase_2"></a>9.2.8. Importing Data Into HBase</h4></div></div></div><p>Sqoop supports additional import targets beyond HDFS and Hive. Sqoop
can also import records into a table in HBase.</p><p>By specifying <code class="literal">--hbase-table</code>, you instruct Sqoop to import
to a table in HBase rather than a directory in HDFS. Sqoop will
import data to the table specified as the argument to <code class="literal">--hbase-table</code>.
Each row of the input table will be transformed into an HBase
<code class="literal">Put</code> operation to a row of the output table. The key for each row is
taken from a column of the input. By default Sqoop will use the split-by
column as the row key column. If that is not specified, it will try to
identify the primary key column, if any, of the source table. You can
manually specify the row key column with <code class="literal">--hbase-row-key</code>. Each output
column will be placed in the same column family, which must be specified
with <code class="literal">--column-family</code>.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>This function is incompatible with direct import (parameter
<code class="literal">--direct</code>).</p></td></tr></table></div><p>If the input table has composite key, the <code class="literal">--hbase-row-key</code> must be
in the form of a comma-separated list of composite key attributes.
In this case, the row key for HBase row will be generated by combining
values of composite key attributes using underscore as a separator.
NOTE: Sqoop import for a table with composite key will work only if
parameter <code class="literal">--hbase-row-key</code> has been specified.</p><p>If the target table and column family do not exist, the Sqoop job will
exit with an error. You should create the target table and column family
before running an import. If you specify <code class="literal">--hbase-create-table</code>, Sqoop
will create the target table and column family if they do not exist,
using the default parameters from your HBase configuration.</p><p>Sqoop currently serializes all values to HBase by converting each field
to its string representation (as if you were importing to HDFS in text
mode), and then inserts the UTF-8 bytes of this string in the target
cell. Sqoop will skip all rows containing null values in all columns
except the row key column.</p><p>To decrease the load on hbase, Sqoop can do bulk loading as opposed to
direct writes. To use bulk loading, enable it using <code class="literal">--hbase-bulkload</code>.</p><div class="table"><a name="idp4003528"></a><p class="title"><b>Table 25. Accumulo arguments:</b></p><div class="table-contents"><table summary="Accumulo arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--accumulo-table &lt;table-nam&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specifies an Accumulo table to use                                      as the target instead of HDFS
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--accumulo-column-family &lt;family&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the target column family for                                      the import
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--accumulo-create-table</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    If specified, create missing                                      Accumulo tables
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--accumulo-row-key &lt;col&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specifies which input column to use                                      as the row key
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--accumulo-visibility &lt;vis&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    (Optional) Specifies a visibility                                      token to apply to all rows inserted                                      into Accumulo.  Default is the                                      empty string.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--accumulo-batch-size &lt;size&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    (Optional) Sets the size in bytes                                      of Accumulo&#8217;s write buffer. Default                                      is 4MB.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--accumulo-max-latency &lt;ms&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    (Optional) Sets the max latency in                                      milliseconds for the Accumulo                                      batch writer.  Default is 0.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--accumulo-zookeepers &lt;host:port&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Comma-separated list of Zookeeper                                      servers used by the Accumulo instance
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--accumulo-instance &lt;table-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Name of the target Accumulo instance
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--accumulo-user &lt;username&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Name of the Accumulo user to import as
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--accumulo-password &lt;password&gt;</code>
    </td><td style="" align="left">
    Password for the Accumulo user
    </td></tr></tbody></table></div></div><br class="table-break"></div><div class="section" title="9.2.9. Importing Data Into Accumulo"><div class="titlepage"><div><div><h4 class="title"><a name="_importing_data_into_accumulo_2"></a>9.2.9. Importing Data Into Accumulo</h4></div></div></div><p>Sqoop supports importing records into a table in Accumulo</p><p>By specifying <code class="literal">--accumulo-table</code>, you instruct Sqoop to import
to a table in Accumulo rather than a directory in HDFS. Sqoop will
import data to the table specified as the argument to <code class="literal">--accumulo-table</code>.
Each row of the input table will be transformed into an Accumulo
<code class="literal">Mutation</code> operation to a row of the output table. The key for each row is
taken from a column of the input. By default Sqoop will use the split-by
column as the row key column. If that is not specified, it will try to
identify the primary key column, if any, of the source table. You can
manually specify the row key column with <code class="literal">--accumulo-row-key</code>. Each output
column will be placed in the same column family, which must be specified
with <code class="literal">--accumulo-column-family</code>.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>This function is incompatible with direct import (parameter
<code class="literal">--direct</code>), and cannot be used in the same operation as an HBase import.</p></td></tr></table></div><p>If the target table does not exist, the Sqoop job will
exit with an error, unless the <code class="literal">--accumulo-create-table</code> parameter is
specified. Otherwise, you should create the target table before running
an import.</p><p>Sqoop currently serializes all values to Accumulo by converting each field
to its string representation (as if you were importing to HDFS in text
mode), and then inserts the UTF-8 bytes of this string in the target
cell.</p><p>By default, no visibility is applied to the resulting cells in Accumulo,
so the data will be visible to any Accumulo user. Use the
<code class="literal">--accumulo-visibility</code> parameter to specify a visibility token to
apply to all rows in the import job.</p><p>For performance tuning, use the optional <code class="literal">--accumulo-buffer-size\</code> and
<code class="literal">--accumulo-max-latency</code> parameters. See Accumulo&#8217;s documentation for
an explanation of the effects of these parameters.</p><p>In order to connect to an Accumulo instance, you must specify the location
of a Zookeeper ensemble using the <code class="literal">--accumulo-zookeepers</code> parameter,
the name of the Accumulo instance (<code class="literal">--accumulo-instance</code>), and the
username and password to connect with (<code class="literal">--accumulo-user</code> and
<code class="literal">--accumulo-password</code> respectively).</p><div class="table"><a name="idp4029024"></a><p class="title"><b>Table 26. Code generation arguments:</b></p><div class="table-contents"><table summary="Code generation arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--bindir &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Output directory for compiled objects
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--class-name &lt;name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the generated class name. This overrides                         <code class="literal">--package-name</code>. When combined with                          <code class="literal">--jar-file</code>, sets the input class.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--jar-file &lt;file&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Disable code generation; use specified jar
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--outdir &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Output directory for generated code
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--package-name &lt;name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Put auto-generated classes in this package
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--map-column-java &lt;m&gt;</code>
    </td><td style="" align="left">
    Override default mapping from SQL type to                         Java type for configured columns.
    </td></tr></tbody></table></div></div><br class="table-break"><p>As mentioned earlier, a byproduct of importing a table to HDFS is a
class which can manipulate the imported data.
You should use this class in your subsequent
MapReduce processing of the data.</p><p>The class is typically named after the partitioned dataset name; a
partitioned dataset named <code class="literal">foo</code> will
generate a class named <code class="literal">foo</code>. You may want to override this class
name. For example, if your partitioned dataset
is named <code class="literal">EMPLOYEES</code>, you may want to
specify <code class="literal">--class-name Employee</code> instead. Similarly, you can specify
just the package name with <code class="literal">--package-name</code>. The following import
generates a class named <code class="literal">com.foocorp.SomePDS</code>:</p><pre class="screen">$ sqoop import-mainframe --connect &lt;host&gt; --dataset SomePDS --package-name com.foocorp</pre><p>The <code class="literal">.java</code> source file for your class will be written to the current
working directory when you run <code class="literal">sqoop</code>. You can control the output
directory with <code class="literal">--outdir</code>. For example, <code class="literal">--outdir src/generated/</code>.</p><p>The import process compiles the source into <code class="literal">.class</code> and <code class="literal">.jar</code> files;
these are ordinarily stored under <code class="literal">/tmp</code>. You can select an alternate
target directory with <code class="literal">--bindir</code>. For example, <code class="literal">--bindir /scratch</code>.</p><p>If you already have a compiled class that can be used to perform the
import and want to suppress the code-generation aspect of the import
process, you can use an existing jar and class by
providing the <code class="literal">--jar-file</code> and <code class="literal">--class-name</code> options. For example:</p><pre class="screen">$ sqoop import-mainframe --dataset SomePDS --jar-file mydatatypes.jar \
    --class-name SomePDSType</pre><p>This command will load the <code class="literal">SomePDSType</code> class out of <code class="literal">mydatatypes.jar</code>.</p></div><div class="section" title="9.2.10. Additional Import Configuration Properties"><div class="titlepage"><div><div><h4 class="title"><a name="_additional_import_configuration_properties_2"></a>9.2.10. Additional Import Configuration Properties</h4></div></div></div><p>There are some additional properties which can be configured by modifying
<code class="literal">conf/sqoop-site.xml</code>. Properties can be specified the same as in Hadoop
configuration files, for example:</p><pre class="screen">  &lt;property&gt;
    &lt;name&gt;property.name&lt;/name&gt;
    &lt;value&gt;property.value&lt;/value&gt;
  &lt;/property&gt;</pre><p>They can also be specified on the command line in the generic arguments, for
example:</p><pre class="screen">sqoop import -D property.name=property.value ...</pre></div></div><div class="section" title="9.3. Example Invocations"><div class="titlepage"><div><div><h3 class="title"><a name="_example_invocations_3"></a>9.3. Example Invocations</h3></div></div></div><p>The following examples illustrate how to use the import tool in a variety
of situations.</p><p>A basic import of all sequential files in a partitioned dataset named
<code class="literal">EMPLOYEES</code> in the mainframe host z390:</p><pre class="screen">$ sqoop import-mainframe --connect z390 --dataset EMPLOYEES \
    --username SomeUser -P
Enter password: (hidden)</pre><p>Controlling the import parallelism (using 8 parallel tasks):</p><pre class="screen">$ sqoop import-mainframe --connect z390 --dataset EMPLOYEES \
    --username SomeUser --password-file mypassword -m 8</pre><p>Importing the data to Hive:</p><pre class="screen">$ sqoop import-mainframe --connect z390 --dataset EMPLOYEES \
    --hive-import</pre></div></div><div class="section" title="10. sqoop-export"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_literal_sqoop_export_literal"></a>10. <code class="literal">sqoop-export</code></h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_purpose_4">10.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_4">10.2. Syntax</a></span></dt><dt><span class="section"><a href="#_inserts_vs_updates">10.3. Inserts vs. Updates</a></span></dt><dt><span class="section"><a href="#_exports_and_transactions">10.4. Exports and Transactions</a></span></dt><dt><span class="section"><a href="#_failed_exports">10.5. Failed Exports</a></span></dt><dt><span class="section"><a href="#_example_invocations_4">10.6. Example Invocations</a></span></dt></dl></div><div class="section" title="10.1. Purpose"><div class="titlepage"><div><div><h3 class="title"><a name="_purpose_4"></a>10.1. Purpose</h3></div></div></div><p>The <code class="literal">export</code> tool exports a set of files from HDFS back to an RDBMS.
The target table must already exist in the database. The input files
are read and parsed into a set of records according to the
user-specified delimiters.</p><p>The default operation is to transform these into a set of <code class="literal">INSERT</code>
statements that inject the records into the database. In "update mode,"
Sqoop will generate <code class="literal">UPDATE</code> statements that replace existing records
in the database, and in "call mode" Sqoop will make a stored procedure
call for each record.</p></div><div class="section" title="10.2. Syntax"><div class="titlepage"><div><div><h3 class="title"><a name="_syntax_4"></a>10.2. Syntax</h3></div></div></div><pre class="screen">$ sqoop export (generic-args) (export-args)
$ sqoop-export (generic-args) (export-args)</pre><p>Although the Hadoop generic arguments must preceed any export arguments,
the export arguments can be entered in any order with respect to one
another.</p><div class="table"><a name="idp4060120"></a><p class="title"><b>Table 27. Common arguments</b></p><div class="table-contents"><table summary="Common arguments" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connect &lt;jdbc-uri&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify JDBC connect string
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connection-manager &lt;class-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify connection manager class to                                          use
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--driver &lt;class-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Manually specify JDBC driver class                                          to use
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hadoop-mapred-home &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Override $HADOOP_MAPRED_HOME
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--help</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Print usage instructions
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--password-file</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set path for a file containing the                                          authentication password
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">-P</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Read password from console
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--password &lt;password&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set authentication password
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--username &lt;username&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set authentication username
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--verbose</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Print more information while working
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connection-param-file &lt;filename&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Optional properties file that                                          provides connection parameters
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--relaxed-isolation</code>
    </td><td style="" align="left">
    Set connection transaction isolation                                          to read uncommitted for the mappers.
    </td></tr></tbody></table></div></div><br class="table-break"><div class="table"><a name="idp4075600"></a><p class="title"><b>Table 28. Validation arguments <a class="link" href="#validation" title="11. validation">More Details</a></b></p><div class="table-contents"><table summary="Validation arguments More Details" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--validate</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Enable validation of data copied,                                             supports single table copy only.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--validator &lt;class-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify validator class to use.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--validation-threshold &lt;class-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify validation threshold class                                             to use.
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--validation-failurehandler &lt;class-name&gt;</code>
    </td><td style="" align="left">
    Specify validation failure                                             handler class to use.
    </td></tr></tbody></table></div></div><br class="table-break"><div class="table"><a name="idp4083240"></a><p class="title"><b>Table 29. Export control arguments:</b></p><div class="table-contents"><table summary="Export control arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--columns &lt;col,col,col&#8230;&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Columns to export to table
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--direct</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Use direct export fast path
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--export-dir &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    HDFS source path for the export
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">-m,--num-mappers &lt;n&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Use <span class="emphasis"><em>n</em></span> map tasks to export in                                         parallel
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--table &lt;table-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Table to populate
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--call &lt;stored-proc-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Stored Procedure to call
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--update-key &lt;col-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Anchor column to use for updates.                                         Use a comma separated list of columns                                         if there are more than one column.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--update-mode &lt;mode&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify how updates are performed                                         when new rows are found with                                         non-matching keys in database.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Legal values for <code class="literal">mode</code> include                                         <code class="literal">updateonly</code> (default) and                                         <code class="literal">allowinsert</code>.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--input-null-string &lt;null-string&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    The string to be interpreted as                                         null for string columns
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--input-null-non-string &lt;null-string&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    The string to be interpreted as                                         null for non-string columns
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--staging-table &lt;staging-table-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    The table in which data will be                                         staged before being inserted into                                         the destination table.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--clear-staging-table</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Indicates that any data present in                                         the staging table can be deleted.
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--batch</code>
    </td><td style="" align="left">
    Use batch mode for underlying                                         statement execution.
    </td></tr></tbody></table></div></div><br class="table-break"><p>The <code class="literal">--export-dir</code> argument and one of <code class="literal">--table</code> or <code class="literal">--call</code> are
required. These specify the table to populate in the database (or the
stored procedure to call), and the directory in HDFS that contains
the source data.</p><p>By default, all columns within a table are selected for export. You
can select a subset of columns and control their ordering by using the
<code class="literal">--columns</code> argument. This should include a comma-delimited list
of columns to export. For example: <code class="literal">--columns "col1,col2,col3"</code>. Note
that columns that are not included in the <code class="literal">--columns</code> parameter need
to have either defined default value or allow <code class="literal">NULL</code> values. Otherwise
your database will reject the imported data which in turn will make
Sqoop job fail.</p><p>You can control the number of mappers independently from the number of
files present in the directory. Export performance depends on the
degree of parallelism. By default, Sqoop will use four tasks in
parallel for the export process. This may not be optimal; you will
need to experiment with your own particular setup. Additional tasks
may offer better concurrency, but if the database is already
bottlenecked on updating indices, invoking triggers, and so on, then
additional load may decrease performance. The <code class="literal">--num-mappers</code> or <code class="literal">-m</code>
arguments control the number of map tasks, which is the degree of
parallelism used.</p><p>Some databases provides a direct mode for exports as well. Use the <code class="literal">--direct</code> argument
to specify this codepath. This may be higher-performance than the standard JDBC codepath.
Details about use of direct mode with each specific RDBMS, installation requirements, available
options and limitations can be found in <a class="xref" href="#connectors" title="25. Notes for specific connectors">Section 25, &#8220;Notes for specific connectors&#8221;</a>.</p><p>The <code class="literal">--input-null-string</code> and <code class="literal">--input-null-non-string</code> arguments are
optional. If <code class="literal">--input-null-string</code> is not specified, then the string
"null" will be interpreted as null for string-type columns.
If <code class="literal">--input-null-non-string</code> is not specified, then both the string
"null" and the empty string will be interpreted as null for non-string
columns. Note that, the empty string will be always interpreted as null
for non-string columns, in addition to other string if specified by
<code class="literal">--input-null-non-string</code>.</p><p>Since Sqoop breaks down export process into multiple transactions, it
is possible that a failed export job may result in partial data being
committed to the database. This can further lead to subsequent jobs
failing due to insert collisions in some cases, or lead to duplicated data
in others. You can overcome this problem by specifying a staging table via
the <code class="literal">--staging-table</code> option which acts as an auxiliary table that is used
to stage exported data. The staged data is finally moved to the destination
table in a single transaction.</p><p>In order to use the staging facility, you must create the staging table
prior to running the export job. This table must be structurally
identical to the target table. This table should either be empty before
the export job runs, or the <code class="literal">--clear-staging-table</code> option must be specified.
If the staging table contains data and the <code class="literal">--clear-staging-table</code> option is
specified, Sqoop will delete all of the data before starting the export job.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>Support for staging data prior to pushing it into the destination
table is not always available for <code class="literal">--direct</code> exports. It is also not available when
export is invoked using the <code class="literal">--update-key</code> option for updating existing data,
and when stored procedures are used to insert the data. It is best to check the <a class="xref" href="#connectors" title="25. Notes for specific connectors">Section 25, &#8220;Notes for specific connectors&#8221;</a> section to validate.</p></td></tr></table></div></div><div class="section" title="10.3. Inserts vs. Updates"><div class="titlepage"><div><div><h3 class="title"><a name="_inserts_vs_updates"></a>10.3. Inserts vs. Updates</h3></div></div></div><p>By default, <code class="literal">sqoop-export</code> appends new rows to a table; each input
record is transformed into an <code class="literal">INSERT</code> statement that adds a row to the
target database table. If your table has constraints (e.g., a primary
key column whose values must be unique) and already contains data, you
must take care to avoid inserting records that violate these
constraints. The export process will fail if an <code class="literal">INSERT</code> statement
fails. This mode is primarily intended for exporting records to a new,
empty table intended to receive these results.</p><p>If you specify the <code class="literal">--update-key</code> argument, Sqoop will instead modify
an existing dataset in the database. Each input record is treated as
an <code class="literal">UPDATE</code> statement that modifies an existing row. The row a
statement modifies is determined by the column name(s) specified with
<code class="literal">--update-key</code>. For example, consider the following table
definition:</p><pre class="screen">CREATE TABLE foo(
    id INT NOT NULL PRIMARY KEY,
    msg VARCHAR(32),
    bar INT);</pre><p>Consider also a dataset in HDFS containing records like these:</p><pre class="screen">0,this is a test,42
1,some more data,100
...</pre><p>Running <code class="literal">sqoop-export --table foo --update-key id --export-dir
/path/to/data --connect &#8230;</code> will run an export job that executes SQL
statements based on the data like so:</p><pre class="screen">UPDATE foo SET msg='this is a test', bar=42 WHERE id=0;
UPDATE foo SET msg='some more data', bar=100 WHERE id=1;
...</pre><p>If an <code class="literal">UPDATE</code> statement modifies no rows, this is not considered an
error; the export will silently continue. (In effect, this means that
an update-based export will not insert new rows into the database.)
Likewise, if the column specified with <code class="literal">--update-key</code> does not
uniquely identify rows and multiple rows are updated by a single
statement, this condition is also undetected.</p><p>The argument <code class="literal">--update-key</code> can also be given a comma separated list of
column names. In which case, Sqoop will match all keys from this list before
updating any existing record.</p><p>Depending on the target database, you may also specify the <code class="literal">--update-mode</code>
argument with <code class="literal">allowinsert</code> mode if you want to update rows if they exist
in the database already or insert rows if they do not exist yet.</p><div class="table"><a name="idp4126480"></a><p class="title"><b>Table 30. Input parsing arguments:</b></p><div class="table-contents"><table summary="Input parsing arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--input-enclosed-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets a required field encloser
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--input-escaped-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the input escape                                          character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--input-fields-terminated-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the input field separator
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--input-lines-terminated-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the input end-of-line                                          character
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--input-optionally-enclosed-by &lt;char&gt;</code>
    </td><td style="" align="left">
    Sets a field enclosing                                          character
    </td></tr></tbody></table></div></div><br class="table-break"><div class="table"><a name="idp4134904"></a><p class="title"><b>Table 31. Output line formatting arguments:</b></p><div class="table-contents"><table summary="Output line formatting arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--enclosed-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets a required field enclosing                                    character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--escaped-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the escape character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--fields-terminated-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the field separator character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--lines-terminated-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the end-of-line character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--mysql-delimiters</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Uses MySQL&#8217;s default delimiter set:                                   fields: <code class="literal">,</code>  lines: <code class="literal">\n</code>                                    escaped-by: <code class="literal">\</code>                                    optionally-enclosed-by: <code class="literal">'</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--optionally-enclosed-by &lt;char&gt;</code>
    </td><td style="" align="left">
    Sets a field enclosing character
    </td></tr></tbody></table></div></div><br class="table-break"><p>Sqoop automatically generates code to parse and interpret records of the
files containing the data to be exported back to the database. If
these files were created with non-default delimiters (comma-separated
fields with newline-separated records), you should specify
the same delimiters again so that Sqoop can parse your files.</p><p>If you specify incorrect delimiters, Sqoop will fail to find enough
columns per line. This will cause export map tasks to fail by throwing
<code class="literal">ParseExceptions</code>.</p><div class="table"><a name="idp4147304"></a><p class="title"><b>Table 32. Code generation arguments:</b></p><div class="table-contents"><table summary="Code generation arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--bindir &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Output directory for compiled objects
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--class-name &lt;name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the generated class name. This overrides                         <code class="literal">--package-name</code>. When combined with                          <code class="literal">--jar-file</code>, sets the input class.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--jar-file &lt;file&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Disable code generation; use specified jar
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--outdir &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Output directory for generated code
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--package-name &lt;name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Put auto-generated classes in this package
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--map-column-java &lt;m&gt;</code>
    </td><td style="" align="left">
    Override default mapping from SQL type to                         Java type for configured columns.
    </td></tr></tbody></table></div></div><br class="table-break"><p>If the records to be exported were generated as the result of a
previous import, then the original generated class can be used to read
the data back. Specifying <code class="literal">--jar-file</code> and <code class="literal">--class-name</code> obviate
the need to specify delimiters in this case.</p><p>The use of existing generated code is incompatible with
<code class="literal">--update-key</code>; an update-mode export requires new code generation to
perform the update. You cannot use <code class="literal">--jar-file</code>, and must fully specify
any non-default delimiters.</p></div><div class="section" title="10.4. Exports and Transactions"><div class="titlepage"><div><div><h3 class="title"><a name="_exports_and_transactions"></a>10.4. Exports and Transactions</h3></div></div></div><p>Exports are performed by multiple writers in parallel. Each writer
uses a separate connection to the database; these have separate
transactions from one another. Sqoop uses the multi-row <code class="literal">INSERT</code>
syntax to insert up to 100 records per statement. Every 100
statements, the current transaction within a writer task is committed,
causing a commit every 10,000 rows. This ensures that transaction
buffers do not grow without bound, and cause out-of-memory conditions.
Therefore, an export is not an atomic process. Partial results from
the export will become visible before the export is complete.</p></div><div class="section" title="10.5. Failed Exports"><div class="titlepage"><div><div><h3 class="title"><a name="_failed_exports"></a>10.5. Failed Exports</h3></div></div></div><p>Exports may fail for a number of reasons:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Loss of connectivity from the Hadoop cluster to the database (either
  due to hardware fault, or server software crashes)
</li><li class="listitem">
Attempting to <code class="literal">INSERT</code> a row which violates a consistency constraint
  (for example, inserting a duplicate primary key value)
</li><li class="listitem">
Attempting to parse an incomplete or malformed record from the HDFS
  source data
</li><li class="listitem">
Attempting to parse records using incorrect delimiters
</li><li class="listitem">
Capacity issues (such as insufficient RAM or disk space)
</li></ul></div><p>If an export map task fails due to these or other reasons, it will
cause the export job to fail. The results of a failed export are
undefined. Each export map task operates in a separate transaction.
Furthermore, individual map tasks <code class="literal">commit</code> their current transaction
periodically. If a task fails, the current transaction will be rolled
back. Any previously-committed transactions will remain durable in the
database, leading to a partially-complete export.</p></div><div class="section" title="10.6. Example Invocations"><div class="titlepage"><div><div><h3 class="title"><a name="_example_invocations_4"></a>10.6. Example Invocations</h3></div></div></div><p>A basic export to populate a table named <code class="literal">bar</code>:</p><pre class="screen">$ sqoop export --connect jdbc:mysql://db.example.com/foo --table bar  \
    --export-dir /results/bar_data</pre><p>This example takes the files in <code class="literal">/results/bar_data</code> and injects their
contents in to the <code class="literal">bar</code> table in the <code class="literal">foo</code> database on <code class="literal">db.example.com</code>.
The target table must already exist in the database. Sqoop performs
a set of <code class="literal">INSERT INTO</code> operations, without regard for existing content. If
Sqoop attempts to insert rows which violate constraints in the database
(for example, a particular primary key value already exists), then the export
fails.</p><p>Alternatively, you can specify the columns to be exported by providing
<code class="literal">--columns "col1,col2,col3"</code>. Please note that columns that are not included
in the <code class="literal">--columns</code> parameter need to have either defined default value or
allow <code class="literal">NULL</code> values. Otherwise your database will reject the imported data
which in turn will make Sqoop job fail.</p><p>Another basic export to populate a table named <code class="literal">bar</code> with validation enabled:
<a class="link" href="#validation" title="11. validation">More Details</a></p><pre class="screen">$ sqoop export --connect jdbc:mysql://db.example.com/foo --table bar  \
    --export-dir /results/bar_data --validate</pre><p>An export that calls a stored procedure named <code class="literal">barproc</code> for every record in
<code class="literal">/results/bar_data</code> would look like:</p><pre class="screen">$ sqoop export --connect jdbc:mysql://db.example.com/foo --call barproc \
    --export-dir /results/bar_data</pre></div></div><div class="section" title="11. validation"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="validation"></a>11. <code class="literal">validation</code></h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_purpose_5">11.1. Purpose</a></span></dt><dt><span class="section"><a href="#_introduction_2">11.2. Introduction</a></span></dt><dt><span class="section"><a href="#_syntax_5">11.3. Syntax</a></span></dt><dt><span class="section"><a href="#_configuration">11.4. Configuration</a></span></dt><dt><span class="section"><a href="#_limitations">11.5. Limitations</a></span></dt><dt><span class="section"><a href="#_example_invocations_5">11.6. Example Invocations</a></span></dt></dl></div><div class="section" title="11.1. Purpose"><div class="titlepage"><div><div><h3 class="title"><a name="_purpose_5"></a>11.1. Purpose</h3></div></div></div><p>Validate the data copied, either import or export by comparing the row
counts from the source and the target post copy.</p></div><div class="section" title="11.2. Introduction"><div class="titlepage"><div><div><h3 class="title"><a name="_introduction_2"></a>11.2. Introduction</h3></div></div></div><p>There are 3 basic interfaces:
ValidationThreshold - Determines if the error margin between the source and
target are acceptable: Absolute, Percentage Tolerant, etc.
Default implementation is AbsoluteValidationThreshold which ensures the row
counts from source and targets are the same.</p><p>ValidationFailureHandler - Responsible for handling failures: log an
error/warning, abort, etc.
Default implementation is LogOnFailureHandler that logs a warning message to
the configured logger.</p><p>Validator - Drives the validation logic by delegating the decision to
ValidationThreshold and delegating failure handling to ValidationFailureHandler.
The default implementation is RowCountValidator which validates the row
counts from source and the target.</p></div><div class="section" title="11.3. Syntax"><div class="titlepage"><div><div><h3 class="title"><a name="_syntax_5"></a>11.3. Syntax</h3></div></div></div><pre class="screen">$ sqoop import (generic-args) (import-args)
$ sqoop export (generic-args) (export-args)</pre><p>Validation arguments are part of import and export arguments.</p></div><div class="section" title="11.4. Configuration"><div class="titlepage"><div><div><h3 class="title"><a name="_configuration"></a>11.4. Configuration</h3></div></div></div><p>The validation framework is extensible and pluggable. It comes with default
implementations but the interfaces can be extended to allow custom
implementations by passing them as part of the command line arguments as
described below.</p><p title="Validator"><b>Validator. </b>
</p><pre class="literallayout">Property:         validator
Description:      Driver for validation,
                  must implement org.apache.sqoop.validation.Validator
Supported values: The value has to be a fully qualified class name.
Default value:    org.apache.sqoop.validation.RowCountValidator</pre><p title="Validator">
</p><p title="Validation Threshold"><b>Validation Threshold. </b>
</p><pre class="literallayout">Property:         validation-threshold
Description:      Drives the decision based on the validation meeting the
                  threshold or not. Must implement
                  org.apache.sqoop.validation.ValidationThreshold
Supported values: The value has to be a fully qualified class name.
Default value:    org.apache.sqoop.validation.AbsoluteValidationThreshold</pre><p title="Validation Threshold">
</p><p title="Validation Failure Handler"><b>Validation Failure Handler. </b>
</p><pre class="literallayout">Property:         validation-failurehandler
Description:      Responsible for handling failures, must implement
                  org.apache.sqoop.validation.ValidationFailureHandler
Supported values: The value has to be a fully qualified class name.
Default value:    org.apache.sqoop.validation.AbortOnFailureHandler</pre><p title="Validation Failure Handler">
</p></div><div class="section" title="11.5. Limitations"><div class="titlepage"><div><div><h3 class="title"><a name="_limitations"></a>11.5. Limitations</h3></div></div></div><p>Validation currently only validates data copied from a single table into HDFS.
The following are the limitations in the current implementation:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
all-tables option
</li><li class="listitem">
free-form query option
</li><li class="listitem">
Data imported into Hive, HBase or Accumulo
</li><li class="listitem">
table import with --where argument
</li><li class="listitem">
incremental imports
</li></ul></div></div><div class="section" title="11.6. Example Invocations"><div class="titlepage"><div><div><h3 class="title"><a name="_example_invocations_5"></a>11.6. Example Invocations</h3></div></div></div><p>A basic import of a table named <code class="literal">EMPLOYEES</code> in the <code class="literal">corp</code> database that uses
validation to validate the row counts:</p><pre class="screen">$ sqoop import --connect jdbc:mysql://db.foo.com/corp  \
    --table EMPLOYEES --validate</pre><p>A basic export to populate a table named <code class="literal">bar</code> with validation enabled:</p><pre class="screen">$ sqoop export --connect jdbc:mysql://db.example.com/foo --table bar  \
    --export-dir /results/bar_data --validate</pre><p>Another example that overrides the validation args:</p><pre class="screen">$ sqoop import --connect jdbc:mysql://db.foo.com/corp --table EMPLOYEES \
    --validate --validator org.apache.sqoop.validation.RowCountValidator \
    --validation-threshold \
          org.apache.sqoop.validation.AbsoluteValidationThreshold \
    --validation-failurehandler \
          org.apache.sqoop.validation.AbortOnFailureHandler</pre></div></div><div class="section" title="12. Saved Jobs"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_saved_jobs"></a>12. Saved Jobs</h2></div></div></div><p>Imports and exports can be repeatedly performed by issuing the same command
multiple times. Especially when using the incremental import capability,
this is an expected scenario.</p><p>Sqoop allows you to define <span class="emphasis"><em>saved jobs</em></span> which make this process easier. A
saved job records the configuration information required to execute a
Sqoop command at a later time. The section on the <code class="literal">sqoop-job</code> tool
describes how to create and work with saved jobs.</p><p>By default, job descriptions are saved to a private repository stored
in <code class="literal">$HOME/.sqoop/</code>. You can configure Sqoop to instead use a shared
<span class="emphasis"><em>metastore</em></span>, which makes saved jobs available to multiple users across a
shared cluster. Starting the metastore is covered by the section on the
<code class="literal">sqoop-metastore</code> tool.</p></div><div class="section" title="13. sqoop-job"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_literal_sqoop_job_literal"></a>13. <code class="literal">sqoop-job</code></h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_purpose_6">13.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_6">13.2. Syntax</a></span></dt><dt><span class="section"><a href="#_saved_jobs_and_passwords">13.3. Saved jobs and passwords</a></span></dt><dt><span class="section"><a href="#_saved_jobs_and_incremental_imports">13.4. Saved jobs and incremental imports</a></span></dt></dl></div><div class="section" title="13.1. Purpose"><div class="titlepage"><div><div><h3 class="title"><a name="_purpose_6"></a>13.1. Purpose</h3></div></div></div><p>The job tool allows you to create and work with saved jobs. Saved jobs
remember the parameters used to specify a job, so they can be
re-executed by invoking the job by its handle.</p><p>If a saved job is configured to perform an incremental import, state regarding
the most recently imported rows is updated in the saved job to allow the job
to continually import only the newest rows.</p></div><div class="section" title="13.2. Syntax"><div class="titlepage"><div><div><h3 class="title"><a name="_syntax_6"></a>13.2. Syntax</h3></div></div></div><pre class="screen">$ sqoop job (generic-args) (job-args) [-- [subtool-name] (subtool-args)]
$ sqoop-job (generic-args) (job-args) [-- [subtool-name] (subtool-args)]</pre><p>Although the Hadoop generic arguments must preceed any job arguments,
the job arguments can be entered in any order with respect to one
another.</p><div class="table"><a name="idp4200936"></a><p class="title"><b>Table 33. Job management options:</b></p><div class="table-contents"><table summary="Job management options:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--create &lt;job-id&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Define a new saved job with the specified                             job-id (name). A second Sqoop                             command-line, separated by a <code class="literal">--</code> should                             be specified; this defines the saved job.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--delete &lt;job-id&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Delete a saved job.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--exec &lt;job-id&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Given a job defined with <code class="literal">--create</code>, run                             the saved job.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--show &lt;job-id&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Show the parameters for a saved job.
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--list</code>
    </td><td style="" align="left">
    List all saved jobs
    </td></tr></tbody></table></div></div><br class="table-break"><p>Creating saved jobs is done with the <code class="literal">--create</code> action. This operation
requires a <code class="literal">--</code> followed by a tool name and its arguments. The tool and
its arguments will form the basis of the saved job. Consider:</p><pre class="screen">$ sqoop job --create myjob -- import --connect jdbc:mysql://example.com/db \
    --table mytable</pre><p>This creates a job named <code class="literal">myjob</code> which can be executed later. The job is not
run. This job is now available in the list of saved jobs:</p><pre class="screen">$ sqoop job --list
Available jobs:
  myjob</pre><p>We can inspect the configuration of a job with the <code class="literal">show</code> action:</p><pre class="screen"> $ sqoop job --show myjob
 Job: myjob
 Tool: import
 Options:
 ----------------------------
 direct.import = false
 codegen.input.delimiters.record = 0
 hdfs.append.dir = false
 db.table = mytable
 ...</pre><p>And if we are satisfied with it, we can run the job with <code class="literal">exec</code>:</p><pre class="screen">$ sqoop job --exec myjob
10/08/19 13:08:45 INFO tool.CodeGenTool: Beginning code generation
...</pre><p>The <code class="literal">exec</code> action allows you to override arguments of the saved job
by supplying them after a <code class="literal">--</code>. For example, if the database were
changed to require a username, we could specify the username and
password with:</p><pre class="screen">$ sqoop job --exec myjob -- --username someuser -P
Enter password:
...</pre><div class="table"><a name="idp4216328"></a><p class="title"><b>Table 34. Metastore connection options:</b></p><div class="table-contents"><table summary="Metastore connection options:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--meta-connect &lt;jdbc-uri&gt;</code>
    </td><td style="" align="left">
    Specifies the JDBC connect string used                              to connect to the metastore
    </td></tr></tbody></table></div></div><br class="table-break"><p>By default, a private metastore is instantiated in <code class="literal">$HOME/.sqoop</code>. If
you have configured a hosted metastore with the <code class="literal">sqoop-metastore</code>
tool, you can connect to it by specifying the <code class="literal">--meta-connect</code>
argument. This is a JDBC connect string just like the ones used to
connect to databases for import.</p><p>In <code class="literal">conf/sqoop-site.xml</code>, you can configure
<code class="literal">sqoop.metastore.client.autoconnect.url</code> with this address, so you do not have
to supply <code class="literal">--meta-connect</code> to use a remote metastore. This parameter can
also be modified to move the private metastore to a location on your
filesystem other than your home directory.</p><p>If you configure <code class="literal">sqoop.metastore.client.enable.autoconnect</code> with the
value <code class="literal">false</code>, then you must explicitly supply <code class="literal">--meta-connect</code>.</p><div class="table"><a name="idp4224864"></a><p class="title"><b>Table 35. Common options:</b></p><div class="table-contents"><table summary="Common options:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--help</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Print usage instructions
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--verbose</code>
    </td><td style="" align="left">
    Print more information while working
    </td></tr></tbody></table></div></div><br class="table-break"></div><div class="section" title="13.3. Saved jobs and passwords"><div class="titlepage"><div><div><h3 class="title"><a name="_saved_jobs_and_passwords"></a>13.3. Saved jobs and passwords</h3></div></div></div><p>The Sqoop metastore is not a secure resource. Multiple users can access
its contents. For this reason, Sqoop does not store passwords in the
metastore. If you create a job that requires a password, you will be
prompted for that password each time you execute the job.</p><p>You can enable passwords in the metastore by setting
<code class="literal">sqoop.metastore.client.record.password</code> to <code class="literal">true</code> in the configuration.</p><p>Note that you have to set <code class="literal">sqoop.metastore.client.record.password</code> to <code class="literal">true</code>
if you are executing saved jobs via Oozie because Sqoop cannot prompt the user
to enter passwords while being executed as Oozie tasks.</p></div><div class="section" title="13.4. Saved jobs and incremental imports"><div class="titlepage"><div><div><h3 class="title"><a name="_saved_jobs_and_incremental_imports"></a>13.4. Saved jobs and incremental imports</h3></div></div></div><p>Incremental imports are performed by comparing the values in a <span class="emphasis"><em>check column</em></span>
against a reference value for the most recent import. For example, if the
<code class="literal">--incremental append</code> argument was specified, along with <code class="literal">--check-column
id</code> and <code class="literal">--last-value 100</code>, all rows with <code class="literal">id &gt; 100</code> will be imported.
If an incremental import is run from the command line, the value which
should be specified as <code class="literal">--last-value</code> in a subsequent incremental import
will be printed to the screen for your reference. If an incremental import is
run from a saved job, this value will be retained in the saved job. Subsequent
runs of <code class="literal">sqoop job --exec someIncrementalJob</code> will continue to import only
newer rows than those previously imported.</p></div></div><div class="section" title="14. sqoop-metastore"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_literal_sqoop_metastore_literal"></a>14. <code class="literal">sqoop-metastore</code></h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_purpose_7">14.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_7">14.2. Syntax</a></span></dt></dl></div><div class="section" title="14.1. Purpose"><div class="titlepage"><div><div><h3 class="title"><a name="_purpose_7"></a>14.1. Purpose</h3></div></div></div><p>The <code class="literal">metastore</code> tool configures Sqoop to host a shared metadata repository.
Multiple users and/or remote users can define and execute saved jobs (created
with <code class="literal">sqoop job</code>) defined in this metastore.</p><p>Clients must be configured to connect to the metastore in <code class="literal">sqoop-site.xml</code> or
with the <code class="literal">--meta-connect</code> argument.</p></div><div class="section" title="14.2. Syntax"><div class="titlepage"><div><div><h3 class="title"><a name="_syntax_7"></a>14.2. Syntax</h3></div></div></div><pre class="screen">$ sqoop metastore (generic-args) (metastore-args)
$ sqoop-metastore (generic-args) (metastore-args)</pre><p>Although the Hadoop generic arguments must preceed any metastore arguments,
the metastore arguments can be entered in any order with respect to one
another.</p><div class="table"><a name="idp4241520"></a><p class="title"><b>Table 36. Metastore management options:</b></p><div class="table-contents"><table summary="Metastore management options:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--shutdown</code>
    </td><td style="" align="left">
    Shuts down a running metastore instance                             on the same machine.
    </td></tr></tbody></table></div></div><br class="table-break"><p>Running <code class="literal">sqoop-metastore</code> launches a shared HSQLDB database instance on
the current machine. Clients can connect to this metastore and create jobs
which can be shared between users for execution.</p><p>The location of the metastore&#8217;s files on disk is controlled by the
<code class="literal">sqoop.metastore.server.location</code> property in <code class="literal">conf/sqoop-site.xml</code>.
This should point to a directory on the local filesystem.</p><p>The metastore is available over TCP/IP. The port is controlled by the
<code class="literal">sqoop.metastore.server.port</code> configuration parameter, and defaults to 16000.</p><p>Clients should connect to the metastore by specifying
<code class="literal">sqoop.metastore.client.autoconnect.url</code> or <code class="literal">--meta-connect</code> with the
value <code class="literal">jdbc:hsqldb:hsql://&lt;server-name&gt;:&lt;port&gt;/sqoop</code>. For example,
<code class="literal">jdbc:hsqldb:hsql://metaserver.example.com:16000/sqoop</code>.</p><p>This metastore may be hosted on a machine within the Hadoop cluster, or
elsewhere on the network.</p></div></div><div class="section" title="15. sqoop-merge"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_literal_sqoop_merge_literal"></a>15. <code class="literal">sqoop-merge</code></h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_purpose_8">15.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_8">15.2. Syntax</a></span></dt></dl></div><div class="section" title="15.1. Purpose"><div class="titlepage"><div><div><h3 class="title"><a name="_purpose_8"></a>15.1. Purpose</h3></div></div></div><p>The merge tool allows you to combine two datasets where entries in one
dataset should overwrite entries of an older dataset. For example, an
incremental import run in last-modified mode will generate multiple datasets
in HDFS where successively newer data appears in each dataset. The <code class="literal">merge</code>
tool will "flatten" two datasets into one, taking the newest available
records for each primary key.</p></div><div class="section" title="15.2. Syntax"><div class="titlepage"><div><div><h3 class="title"><a name="_syntax_8"></a>15.2. Syntax</h3></div></div></div><pre class="screen">$ sqoop merge (generic-args) (merge-args)
$ sqoop-merge (generic-args) (merge-args)</pre><p>Although the Hadoop generic arguments must preceed any merge arguments,
the job arguments can be entered in any order with respect to one
another.</p><div class="table"><a name="idp4254720"></a><p class="title"><b>Table 37. Merge options:</b></p><div class="table-contents"><table summary="Merge options:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--class-name &lt;class&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify the name of the record-specific                             class to use during the merge job.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--jar-file &lt;file&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify the name of the jar to load the                             record class from.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--merge-key &lt;col&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify the name of a column to use as                             the merge key.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--new-data &lt;path&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify the path of the newer dataset.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--onto &lt;path&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify the path of the older dataset.
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--target-dir &lt;path&gt;</code>
    </td><td style="" align="left">
    Specify the target path for the output                             of the merge job.
    </td></tr></tbody></table></div></div><br class="table-break"><p>The <code class="literal">merge</code> tool runs a MapReduce job that takes two directories as
input: a newer dataset, and an older one. These are specified with
<code class="literal">--new-data</code> and <code class="literal">--onto</code> respectively. The output of the MapReduce
job will be placed in the directory in HDFS specified by <code class="literal">--target-dir</code>.</p><p>When merging the datasets, it is assumed that there is a unique primary
key value in each record. The column for the primary key is specified
with <code class="literal">--merge-key</code>. Multiple rows in the same dataset should not
have the same primary key, or else data loss may occur.</p><p>To parse the dataset and extract the key column, the auto-generated
class from a previous import must be used. You should specify the
class name and jar file with <code class="literal">--class-name</code> and <code class="literal">--jar-file</code>. If
this is not availab,e you can recreate the class using the <code class="literal">codegen</code>
tool.</p><p>The merge tool is typically run after an incremental import with the
date-last-modified mode (<code class="literal">sqoop import --incremental lastmodified &#8230;</code>).</p><p>Supposing two incremental imports were performed, where some older data
is in an HDFS directory named <code class="literal">older</code> and newer data is in an HDFS
directory named <code class="literal">newer</code>, these could be merged like so:</p><pre class="screen">$ sqoop merge --new-data newer --onto older --target-dir merged \
    --jar-file datatypes.jar --class-name Foo --merge-key id</pre><p>This would run a MapReduce job where the value in the <code class="literal">id</code> column
of each row is used to join rows; rows in the <code class="literal">newer</code> dataset will
be used in preference to rows in the <code class="literal">older</code> dataset.</p><p>This can be used with both SequenceFile-, Avro- and text-based
incremental imports. The file types of the newer and older datasets
must be the same.</p></div></div><div class="section" title="16. sqoop-codegen"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_literal_sqoop_codegen_literal"></a>16. <code class="literal">sqoop-codegen</code></h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_purpose_9">16.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_9">16.2. Syntax</a></span></dt><dt><span class="section"><a href="#_example_invocations_6">16.3. Example Invocations</a></span></dt></dl></div><div class="section" title="16.1. Purpose"><div class="titlepage"><div><div><h3 class="title"><a name="_purpose_9"></a>16.1. Purpose</h3></div></div></div><p>The <code class="literal">codegen</code> tool generates Java classes which encapsulate and
interpret imported records. The Java definition of a record is
instantiated as part of the import process, but can also be performed
separately. For example, if Java source is lost, it can be recreated.
New versions of a class can be created which use different delimiters
between fields, and so on.</p></div><div class="section" title="16.2. Syntax"><div class="titlepage"><div><div><h3 class="title"><a name="_syntax_9"></a>16.2. Syntax</h3></div></div></div><pre class="screen">$ sqoop codegen (generic-args) (codegen-args)
$ sqoop-codegen (generic-args) (codegen-args)</pre><p>Although the Hadoop generic arguments must preceed any codegen arguments,
the codegen arguments can be entered in any order with respect to one
another.</p><div class="table"><a name="idp4276888"></a><p class="title"><b>Table 38. Common arguments</b></p><div class="table-contents"><table summary="Common arguments" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connect &lt;jdbc-uri&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify JDBC connect string
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connection-manager &lt;class-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify connection manager class to                                          use
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--driver &lt;class-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Manually specify JDBC driver class                                          to use
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hadoop-mapred-home &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Override $HADOOP_MAPRED_HOME
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--help</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Print usage instructions
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--password-file</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set path for a file containing the                                          authentication password
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">-P</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Read password from console
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--password &lt;password&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set authentication password
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--username &lt;username&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set authentication username
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--verbose</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Print more information while working
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connection-param-file &lt;filename&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Optional properties file that                                          provides connection parameters
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--relaxed-isolation</code>
    </td><td style="" align="left">
    Set connection transaction isolation                                          to read uncommitted for the mappers.
    </td></tr></tbody></table></div></div><br class="table-break"><div class="table"><a name="idp4292368"></a><p class="title"><b>Table 39. Code generation arguments:</b></p><div class="table-contents"><table summary="Code generation arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--bindir &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Output directory for compiled objects
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--class-name &lt;name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the generated class name. This overrides                         <code class="literal">--package-name</code>. When combined with                          <code class="literal">--jar-file</code>, sets the input class.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--jar-file &lt;file&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Disable code generation; use specified jar
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--outdir &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Output directory for generated code
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--package-name &lt;name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Put auto-generated classes in this package
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--map-column-java &lt;m&gt;</code>
    </td><td style="" align="left">
    Override default mapping from SQL type to                         Java type for configured columns.
    </td></tr></tbody></table></div></div><br class="table-break"><div class="table"><a name="idp4302224"></a><p class="title"><b>Table 40. Output line formatting arguments:</b></p><div class="table-contents"><table summary="Output line formatting arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--enclosed-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets a required field enclosing                                    character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--escaped-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the escape character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--fields-terminated-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the field separator character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--lines-terminated-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the end-of-line character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--mysql-delimiters</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Uses MySQL&#8217;s default delimiter set:                                   fields: <code class="literal">,</code>  lines: <code class="literal">\n</code>                                    escaped-by: <code class="literal">\</code>                                    optionally-enclosed-by: <code class="literal">'</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--optionally-enclosed-by &lt;char&gt;</code>
    </td><td style="" align="left">
    Sets a field enclosing character
    </td></tr></tbody></table></div></div><br class="table-break"><div class="table"><a name="idp4312952"></a><p class="title"><b>Table 41. Input parsing arguments:</b></p><div class="table-contents"><table summary="Input parsing arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--input-enclosed-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets a required field encloser
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--input-escaped-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the input escape                                          character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--input-fields-terminated-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the input field separator
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--input-lines-terminated-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the input end-of-line                                          character
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--input-optionally-enclosed-by &lt;char&gt;</code>
    </td><td style="" align="left">
    Sets a field enclosing                                          character
    </td></tr></tbody></table></div></div><br class="table-break"><div class="table"><a name="idp4321376"></a><p class="title"><b>Table 42. Hive arguments:</b></p><div class="table-contents"><table summary="Hive arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-home &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Override <code class="literal">$HIVE_HOME</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-import</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Import tables into Hive (Uses Hive&#8217;s                               default delimiters if none are set.)
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-overwrite</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Overwrite existing data in the Hive table.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--create-hive-table</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    If set, then the job will fail if the target hive
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    table exits. By default this property is false.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-table &lt;table-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the table name to use when importing                              to Hive.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-drop-import-delims</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Drops <span class="emphasis"><em>\n</em></span>, <span class="emphasis"><em>\r</em></span>, and <span class="emphasis"><em>\01</em></span> from string                              fields when importing to Hive.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-delims-replacement</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Replace <span class="emphasis"><em>\n</em></span>, <span class="emphasis"><em>\r</em></span>, and <span class="emphasis"><em>\01</em></span> from string                              fields with user defined string when importing to Hive.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-partition-key</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Name of a hive field to partition are                               sharded on
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-partition-value &lt;v&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    String-value that serves as partition key                              for this imported into hive in this job.
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--map-column-hive &lt;map&gt;</code>
    </td><td style="" align="left">
    Override default mapping from SQL type to                              Hive type for configured columns.
    </td></tr></tbody></table></div></div><br class="table-break"><p>If Hive arguments are provided to the code generation tool, Sqoop
generates a file containing the HQL statements to create a table and
load data.</p></div><div class="section" title="16.3. Example Invocations"><div class="titlepage"><div><div><h3 class="title"><a name="_example_invocations_6"></a>16.3. Example Invocations</h3></div></div></div><p>Recreate the record interpretation code for the <code class="literal">employees</code> table of a
corporate database:</p><pre class="screen">$ sqoop codegen --connect jdbc:mysql://db.example.com/corp \
    --table employees</pre></div></div><div class="section" title="17. sqoop-create-hive-table"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_literal_sqoop_create_hive_table_literal"></a>17. <code class="literal">sqoop-create-hive-table</code></h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_purpose_10">17.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_10">17.2. Syntax</a></span></dt><dt><span class="section"><a href="#_example_invocations_7">17.3. Example Invocations</a></span></dt></dl></div><div class="section" title="17.1. Purpose"><div class="titlepage"><div><div><h3 class="title"><a name="_purpose_10"></a>17.1. Purpose</h3></div></div></div><p>The <code class="literal">create-hive-table</code> tool populates a Hive metastore with a
definition for a table based on a database table previously imported
to HDFS, or one planned to be imported. This effectively performs the
"<code class="literal">--hive-import</code>" step of <code class="literal">sqoop-import</code> without running the
preceeding import.</p><p>If data was already loaded to HDFS, you can use this tool to finish
the pipeline of importing the data to Hive. You can also create Hive tables
with this tool; data then can be imported and populated into
the target after a preprocessing step run by the user.</p></div><div class="section" title="17.2. Syntax"><div class="titlepage"><div><div><h3 class="title"><a name="_syntax_10"></a>17.2. Syntax</h3></div></div></div><pre class="screen">$ sqoop create-hive-table (generic-args) (create-hive-table-args)
$ sqoop-create-hive-table (generic-args) (create-hive-table-args)</pre><p>Although the Hadoop generic arguments must preceed any create-hive-table
arguments, the create-hive-table arguments can be entered in any order
with respect to one another.</p><div class="table"><a name="idp4344248"></a><p class="title"><b>Table 43. Common arguments</b></p><div class="table-contents"><table summary="Common arguments" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connect &lt;jdbc-uri&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify JDBC connect string
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connection-manager &lt;class-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify connection manager class to                                          use
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--driver &lt;class-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Manually specify JDBC driver class                                          to use
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hadoop-mapred-home &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Override $HADOOP_MAPRED_HOME
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--help</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Print usage instructions
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--password-file</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set path for a file containing the                                          authentication password
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">-P</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Read password from console
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--password &lt;password&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set authentication password
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--username &lt;username&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set authentication username
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--verbose</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Print more information while working
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connection-param-file &lt;filename&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Optional properties file that                                          provides connection parameters
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--relaxed-isolation</code>
    </td><td style="" align="left">
    Set connection transaction isolation                                          to read uncommitted for the mappers.
    </td></tr></tbody></table></div></div><br class="table-break"><div class="table"><a name="idp4359680"></a><p class="title"><b>Table 44. Hive arguments:</b></p><div class="table-contents"><table summary="Hive arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-home &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Override <code class="literal">$HIVE_HOME</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-overwrite</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Overwrite existing data in the Hive table.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--create-hive-table</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    If set, then the job will fail if the target hive
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    table exits. By default this property is false.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hive-table &lt;table-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the table name to use when importing                               to Hive.
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--table</code>
    </td><td style="" align="left">
    The database table to read the                               definition from.
    </td></tr></tbody></table></div></div><br class="table-break"><div class="table"><a name="idp4368752"></a><p class="title"><b>Table 45. Output line formatting arguments:</b></p><div class="table-contents"><table summary="Output line formatting arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--enclosed-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets a required field enclosing                                    character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--escaped-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the escape character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--fields-terminated-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the field separator character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--lines-terminated-by &lt;char&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Sets the end-of-line character
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--mysql-delimiters</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Uses MySQL&#8217;s default delimiter set:                                   fields: <code class="literal">,</code>  lines: <code class="literal">\n</code>                                    escaped-by: <code class="literal">\</code>                                    optionally-enclosed-by: <code class="literal">'</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--optionally-enclosed-by &lt;char&gt;</code>
    </td><td style="" align="left">
    Sets a field enclosing character
    </td></tr></tbody></table></div></div><br class="table-break"><p>Do not use enclosed-by or escaped-by delimiters with output formatting
arguments used to import to Hive. Hive cannot currently parse them.</p></div><div class="section" title="17.3. Example Invocations"><div class="titlepage"><div><div><h3 class="title"><a name="_example_invocations_7"></a>17.3. Example Invocations</h3></div></div></div><p>Define in Hive a table named <code class="literal">emps</code> with a definition based on a
database table named <code class="literal">employees</code>:</p><pre class="screen">$ sqoop create-hive-table --connect jdbc:mysql://db.example.com/corp \
    --table employees --hive-table emps</pre></div></div><div class="section" title="18. sqoop-eval"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_literal_sqoop_eval_literal"></a>18. <code class="literal">sqoop-eval</code></h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_purpose_11">18.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_11">18.2. Syntax</a></span></dt><dt><span class="section"><a href="#_example_invocations_8">18.3. Example Invocations</a></span></dt></dl></div><div class="section" title="18.1. Purpose"><div class="titlepage"><div><div><h3 class="title"><a name="_purpose_11"></a>18.1. Purpose</h3></div></div></div><p>The <code class="literal">eval</code> tool allows users to quickly run simple SQL queries against
a database; results are printed to the console. This allows users to
preview their import queries to ensure they import the data they
expect.</p><div class="warning" title="Warning" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Warning"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Warning]" src="images/warning.png"></td><th align="left">Warning</th></tr><tr><td align="left" valign="top"><p>The <code class="literal">eval</code> tool is provided for evaluation purpose only. You can use it to verify database connection from within the Sqoop or to test simple queries. It&#8217;s not suppose to be used in production workflows.</p></td></tr></table></div></div><div class="section" title="18.2. Syntax"><div class="titlepage"><div><div><h3 class="title"><a name="_syntax_11"></a>18.2. Syntax</h3></div></div></div><pre class="screen">$ sqoop eval (generic-args) (eval-args)
$ sqoop-eval (generic-args) (eval-args)</pre><p>Although the Hadoop generic arguments must preceed any eval arguments,
the eval arguments can be entered in any order with respect to one
another.</p><div class="table"><a name="idp4386264"></a><p class="title"><b>Table 46. Common arguments</b></p><div class="table-contents"><table summary="Common arguments" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connect &lt;jdbc-uri&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify JDBC connect string
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connection-manager &lt;class-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify connection manager class to                                          use
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--driver &lt;class-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Manually specify JDBC driver class                                          to use
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hadoop-mapred-home &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Override $HADOOP_MAPRED_HOME
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--help</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Print usage instructions
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--password-file</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set path for a file containing the                                          authentication password
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">-P</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Read password from console
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--password &lt;password&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set authentication password
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--username &lt;username&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set authentication username
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--verbose</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Print more information while working
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connection-param-file &lt;filename&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Optional properties file that                                          provides connection parameters
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--relaxed-isolation</code>
    </td><td style="" align="left">
    Set connection transaction isolation                                          to read uncommitted for the mappers.
    </td></tr></tbody></table></div></div><br class="table-break"><div class="table"><a name="idp4401744"></a><p class="title"><b>Table 47. SQL evaluation arguments:</b></p><div class="table-contents"><table summary="SQL evaluation arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">-e,--query &lt;statement&gt;</code>
    </td><td style="" align="left">
    Execute <span class="emphasis"><em><code class="literal">statement</code></em></span> in SQL.
    </td></tr></tbody></table></div></div><br class="table-break"></div><div class="section" title="18.3. Example Invocations"><div class="titlepage"><div><div><h3 class="title"><a name="_example_invocations_8"></a>18.3. Example Invocations</h3></div></div></div><p>Select ten records from the <code class="literal">employees</code> table:</p><pre class="screen">$ sqoop eval --connect jdbc:mysql://db.example.com/corp \
    --query "SELECT * FROM employees LIMIT 10"</pre><p>Insert a row into the <code class="literal">foo</code> table:</p><pre class="screen">$ sqoop eval --connect jdbc:mysql://db.example.com/corp \
    -e "INSERT INTO foo VALUES(42, 'bar')"</pre></div></div><div class="section" title="19. sqoop-list-databases"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_literal_sqoop_list_databases_literal"></a>19. <code class="literal">sqoop-list-databases</code></h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_purpose_12">19.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_12">19.2. Syntax</a></span></dt><dt><span class="section"><a href="#_example_invocations_9">19.3. Example Invocations</a></span></dt></dl></div><div class="section" title="19.1. Purpose"><div class="titlepage"><div><div><h3 class="title"><a name="_purpose_12"></a>19.1. Purpose</h3></div></div></div><p>List database schemas present on a server.</p></div><div class="section" title="19.2. Syntax"><div class="titlepage"><div><div><h3 class="title"><a name="_syntax_12"></a>19.2. Syntax</h3></div></div></div><pre class="screen">$ sqoop list-databases (generic-args) (list-databases-args)
$ sqoop-list-databases (generic-args) (list-databases-args)</pre><p>Although the Hadoop generic arguments must preceed any list-databases
arguments, the list-databases arguments can be entered in any order
with respect to one another.</p><div class="table"><a name="idp4411640"></a><p class="title"><b>Table 48. Common arguments</b></p><div class="table-contents"><table summary="Common arguments" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connect &lt;jdbc-uri&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify JDBC connect string
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connection-manager &lt;class-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify connection manager class to                                          use
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--driver &lt;class-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Manually specify JDBC driver class                                          to use
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hadoop-mapred-home &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Override $HADOOP_MAPRED_HOME
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--help</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Print usage instructions
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--password-file</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set path for a file containing the                                          authentication password
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">-P</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Read password from console
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--password &lt;password&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set authentication password
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--username &lt;username&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set authentication username
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--verbose</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Print more information while working
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connection-param-file &lt;filename&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Optional properties file that                                          provides connection parameters
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--relaxed-isolation</code>
    </td><td style="" align="left">
    Set connection transaction isolation                                          to read uncommitted for the mappers.
    </td></tr></tbody></table></div></div><br class="table-break"></div><div class="section" title="19.3. Example Invocations"><div class="titlepage"><div><div><h3 class="title"><a name="_example_invocations_9"></a>19.3. Example Invocations</h3></div></div></div><p>List database schemas available on a MySQL server:</p><pre class="screen">$ sqoop list-databases --connect jdbc:mysql://database.example.com/
information_schema
employees</pre><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>This only works with HSQLDB, MySQL and Oracle. When using with Oracle,
it is necessary that the user connecting to the database has DBA privileges.</p></td></tr></table></div></div></div><div class="section" title="20. sqoop-list-tables"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_literal_sqoop_list_tables_literal"></a>20. <code class="literal">sqoop-list-tables</code></h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_purpose_13">20.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_13">20.2. Syntax</a></span></dt><dt><span class="section"><a href="#_example_invocations_10">20.3. Example Invocations</a></span></dt></dl></div><div class="section" title="20.1. Purpose"><div class="titlepage"><div><div><h3 class="title"><a name="_purpose_13"></a>20.1. Purpose</h3></div></div></div><p>List tables present in a database.</p></div><div class="section" title="20.2. Syntax"><div class="titlepage"><div><div><h3 class="title"><a name="_syntax_13"></a>20.2. Syntax</h3></div></div></div><pre class="screen">$ sqoop list-tables (generic-args) (list-tables-args)
$ sqoop-list-tables (generic-args) (list-tables-args)</pre><p>Although the Hadoop generic arguments must preceed any list-tables
arguments, the list-tables arguments can be entered in any order
with respect to one another.</p><div class="table"><a name="idp4431936"></a><p class="title"><b>Table 49. Common arguments</b></p><div class="table-contents"><table summary="Common arguments" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connect &lt;jdbc-uri&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify JDBC connect string
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connection-manager &lt;class-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify connection manager class to                                          use
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--driver &lt;class-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Manually specify JDBC driver class                                          to use
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--hadoop-mapred-home &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Override $HADOOP_MAPRED_HOME
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--help</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Print usage instructions
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--password-file</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set path for a file containing the                                          authentication password
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">-P</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Read password from console
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--password &lt;password&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set authentication password
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--username &lt;username&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set authentication username
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--verbose</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Print more information while working
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--connection-param-file &lt;filename&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Optional properties file that                                          provides connection parameters
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--relaxed-isolation</code>
    </td><td style="" align="left">
    Set connection transaction isolation                                          to read uncommitted for the mappers.
    </td></tr></tbody></table></div></div><br class="table-break"></div><div class="section" title="20.3. Example Invocations"><div class="titlepage"><div><div><h3 class="title"><a name="_example_invocations_10"></a>20.3. Example Invocations</h3></div></div></div><p>List tables available in the "corp" database:</p><pre class="screen">$ sqoop list-tables --connect jdbc:mysql://database.example.com/corp
employees
payroll_checks
job_descriptions
office_supplies</pre><p>In case of postgresql, list tables command with common arguments fetches only "public" schema. For custom schema, use --schema argument to list tables of particular schema
Example</p><pre class="screen">$ sqoop list-tables --connect jdbc:postgresql://localhost/corp  --username name -P -- --schema payrolldept
employees
expenses</pre></div></div><div class="section" title="21. sqoop-help"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_literal_sqoop_help_literal"></a>21. <code class="literal">sqoop-help</code></h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_purpose_14">21.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_14">21.2. Syntax</a></span></dt><dt><span class="section"><a href="#_example_invocations_11">21.3. Example Invocations</a></span></dt></dl></div><div class="section" title="21.1. Purpose"><div class="titlepage"><div><div><h3 class="title"><a name="_purpose_14"></a>21.1. Purpose</h3></div></div></div><p>List tools available in Sqoop and explain their usage.</p></div><div class="section" title="21.2. Syntax"><div class="titlepage"><div><div><h3 class="title"><a name="_syntax_14"></a>21.2. Syntax</h3></div></div></div><pre class="screen">$ sqoop help [tool-name]
$ sqoop-help [tool-name]</pre><p>If no tool name is provided (for example, the user runs <code class="literal">sqoop help</code>), then
the available tools are listed. With a tool name, the usage
instructions for that specific tool are presented on the console.</p></div><div class="section" title="21.3. Example Invocations"><div class="titlepage"><div><div><h3 class="title"><a name="_example_invocations_11"></a>21.3. Example Invocations</h3></div></div></div><p>List available tools:</p><pre class="screen">$ sqoop help
usage: sqoop COMMAND [ARGS]

Available commands:
  codegen            Generate code to interact with database records
  create-hive-table  Import a table definition into Hive
  eval               Evaluate a SQL statement and display the results
  export             Export an HDFS directory to a database table

...

See 'sqoop help COMMAND' for information on a specific command.</pre><p>Display usage instructions for the <code class="literal">import</code> tool:</p><pre class="screen">$ bin/sqoop help import
usage: sqoop import [GENERIC-ARGS] [TOOL-ARGS]

Common arguments:
   --connect &lt;jdbc-uri&gt;     Specify JDBC connect string
   --connection-manager &lt;class-name&gt;     Specify connection manager class to use
   --driver &lt;class-name&gt;    Manually specify JDBC driver class to use
   --hadoop-mapred-home &lt;dir&gt;            Override $HADOOP_MAPRED_HOME
   --help                   Print usage instructions
   --password-file          Set path for file containing authentication password
   -P                       Read password from console
   --password &lt;password&gt;    Set authentication password
   --username &lt;username&gt;    Set authentication username
   --verbose                Print more information while working
   --hadoop-home &lt;dir&gt;      Deprecated. Override $HADOOP_HOME

Import control arguments:
   --as-avrodatafile             Imports data to Avro Data Files
   --as-sequencefile             Imports data to SequenceFiles
   --as-textfile                 Imports data as plain text (default)
   --as-parquetfile              Imports data to Parquet Data Files
...</pre></div></div><div class="section" title="22. sqoop-version"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_literal_sqoop_version_literal"></a>22. <code class="literal">sqoop-version</code></h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_purpose_15">22.1. Purpose</a></span></dt><dt><span class="section"><a href="#_syntax_15">22.2. Syntax</a></span></dt><dt><span class="section"><a href="#_example_invocations_12">22.3. Example Invocations</a></span></dt></dl></div><div class="section" title="22.1. Purpose"><div class="titlepage"><div><div><h3 class="title"><a name="_purpose_15"></a>22.1. Purpose</h3></div></div></div><p>Display version information for Sqoop.</p></div><div class="section" title="22.2. Syntax"><div class="titlepage"><div><div><h3 class="title"><a name="_syntax_15"></a>22.2. Syntax</h3></div></div></div><pre class="screen">$ sqoop version
$ sqoop-version</pre></div><div class="section" title="22.3. Example Invocations"><div class="titlepage"><div><div><h3 class="title"><a name="_example_invocations_12"></a>22.3. Example Invocations</h3></div></div></div><p>Display the version:</p><pre class="screen">$ sqoop version
Sqoop {revnumber}
git commit id 46b3e06b79a8411320d77c984c3030db47dd1c22
Compiled by aaron@jargon on Mon May 17 13:43:22 PDT 2010</pre></div></div><div class="section" title="23. Sqoop-HCatalog Integration"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_sqoop_hcatalog_integration"></a>23. Sqoop-HCatalog Integration</h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_hcatalog_background">23.1. HCatalog Background</a></span></dt><dt><span class="section"><a href="#_exposing_hcatalog_tables_to_sqoop">23.2. Exposing HCatalog Tables to Sqoop</a></span></dt><dd><dl><dt><span class="section"><a href="#_new_command_line_options">23.2.1. New Command Line Options</a></span></dt><dt><span class="section"><a href="#_supported_sqoop_hive_options">23.2.2. Supported Sqoop Hive Options</a></span></dt><dt><span class="section"><a href="#_direct_mode_support">23.2.3. Direct Mode support</a></span></dt><dt><span class="section"><a href="#_unsupported_sqoop_options">23.2.4. Unsupported Sqoop Options</a></span></dt><dd><dl><dt><span class="section"><a href="#_unsupported_sqoop_hive_import_options">23.2.4.1. Unsupported Sqoop Hive Import Options</a></span></dt><dt><span class="section"><a href="#_unsupported_sqoop_export_and_import_options">23.2.4.2. Unsupported Sqoop Export and Import Options</a></span></dt></dl></dd><dt><span class="section"><a href="#_ignored_sqoop_options">23.2.5. Ignored Sqoop Options</a></span></dt></dl></dd><dt><span class="section"><a href="#_automatic_table_creation">23.3. Automatic Table Creation</a></span></dt><dt><span class="section"><a href="#_delimited_text_formats_and_field_and_line_delimiter_characters">23.4. Delimited Text Formats and Field and Line Delimiter Characters</a></span></dt><dt><span class="section"><a href="#_hcatalog_table_requirements">23.5. HCatalog Table Requirements</a></span></dt><dt><span class="section"><a href="#_support_for_partitioning">23.6. Support for Partitioning</a></span></dt><dt><span class="section"><a href="#_schema_mapping">23.7. Schema Mapping</a></span></dt><dt><span class="section"><a href="#_support_for_hcatalog_data_types">23.8. Support for HCatalog Data Types</a></span></dt><dt><span class="section"><a href="#_providing_hive_and_hcatalog_libraries_for_the_sqoop_job">23.9. Providing Hive and HCatalog Libraries for the Sqoop Job</a></span></dt><dt><span class="section"><a href="#_examples">23.10. Examples</a></span></dt><dt><span class="section"><a href="#_import">23.11. Import</a></span></dt><dt><span class="section"><a href="#_export">23.12. Export</a></span></dt></dl></div><div class="section" title="23.1. HCatalog Background"><div class="titlepage"><div><div><h3 class="title"><a name="_hcatalog_background"></a>23.1. HCatalog Background</h3></div></div></div><p>HCatalog is a table and storage management service for Hadoop that enables
users with different data processing tools Pig, MapReduce, and Hive
to more easily read and write data on the grid. HCatalog&#8217;s table abstraction
presents users with a relational view of data in the Hadoop distributed
file system (HDFS) and ensures that users need not worry about where or
in what format their data is stored: RCFile format, text files, or
SequenceFiles.</p><p>HCatalog supports reading and writing files in any format for which a Hive
SerDe (serializer-deserializer) has been written. By default, HCatalog
supports RCFile, CSV, JSON, and SequenceFile formats. To use a custom
format, you must provide the InputFormat and OutputFormat as well as the SerDe.</p><p>The ability of HCatalog to abstract various storage formats is used in
providing the RCFile (and future file types) support to Sqoop.</p></div><div class="section" title="23.2. Exposing HCatalog Tables to Sqoop"><div class="titlepage"><div><div><h3 class="title"><a name="_exposing_hcatalog_tables_to_sqoop"></a>23.2. Exposing HCatalog Tables to Sqoop</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_new_command_line_options">23.2.1. New Command Line Options</a></span></dt><dt><span class="section"><a href="#_supported_sqoop_hive_options">23.2.2. Supported Sqoop Hive Options</a></span></dt><dt><span class="section"><a href="#_direct_mode_support">23.2.3. Direct Mode support</a></span></dt><dt><span class="section"><a href="#_unsupported_sqoop_options">23.2.4. Unsupported Sqoop Options</a></span></dt><dd><dl><dt><span class="section"><a href="#_unsupported_sqoop_hive_import_options">23.2.4.1. Unsupported Sqoop Hive Import Options</a></span></dt><dt><span class="section"><a href="#_unsupported_sqoop_export_and_import_options">23.2.4.2. Unsupported Sqoop Export and Import Options</a></span></dt></dl></dd><dt><span class="section"><a href="#_ignored_sqoop_options">23.2.5. Ignored Sqoop Options</a></span></dt></dl></div><p>HCatalog integration with Sqoop is patterned on an existing feature set that
supports Avro and Hive tables. Seven new command line options are introduced,
and some command line options defined for Hive have been reused.</p><div class="section" title="23.2.1. New Command Line Options"><div class="titlepage"><div><div><h4 class="title"><a name="_new_command_line_options"></a>23.2.1. New Command Line Options</h4></div></div></div><div class="variablelist"><dl><dt><span class="term">
<code class="literal">--hcatalog-database</code>
</span></dt><dd>
Specifies the database name for the HCatalog table. If not specified,
the default database name <code class="literal">default</code> is used. Providing the
<code class="literal">--hcatalog-database</code> option without <code class="literal">--hcatalog-table</code> is an error.
This is not a required option.
</dd><dt><span class="term">
<code class="literal">--hcatalog-table</code>
</span></dt><dd>
The argument value for this option is the HCatalog tablename.
The presence of the <code class="literal">--hcatalog-table</code> option signifies that the import
or export job is done using HCatalog tables, and it is a required option for
HCatalog jobs.
</dd><dt><span class="term">
<code class="literal">--hcatalog-home</code>
</span></dt><dd>
The home directory for the HCatalog installation. The directory is
expected to have a <code class="literal">lib</code> subdirectory and a <code class="literal">share/hcatalog</code> subdirectory
with necessary HCatalog libraries. If not specified, the system property
<code class="literal">hcatalog.home</code> will be checked and failing that, a system environment
variable <code class="literal">HCAT_HOME</code> will be checked.  If none of these are set, the
default value will be used and currently the default is set to
<code class="literal">/usr/lib/hcatalog</code>.
This is not a required option.
</dd><dt><span class="term">
<code class="literal">--create-hcatalog-table</code>
</span></dt><dd>
This option specifies whether an HCatalog table should be created
automatically when importing data. By default, HCatalog tables are assumed
to exist. The table name will be the same as the database table name
translated to lower case. Further described in <code class="literal">Automatic Table Creation</code>
below.
</dd><dt><span class="term">
<code class="literal">--hcatalog-storage-stanza</code>
</span></dt><dd>
This option specifies the storage stanza to be appended to the table.
Further described in <code class="literal">Automatic Table Creation</code> below.
</dd><dt><span class="term">
<code class="literal">--hcatalog-partition-keys</code> and <code class="literal">--hcatalog-partition-values</code>
</span></dt><dd>
These two options are used to specify multiple static partition key/value
pairs.  In the prior releases, <code class="literal">--hive-partition-key</code> and
<code class="literal">--hive-partition-value</code> options were used to specify the static partition
key/value pair, but only one level of static partition keys could be provided.
The options <code class="literal">--hcatalog-partition-keys</code> and <code class="literal">--hcatalog-partition-values</code>
allow multiple keys and values to be provided as static partitioning keys.
Multiple option values are to be separated by <span class="emphasis"><em>,</em></span> (comma).
</dd></dl></div><p>For example, if the hive partition keys for the table to export/import from are
defined with partition key names year, month and date and a specific partition
with year=1999, month=12, day=31 is the desired partition, then the values
for the two options will be as follows:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<code class="literal">--hcatalog-partition-keys</code> year,month,day
</li><li class="listitem">
<code class="literal">--hcatalog-partition-values</code> 1999,12,31
</li></ul></div><p>To provide backward compatibility, if <code class="literal">--hcatalog-partition-keys</code> or
<code class="literal">--hcatalog-partition-values</code> options are not provided, then
<code class="literal">--hive-partitition-key</code> and <code class="literal">--hive-partition-value</code> will be used if provided.</p><p>It is an error to specify only one of <code class="literal">--hcatalog-partition-keys</code> or
<code class="literal">--hcatalog-partition-values</code> options. Either both of the options should be
provided or neither of the options should be provided.</p></div><div class="section" title="23.2.2. Supported Sqoop Hive Options"><div class="titlepage"><div><div><h4 class="title"><a name="_supported_sqoop_hive_options"></a>23.2.2. Supported Sqoop Hive Options</h4></div></div></div><p>The following Sqoop options are also used along with the <code class="literal">--hcatalog-table</code>
option to provide additional input to the HCatalog jobs. Some of the existing
Hive import job options are reused with HCatalog jobs instead of creating
HCatalog-specific options for the same purpose.</p><div class="variablelist"><dl><dt><span class="term">
<code class="literal">--map-column-hive</code>
</span></dt><dd>
This option maps a database column to HCatalog with a specific HCatalog
type.
</dd><dt><span class="term">
<code class="literal">--hive-home</code>
</span></dt><dd>
The Hive home location.
</dd><dt><span class="term">
<code class="literal">--hive-partition-key</code>
</span></dt><dd>
Used for static partitioning filter. The partitioning key should be of
type STRING. There can be only one static partitioning key.
Please see the discussion about <code class="literal">--hcatalog-partition-keys</code> and
<code class="literal">--hcatalog-partition-values</code> options.
</dd><dt><span class="term">
<code class="literal">--hive-partition-value</code>
</span></dt><dd>
The value associated with the partition.
Please see the discussion about <code class="literal">--hcatalog-partition-keys</code> and
<code class="literal">--hcatalog-partition-values</code> options.
</dd></dl></div></div><div class="section" title="23.2.3. Direct Mode support"><div class="titlepage"><div><div><h4 class="title"><a name="_direct_mode_support"></a>23.2.3. Direct Mode support</h4></div></div></div><p>HCatalog integration in Sqoop has been enhanced to support direct mode
connectors (which are high performance connectors specific to a database).
Netezza direct mode connector has been enhanced to take advatange of this
feature.</p><div class="important" title="Important" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Important"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Important]" src="images/important.png"></td><th align="left">Important</th></tr><tr><td align="left" valign="top"><p>Only Netezza direct mode connector is currently enabled to work
with HCatalog.</p></td></tr></table></div></div><div class="section" title="23.2.4. Unsupported Sqoop Options"><div class="titlepage"><div><div><h4 class="title"><a name="_unsupported_sqoop_options"></a>23.2.4. Unsupported Sqoop Options</h4></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_unsupported_sqoop_hive_import_options">23.2.4.1. Unsupported Sqoop Hive Import Options</a></span></dt><dt><span class="section"><a href="#_unsupported_sqoop_export_and_import_options">23.2.4.2. Unsupported Sqoop Export and Import Options</a></span></dt></dl></div><div class="section" title="23.2.4.1. Unsupported Sqoop Hive Import Options"><div class="titlepage"><div><div><h5 class="title"><a name="_unsupported_sqoop_hive_import_options"></a>23.2.4.1. Unsupported Sqoop Hive Import Options</h5></div></div></div><p>The following Sqoop Hive import options are not supported with HCatalog jobs.</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<code class="literal">--hive-import</code>
</li><li class="listitem">
<code class="literal">--hive-overwrite</code>
</li></ul></div></div><div class="section" title="23.2.4.2. Unsupported Sqoop Export and Import Options"><div class="titlepage"><div><div><h5 class="title"><a name="_unsupported_sqoop_export_and_import_options"></a>23.2.4.2. Unsupported Sqoop Export and Import Options</h5></div></div></div><p>The following Sqoop export and import options are not supported with HCatalog jobs.</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<code class="literal">--export-dir</code>
</li><li class="listitem">
<code class="literal">--target-dir</code>
</li><li class="listitem">
<code class="literal">--warehouse-dir</code>
</li><li class="listitem">
<code class="literal">--append</code>
</li><li class="listitem">
<code class="literal">--as-sequencefile</code>
</li><li class="listitem">
<code class="literal">--as-avrodatafile</code>
</li><li class="listitem">
<code class="literal">--as-parquetfile</code>
</li></ul></div></div></div><div class="section" title="23.2.5. Ignored Sqoop Options"><div class="titlepage"><div><div><h4 class="title"><a name="_ignored_sqoop_options"></a>23.2.5. Ignored Sqoop Options</h4></div></div></div><p>The following options are ignored with HCatalog jobs.</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
All input delimiter options are ignored.
</li><li class="listitem">
Output delimiters are generally ignored unless either
<code class="literal">--hive-drop-import-delims</code> or <code class="literal">--hive-delims-replacement</code> is used. When the
<code class="literal">--hive-drop-import-delims</code> or <code class="literal">--hive-delims-replacement</code> option is
specified, all <code class="literal">CHAR</code> type database table columns will be post-processed
to either remove or replace the delimiters, respectively. See <code class="literal">Delimited Text
Formats and Field and Line Delimiter Characters</code> below. This is only needed
if the HCatalog table uses text formats.
</li></ul></div></div></div><div class="section" title="23.3. Automatic Table Creation"><div class="titlepage"><div><div><h3 class="title"><a name="_automatic_table_creation"></a>23.3. Automatic Table Creation</h3></div></div></div><p>One of the key features of Sqoop is to manage and create the table metadata
when importing into Hadoop. HCatalog import jobs also provide for this
feature with the option <code class="literal">--create-hcatalog-table</code>. Furthermore, one of the
important benefits of the HCatalog integration is to provide storage
agnosticism to Sqoop data movement jobs. To provide for that feature,
HCatalog import jobs provide an option that lets a user specifiy the
storage format for the created table.</p><p>The option <code class="literal">--create-hcatalog-table</code> is used as an indicator that a table
has to be created as part of the HCatalog import job.  If the option
<code class="literal">--create-hcatalog-table</code> is specified and the table exists, then the
table creation will fail and the job will be aborted.</p><p>The option <code class="literal">--hcatalog-storage-stanza</code> can be used to specify the storage
format of the newly created table. The default value for this option is
<code class="literal">stored as rcfile</code>. The value specified for this option is assumed to be a
valid Hive storage format expression. It will be appended to the <code class="literal">create table</code>
command generated by the HCatalog import job as part of automatic table
creation. Any error in the storage stanza will cause the table creation to
fail and the import job will be aborted.</p><p>Any additional resources needed to support the storage format referenced in
the option <code class="literal">--hcatalog-storage-stanza</code> should be provided to the job either
by placing them in <code class="literal">$HIVE_HOME/lib</code> or by providing them in <code class="literal">HADOOP_CLASSPATH</code>
and <code class="literal">LIBJAR</code> files.</p><p>If the option <code class="literal">--hive-partition-key</code> is specified, then the value of this
option is used as the partitioning key for the newly created table. Only
one partitioning key can be specified with this option.</p><p>Object names are mapped to the lowercase equivalents as specified below
when mapped to an HCatalog table. This includes the table name (which
is the same as the external store table name converted to lower case)
and field names.</p></div><div class="section" title="23.4. Delimited Text Formats and Field and Line Delimiter Characters"><div class="titlepage"><div><div><h3 class="title"><a name="_delimited_text_formats_and_field_and_line_delimiter_characters"></a>23.4. Delimited Text Formats and Field and Line Delimiter Characters</h3></div></div></div><p>HCatalog supports delimited text format as one of the table storage formats.
But when delimited text is used and the imported data has fields that contain
those delimiters, then the data may be parsed into a different number of
fields and records by Hive, thereby losing data fidelity.</p><p>For this case, one of these existing Sqoop import options can be used:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<code class="literal">--hive-delims-replacement</code>
</li><li class="listitem">
<code class="literal">--hive-drop-import-delims</code>
</li></ul></div><p>If either of these options is provided for import, then any column of type
STRING will be formatted with the Hive delimiter processing and then written
to the HCatalog table.</p></div><div class="section" title="23.5. HCatalog Table Requirements"><div class="titlepage"><div><div><h3 class="title"><a name="_hcatalog_table_requirements"></a>23.5. HCatalog Table Requirements</h3></div></div></div><p>The HCatalog table should be created before using it as part of a Sqoop job
if the default table creation options (with optional storage stanza) are not
sufficient. All storage formats supported by HCatalog can be used with the
creation of the HCatalog tables. This makes this feature readily adopt new
storage formats that come into the Hive project, such as ORC files.</p></div><div class="section" title="23.6. Support for Partitioning"><div class="titlepage"><div><div><h3 class="title"><a name="_support_for_partitioning"></a>23.6. Support for Partitioning</h3></div></div></div><p>The Sqoop HCatalog feature supports the following table types:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Unpartitioned tables
</li><li class="listitem">
Partitioned tables with a static partitioning key specified
</li><li class="listitem">
Partitioned tables with dynamic partition keys from the database
result set
</li><li class="listitem">
Partitioned tables with a combination of a static key and additional
dynamic partitioning keys
</li></ul></div></div><div class="section" title="23.7. Schema Mapping"><div class="titlepage"><div><div><h3 class="title"><a name="_schema_mapping"></a>23.7. Schema Mapping</h3></div></div></div><p>Sqoop currently does not support column name mapping. However, the user
is allowed to override the type mapping. Type mapping loosely follows
the Hive type mapping already present in Sqoop except that SQL types
FLOAT and REAL are mapped to HCatalog type float. In the Sqoop type
mapping for Hive, these two are mapped to double. Type mapping is primarily
used for checking the column definition correctness only and can be overridden
with the --map-column-hive option.</p><p>All types except binary are assignable to a String type.</p><p>Any field of number type (int, shortint, tinyint, bigint and bigdecimal,
float and double) is assignable to another field of any number type during
exports and imports. Depending on the precision and scale of the target type
of assignment, truncations can occur.</p><p>Furthermore, date/time/timestamps are mapped to date/timestamp hive types.
(the full date/time/timestamp representation).   Date/time/timstamp columns
can also be mapped to bigint Hive type in which case the value will be
the number of milliseconds since epoch.</p><p>BLOBs and CLOBs are only supported for imports. The BLOB/CLOB objects when
imported are stored in a Sqoop-specific format and knowledge of this format
is needed for processing these objects in a Pig/Hive job or another Map Reduce
job.</p><p>Database column names are mapped to their lowercase equivalents when mapped
to the HCatalog fields. Currently, case-sensitive database object names are
not supported.</p><p>Projection of a set of columns from a table to an HCatalog table or loading
to a column projection is allowed, subject to table constraints. The dynamic
partitioning columns, if any, must be part of the projection when importing
data into HCatalog tables.</p><p>Dynamic partitioning fields should be mapped to database columns that are
defined with the NOT NULL attribute (although this is not enforced during
schema mapping). A null value during import for a dynamic partitioning
column will abort the Sqoop job.</p></div><div class="section" title="23.8. Support for HCatalog Data Types"><div class="titlepage"><div><div><h3 class="title"><a name="_support_for_hcatalog_data_types"></a>23.8. Support for HCatalog Data Types</h3></div></div></div><p>All the primitive Hive types that are part of Hive 0.13 version are supported.
Currently all the complex HCatalog types are not supported.</p><p>BLOB/CLOB database types are only supported for imports.</p></div><div class="section" title="23.9. Providing Hive and HCatalog Libraries for the Sqoop Job"><div class="titlepage"><div><div><h3 class="title"><a name="_providing_hive_and_hcatalog_libraries_for_the_sqoop_job"></a>23.9. Providing Hive and HCatalog Libraries for the Sqoop Job</h3></div></div></div><p>With the support for HCatalog added to Sqoop, any HCatalog job depends on a
set of jar files being available both on the Sqoop client host and where the
Map/Reduce tasks run. To run HCatalog jobs, the environment variable
<code class="literal">HADOOP_CLASSPATH</code> must be set up as shown below before launching the Sqoop
HCatalog jobs.</p><p><code class="literal">HADOOP_CLASSPATH=$(hcat -classpath)</code>
<code class="literal">export HADOOP_CLASSPATH</code></p><p>The necessary HCatalog dependencies will be copied to the distributed cache
automatically by the Sqoop job.</p></div><div class="section" title="23.10. Examples"><div class="titlepage"><div><div><h3 class="title"><a name="_examples"></a>23.10. Examples</h3></div></div></div><p>Create an HCatalog table, such as:</p><p><code class="literal">hcat -e "create table txn(txn_date string, cust_id string, amount float,
store_id int) partitioned by (cust_id string) stored as rcfile;"</code></p><p>Then Sqoop import and export of the "txn" HCatalog table can be invoked as
follows:</p></div><div class="section" title="23.11. Import"><div class="titlepage"><div><div><h3 class="title"><a name="_import"></a>23.11. Import</h3></div></div></div><p><code class="literal">$SQOOP_HOME/bin/sqoop import --connect &lt;jdbc-url&gt; -table &lt;table-name&gt; --hcatalog-table txn &lt;other sqoop options&gt;</code></p></div><div class="section" title="23.12. Export"><div class="titlepage"><div><div><h3 class="title"><a name="_export"></a>23.12. Export</h3></div></div></div><p><code class="literal">$SQOOP_HOME/bin/sqoop export --connect &lt;jdbc-url&gt; -table &lt;table-name&gt; --hcatalog-table txn &lt;other sqoop options&gt;</code></p></div></div><div class="section" title="24. Compatibility Notes"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_compatibility_notes"></a>24. Compatibility Notes</h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_supported_databases">24.1. Supported Databases</a></span></dt><dt><span class="section"><a href="#_mysql">24.2. MySQL</a></span></dt><dd><dl><dt><span class="section"><a href="#_zerodatetimebehavior">24.2.1. zeroDateTimeBehavior</a></span></dt><dt><span class="section"><a href="#_literal_unsigned_literal_columns">24.2.2. <code class="literal">UNSIGNED</code> columns</a></span></dt><dt><span class="section"><a href="#_literal_blob_literal_and_literal_clob_literal_columns">24.2.3. <code class="literal">BLOB</code> and <code class="literal">CLOB</code> columns</a></span></dt><dt><span class="section"><a href="#_importing_views_in_direct_mode">24.2.4. Importing views in direct mode</a></span></dt></dl></dd><dt><span class="section"><a href="#_postgresql">24.3. PostgreSQL</a></span></dt><dd><dl><dt><span class="section"><a href="#_importing_views_in_direct_mode_2">24.3.1. Importing views in direct mode</a></span></dt></dl></dd><dt><span class="section"><a href="#_oracle">24.4. Oracle</a></span></dt><dd><dl><dt><span class="section"><a href="#_dates_and_times">24.4.1. Dates and Times</a></span></dt></dl></dd><dt><span class="section"><a href="#_schema_definition_in_hive">24.5. Schema Definition in Hive</a></span></dt><dt><span class="section"><a href="#_cubrid">24.6. CUBRID</a></span></dt></dl></div><p>Sqoop uses JDBC to connect to databases and adheres to
published standards as much as possible. For databases which do not
support standards-compliant SQL, Sqoop uses alternate codepaths to
provide functionality. In general, Sqoop is believed to be compatible
with a large number of databases, but it is tested with only a few.</p><p>Nonetheless, several database-specific decisions were made in the
implementation of Sqoop, and some databases offer additional settings
which are extensions to the standard.</p><p>This section describes the databases tested with Sqoop, any
exceptions in Sqoop&#8217;s handling of each database relative to the
norm, and any database-specific settings available in Sqoop.</p><div class="section" title="24.1. Supported Databases"><div class="titlepage"><div><div><h3 class="title"><a name="_supported_databases"></a>24.1. Supported Databases</h3></div></div></div><p>While JDBC is a compatibility layer that allows a program to access
many different databases through a common API, slight differences in
the SQL language spoken by each database may mean that Sqoop can&#8217;t use
every database out of the box, or that some databases may be used in
an inefficient manner.</p><p>When you provide a connect string to Sqoop, it inspects the protocol scheme to
determine appropriate vendor-specific logic to use. If Sqoop knows about
a given database, it will work automatically. If not, you may need to
specify the driver class to load via <code class="literal">--driver</code>. This will use a generic
code path which will use standard SQL to access the database. Sqoop provides
some databases with faster, non-JDBC-based access mechanisms. These can be
enabled by specfying the <code class="literal">--direct</code> parameter.</p><p>Sqoop includes vendor-specific support for the following databases:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Database
    </th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    version
    </th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--direct</code> support?
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    connect string matches
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    HSQLDB
    </td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    1.8.0+
    </td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    No
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">jdbc:hsqldb:*//</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    MySQL
    </td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    5.0+
    </td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Yes
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">jdbc:mysql://</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Oracle
    </td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    10.2.0+
    </td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    No
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">jdbc:oracle:*//</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    PostgreSQL
    </td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    8.3+
    </td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Yes (import only)
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">jdbc:postgresql://</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    CUBRID
    </td><td style="border-right: 0.5pt solid ; " align="left">
    9.2+
    </td><td style="border-right: 0.5pt solid ; " align="left">
    NO
    </td><td style="" align="left">
    <code class="literal">jdbc:cubrid:*</code>
    </td></tr></tbody></table></div><p>Sqoop may work with older versions of the databases listed, but we have
only tested it with the versions specified above.</p><p>Even if Sqoop supports a database internally, you may still need to
install the database vendor&#8217;s JDBC driver in your <code class="literal">$SQOOP_HOME/lib</code>
path on your client. Sqoop can load classes from any jars in
<code class="literal">$SQOOP_HOME/lib</code> on the client and will use them as part of any
MapReduce jobs it runs; unlike older versions, you no longer need to
install JDBC jars in the Hadoop library path on your servers.</p></div><div class="section" title="24.2. MySQL"><div class="titlepage"><div><div><h3 class="title"><a name="_mysql"></a>24.2. MySQL</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_zerodatetimebehavior">24.2.1. zeroDateTimeBehavior</a></span></dt><dt><span class="section"><a href="#_literal_unsigned_literal_columns">24.2.2. <code class="literal">UNSIGNED</code> columns</a></span></dt><dt><span class="section"><a href="#_literal_blob_literal_and_literal_clob_literal_columns">24.2.3. <code class="literal">BLOB</code> and <code class="literal">CLOB</code> columns</a></span></dt><dt><span class="section"><a href="#_importing_views_in_direct_mode">24.2.4. Importing views in direct mode</a></span></dt></dl></div><p>JDBC Driver: <a class="ulink" href="http://www.mysql.com/downloads/connector/j/" target="_top">MySQL
Connector/J</a></p><p>MySQL v5.0 and above offers very thorough coverage by Sqoop. Sqoop
has been tested with <code class="literal">mysql-connector-java-5.1.13-bin.jar</code>.</p><div class="section" title="24.2.1. zeroDateTimeBehavior"><div class="titlepage"><div><div><h4 class="title"><a name="_zerodatetimebehavior"></a>24.2.1. zeroDateTimeBehavior</h4></div></div></div><p>MySQL allows values of <code class="literal">'0000-00-00\'</code> for <code class="literal">DATE</code> columns, which is a
non-standard extension to SQL. When communicated via JDBC, these
values are handled in one of three different ways:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Convert to <code class="literal">NULL</code>.
</li><li class="listitem">
Throw an exception in the client.
</li><li class="listitem">
Round to the nearest legal date (<code class="literal">'0001-01-01\'</code>).
</li></ul></div><p>You specify the behavior by using the <code class="literal">zeroDateTimeBehavior</code>
property of the connect string. If a <code class="literal">zeroDateTimeBehavior</code> property
is not specified, Sqoop uses the <code class="literal">convertToNull</code> behavior.</p><p>You can override this behavior. For example:</p><pre class="screen">$ sqoop import --table foo \
    --connect jdbc:mysql://db.example.com/someDb?zeroDateTimeBehavior=round</pre></div><div class="section" title="24.2.2. UNSIGNED columns"><div class="titlepage"><div><div><h4 class="title"><a name="_literal_unsigned_literal_columns"></a>24.2.2. <code class="literal">UNSIGNED</code> columns</h4></div></div></div><p>Columns with type <code class="literal">UNSIGNED</code> in MySQL can hold values between 0 and
2^32 (<code class="literal">4294967295</code>), but the database will report the data type to Sqoop
as <code class="literal">INTEGER</code>, which will can hold values between <code class="literal">-2147483648</code> and
<code class="literal">\+2147483647</code>. Sqoop cannot currently import <code class="literal">UNSIGNED</code> values above
<code class="literal">2147483647</code>.</p></div><div class="section" title="24.2.3. BLOB and CLOB columns"><div class="titlepage"><div><div><h4 class="title"><a name="_literal_blob_literal_and_literal_clob_literal_columns"></a>24.2.3. <code class="literal">BLOB</code> and <code class="literal">CLOB</code> columns</h4></div></div></div><p>Sqoop&#8217;s direct mode does not support imports of <code class="literal">BLOB</code>, <code class="literal">CLOB</code>, or
<code class="literal">LONGVARBINARY</code> columns. Use JDBC-based imports for these
columns; do not supply the <code class="literal">--direct</code> argument to the import tool.</p></div><div class="section" title="24.2.4. Importing views in direct mode"><div class="titlepage"><div><div><h4 class="title"><a name="_importing_views_in_direct_mode"></a>24.2.4. Importing views in direct mode</h4></div></div></div><p>Sqoop is currently not supporting import from view in direct mode. Use
JDBC based (non direct) mode in case that you need to import view (simply
omit <code class="literal">--direct</code> parameter).</p></div></div><div class="section" title="24.3. PostgreSQL"><div class="titlepage"><div><div><h3 class="title"><a name="_postgresql"></a>24.3. PostgreSQL</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_importing_views_in_direct_mode_2">24.3.1. Importing views in direct mode</a></span></dt></dl></div><p>Sqoop supports JDBC-based connector for PostgreSQL: <a class="ulink" href="http://jdbc.postgresql.org/" target="_top">http://jdbc.postgresql.org/</a></p><p>The connector has been tested using JDBC driver version "9.1-903 JDBC 4" with
PostgreSQL server 9.1.</p><div class="section" title="24.3.1. Importing views in direct mode"><div class="titlepage"><div><div><h4 class="title"><a name="_importing_views_in_direct_mode_2"></a>24.3.1. Importing views in direct mode</h4></div></div></div><p>Sqoop is currently not supporting import from view in direct mode. Use
JDBC based (non direct) mode in case that you need to import view (simply
omit <code class="literal">--direct</code> parameter).</p></div></div><div class="section" title="24.4. Oracle"><div class="titlepage"><div><div><h3 class="title"><a name="_oracle"></a>24.4. Oracle</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_dates_and_times">24.4.1. Dates and Times</a></span></dt></dl></div><p>JDBC Driver:
<a class="ulink" href="http://www.oracle.com/technology/software/tech/java/sqlj_jdbc/htdocs/jdbc_112010.html" target="_top">Oracle
JDBC Thin Driver</a> - Sqoop is compatible with <code class="literal">ojdbc6.jar</code>.</p><p>Sqoop has been tested with Oracle 10.2.0 Express Edition. Oracle is
notable in its different approach to SQL from the ANSI standard, and
its non-standard JDBC driver. Therefore, several features work
differently.</p><div class="section" title="24.4.1. Dates and Times"><div class="titlepage"><div><div><h4 class="title"><a name="_dates_and_times"></a>24.4.1. Dates and Times</h4></div></div></div><p>Oracle JDBC represents <code class="literal">DATE</code> and <code class="literal">TIME</code> SQL types as <code class="literal">TIMESTAMP</code>
values. Any <code class="literal">DATE</code> columns in an Oracle database will be imported as a
<code class="literal">TIMESTAMP</code> in Sqoop, and Sqoop-generated code will store these values
in <code class="literal">java.sql.Timestamp</code> fields.</p><p>When exporting data back to a database, Sqoop parses text fields as
<code class="literal">TIMESTAMP</code> types (with the form <code class="literal">yyyy-mm-dd HH:MM:SS.ffffffff</code>) even
if you expect these fields to be formatted with the JDBC date escape
format of <code class="literal">yyyy-mm-dd</code>. Dates exported to Oracle should be formatted
as full timestamps.</p><p>Oracle also includes the additional date/time types <code class="literal">TIMESTAMP WITH
TIMEZONE</code> and <code class="literal">TIMESTAMP WITH LOCAL TIMEZONE</code>. To support these types,
the user&#8217;s session timezone must be specified. By default, Sqoop will
specify the timezone <code class="literal">"GMT"</code> to Oracle. You can override this setting
by specifying a Hadoop property <code class="literal">oracle.sessionTimeZone</code> on the
command-line when running a Sqoop job. For example:</p><pre class="screen">$ sqoop import -D oracle.sessionTimeZone=America/Los_Angeles \
    --connect jdbc:oracle:thin:@//db.example.com/foo --table bar</pre><p>Note that Hadoop parameters (<code class="literal">-D &#8230;</code>) are <span class="emphasis"><em>generic arguments</em></span> and
must appear before the tool-specific arguments (<code class="literal">--connect</code>,
<code class="literal">--table</code>, and so on).</p><p>Legal values for the session timezone string are enumerated at
<a class="ulink" href="http://download-west.oracle.com/docs/cd/B19306_01/server.102/b14225/applocaledata.htm#i637736" target="_top">http://download-west.oracle.com/docs/cd/B19306_01/server.102/b14225/applocaledata.htm#i637736</a>.</p></div></div><div class="section" title="24.5. Schema Definition in Hive"><div class="titlepage"><div><div><h3 class="title"><a name="_schema_definition_in_hive"></a>24.5. Schema Definition in Hive</h3></div></div></div><p>Hive users will note that there is not a one-to-one mapping between
SQL types and Hive types. In general, SQL types that do not have a
direct mapping (for example, <code class="literal">DATE</code>, <code class="literal">TIME</code>, and <code class="literal">TIMESTAMP</code>) will be coerced to
<code class="literal">STRING</code> in Hive. The <code class="literal">NUMERIC</code> and <code class="literal">DECIMAL</code> SQL types will be coerced to
<code class="literal">DOUBLE</code>. In these cases, Sqoop will emit a warning in its log messages
informing you of the loss of precision.</p></div><div class="section" title="24.6. CUBRID"><div class="titlepage"><div><div><h3 class="title"><a name="_cubrid"></a>24.6. CUBRID</h3></div></div></div><p>Sqoop supports JDBC-based connector for Cubrid: <a class="ulink" href="http://www.cubrid.org/?mid=downloads&amp;item=jdbc_driver" target="_top">http://www.cubrid.org/?mid=downloads&amp;item=jdbc_driver</a></p><p>The connector has been tested using JDBC driver version "JDBC-9.2.0.0155-cubrid.jar" with Cubrid 9.2.</p></div></div><div class="section" title="25. Notes for specific connectors"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="connectors"></a>25. Notes for specific connectors</h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_mysql_jdbc_connector">25.1. MySQL JDBC Connector</a></span></dt><dd><dl><dt><span class="section"><a href="#_upsert_functionality">25.1.1. Upsert functionality</a></span></dt></dl></dd><dt><span class="section"><a href="#_mysql_direct_connector">25.2. MySQL Direct Connector</a></span></dt><dd><dl><dt><span class="section"><a href="#_requirements">25.2.1. Requirements</a></span></dt><dt><span class="section"><a href="#_limitations_2">25.2.2. Limitations</a></span></dt><dt><span class="section"><a href="#_direct_mode_transactions">25.2.3. Direct-mode Transactions</a></span></dt></dl></dd><dt><span class="section"><a href="#_microsoft_sql_connector">25.3. Microsoft SQL Connector</a></span></dt><dd><dl><dt><span class="section"><a href="#_extra_arguments">25.3.1. Extra arguments</a></span></dt><dt><span class="section"><a href="#_allow_identity_inserts">25.3.2. Allow identity inserts</a></span></dt><dt><span class="section"><a href="#_non_resilient_operations">25.3.3. Non-resilient operations</a></span></dt><dt><span class="section"><a href="#_schema_support">25.3.4. Schema support</a></span></dt><dt><span class="section"><a href="#_table_hints">25.3.5. Table hints</a></span></dt></dl></dd><dt><span class="section"><a href="#_postgresql_connector">25.4. PostgreSQL Connector</a></span></dt><dd><dl><dt><span class="section"><a href="#_extra_arguments_2">25.4.1. Extra arguments</a></span></dt><dt><span class="section"><a href="#_schema_support_2">25.4.2. Schema support</a></span></dt></dl></dd><dt><span class="section"><a href="#_postgresql_direct_connector">25.5. PostgreSQL Direct Connector</a></span></dt><dd><dl><dt><span class="section"><a href="#_requirements_2">25.5.1. Requirements</a></span></dt><dt><span class="section"><a href="#_limitations_3">25.5.2. Limitations</a></span></dt></dl></dd><dt><span class="section"><a href="#_pg_bulkload_connector">25.6. pg_bulkload connector</a></span></dt><dd><dl><dt><span class="section"><a href="#_purpose_16">25.6.1. Purpose</a></span></dt><dt><span class="section"><a href="#_requirements_3">25.6.2. Requirements</a></span></dt><dt><span class="section"><a href="#_syntax_16">25.6.3. Syntax</a></span></dt><dt><span class="section"><a href="#_data_staging">25.6.4. Data Staging</a></span></dt></dl></dd><dt><span class="section"><a href="#_netezza_connector">25.7. Netezza Connector</a></span></dt><dd><dl><dt><span class="section"><a href="#_extra_arguments_3">25.7.1. Extra arguments</a></span></dt><dt><span class="section"><a href="#_direct_mode">25.7.2. Direct Mode</a></span></dt><dt><span class="section"><a href="#_null_string_handling">25.7.3. Null string handling</a></span></dt></dl></dd><dt><span class="section"><a href="#_data_connector_for_oracle_and_hadoop">25.8. Data Connector for Oracle and Hadoop</a></span></dt><dd><dl><dt><span class="section"><a href="#_about">25.8.1. About</a></span></dt><dd><dl><dt><span class="section"><a href="#_jobs">25.8.1.1. Jobs</a></span></dt><dt><span class="section"><a href="#_how_the_standard_oracle_manager_works_for_imports">25.8.1.2. How The Standard Oracle Manager Works for Imports</a></span></dt><dt><span class="section"><a href="#_how_the_data_connector_for_oracle_and_hadoop_works_for_imports">25.8.1.3. How The Data Connector for Oracle and Hadoop Works for Imports</a></span></dt><dt><span class="section"><a href="#_data_connector_for_oracle_and_hadoop_exports">25.8.1.4. Data Connector for Oracle and Hadoop Exports</a></span></dt></dl></dd><dt><span class="section"><a href="#_requirements_4">25.8.2. Requirements</a></span></dt><dd><dl><dt><span class="section"><a href="#_ensure_the_oracle_database_jdbc_driver_is_setup_correctly">25.8.2.1. Ensure The Oracle Database JDBC Driver Is Setup Correctly</a></span></dt><dt><span class="section"><a href="#_oracle_roles_and_privileges">25.8.2.2. Oracle Roles and Privileges</a></span></dt><dt><span class="section"><a href="#_additional_oracle_roles_and_privileges_required_for_export">25.8.2.3. Additional Oracle Roles And Privileges Required for Export</a></span></dt><dt><span class="section"><a href="#_supported_data_types">25.8.2.4. Supported Data Types</a></span></dt></dl></dd><dt><span class="section"><a href="#_execute_sqoop_with_data_connector_for_oracle_and_hadoop">25.8.3. Execute Sqoop With Data Connector for Oracle and Hadoop</a></span></dt><dd><dl><dt><span class="section"><a href="#_connect_to_oracle_oracle_rac">25.8.3.1. Connect to Oracle / Oracle RAC</a></span></dt><dt><span class="section"><a href="#_connect_to_an_oracle_database_instance">25.8.3.2. Connect to An Oracle Database Instance</a></span></dt><dt><span class="section"><a href="#_connect_to_an_oracle_rac">25.8.3.3. Connect to An Oracle RAC</a></span></dt><dt><span class="section"><a href="#_login_to_the_oracle_instance">25.8.3.4. Login to The Oracle Instance</a></span></dt><dt><span class="section"><a href="#_kill_data_connector_for_oracle_and_hadoop_jobs">25.8.3.5. Kill Data Connector for Oracle and Hadoop Jobs</a></span></dt></dl></dd><dt><span class="section"><a href="#_import_data_from_oracle">25.8.4. Import Data from Oracle</a></span></dt><dd><dl><dt><span class="section"><a href="#_match_hadoop_files_to_oracle_table_partitions">25.8.4.1. Match Hadoop Files to Oracle Table Partitions</a></span></dt><dt><span class="section"><a href="#_specify_the_partitions_to_import">25.8.4.2. Specify The Partitions To Import</a></span></dt><dt><span class="section"><a href="#_consistent_read_all_mappers_read_from_the_same_point_in_time">25.8.4.3. Consistent Read: All Mappers Read From The Same Point In Time</a></span></dt></dl></dd><dt><span class="section"><a href="#_export_data_into_oracle">25.8.5. Export Data into Oracle</a></span></dt><dd><dl><dt><span class="section"><a href="#_insert_export">25.8.5.1. Insert-Export</a></span></dt><dt><span class="section"><a href="#_update_export">25.8.5.2. Update-Export</a></span></dt><dt><span class="section"><a href="#_merge_export">25.8.5.3. Merge-Export</a></span></dt><dt><span class="section"><a href="#_create_oracle_tables">25.8.5.4. Create Oracle Tables</a></span></dt><dt><span class="section"><a href="#_nologging">25.8.5.5. NOLOGGING</a></span></dt><dt><span class="section"><a href="#_partitioning">25.8.5.6. Partitioning</a></span></dt><dt><span class="section"><a href="#_match_rows_via_multiple_columns">25.8.5.7. Match Rows Via Multiple Columns</a></span></dt><dt><span class="section"><a href="#_storage_clauses">25.8.5.8. Storage Clauses</a></span></dt></dl></dd><dt><span class="section"><a href="#_manage_date_and_timestamp_data_types">25.8.6. Manage Date And Timestamp Data Types</a></span></dt><dd><dl><dt><span class="section"><a href="#_import_date_and_timestamp_data_types_from_oracle">25.8.6.1. Import Date And Timestamp Data Types from Oracle</a></span></dt><dt><span class="section"><a href="#_the_data_connector_for_oracle_and_hadoop_does_not_apply_a_time_zone_to_date_timestamp_data_types">25.8.6.2. The Data Connector for Oracle and Hadoop Does Not Apply A Time Zone to DATE / TIMESTAMP Data Types</a></span></dt><dt><span class="section"><a href="#_the_data_connector_for_oracle_and_hadoop_retains_time_zone_information_in_timezone_data_types">25.8.6.3. The Data Connector for Oracle and Hadoop Retains Time Zone Information in TIMEZONE Data Types</a></span></dt><dt><span class="section"><a href="#_data_connector_for_oracle_and_hadoop_explicitly_states_time_zone_for_local_timezone_data_types">25.8.6.4. Data Connector for Oracle and Hadoop Explicitly States Time Zone for LOCAL TIMEZONE Data Types</a></span></dt><dt><span class="section"><a href="#_java_sql_timestamp">25.8.6.5. java.sql.Timestamp</a></span></dt><dt><span class="section"><a href="#_export_date_and_timestamp_data_types_into_oracle">25.8.6.6. Export Date And Timestamp Data Types into Oracle</a></span></dt></dl></dd><dt><span class="section"><a href="#_configure_the_data_connector_for_oracle_and_hadoop">25.8.7. Configure The Data Connector for Oracle and Hadoop</a></span></dt><dd><dl><dt><span class="section"><a href="#_oraoop_site_template_xml">25.8.7.1. oraoop-site-template.xml</a></span></dt><dt><span class="section"><a href="#_oraoop_oracle_session_initialization_statements">25.8.7.2. oraoop.oracle.session.initialization.statements</a></span></dt><dt><span class="section"><a href="#_oraoop_table_import_where_clause_location">25.8.7.3. oraoop.table.import.where.clause.location</a></span></dt><dt><span class="section"><a href="#_oracle_row_fetch_size">25.8.7.4. oracle.row.fetch.size</a></span></dt><dt><span class="section"><a href="#_oraoop_import_hint">25.8.7.5. oraoop.import.hint</a></span></dt><dt><span class="section"><a href="#_oraoop_oracle_append_values_hint_usage">25.8.7.6. oraoop.oracle.append.values.hint.usage</a></span></dt><dt><span class="section"><a href="#_mapred_map_tasks_speculative_execution">25.8.7.7. mapred.map.tasks.speculative.execution</a></span></dt><dt><span class="section"><a href="#_oraoop_block_allocation">25.8.7.8. oraoop.block.allocation</a></span></dt><dt><span class="section"><a href="#_oraoop_import_omit_lobs_and_long">25.8.7.9. oraoop.import.omit.lobs.and.long</a></span></dt><dt><span class="section"><a href="#_oraoop_locations">25.8.7.10. oraoop.locations</a></span></dt><dt><span class="section"><a href="#_sqoop_connection_factories">25.8.7.11. sqoop.connection.factories</a></span></dt><dt><span class="section"><a href="#_expressions_in_oraoop_site_xml">25.8.7.12. Expressions in oraoop-site.xml</a></span></dt></dl></dd><dt><span class="section"><a href="#_troubleshooting_the_data_connector_for_oracle_and_hadoop">25.8.8. Troubleshooting The Data Connector for Oracle and Hadoop</a></span></dt><dd><dl><dt><span class="section"><a href="#_quote_oracle_owners_and_tables">25.8.8.1. Quote Oracle Owners And Tables</a></span></dt><dt><span class="section"><a href="#_quote_oracle_columns">25.8.8.2. Quote Oracle Columns</a></span></dt><dt><span class="section"><a href="#_confirm_the_data_connector_for_oracle_and_hadoop_can_initialize_the_oracle_session">25.8.8.3. Confirm The Data Connector for Oracle and Hadoop Can Initialize The Oracle Session</a></span></dt><dt><span class="section"><a href="#_check_the_sqoop_debug_logs_for_error_messages">25.8.8.4. Check The Sqoop Debug Logs for Error Messages</a></span></dt><dt><span class="section"><a href="#_export_check_tables_are_compatible">25.8.8.5. Export: Check Tables Are Compatible</a></span></dt><dt><span class="section"><a href="#_export_parallelization">25.8.8.6. Export: Parallelization</a></span></dt><dt><span class="section"><a href="#_export_check_oraoop_oracle_append_values_hint_usage">25.8.8.7. Export: Check oraoop.oracle.append.values.hint.usage</a></span></dt><dt><span class="section"><a href="#_turn_on_verbose">25.8.8.8. Turn On Verbose</a></span></dt></dl></dd></dl></dd></dl></div><div class="section" title="25.1. MySQL JDBC Connector"><div class="titlepage"><div><div><h3 class="title"><a name="_mysql_jdbc_connector"></a>25.1. MySQL JDBC Connector</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_upsert_functionality">25.1.1. Upsert functionality</a></span></dt></dl></div><p>This section contains information specific to MySQL JDBC Connector.</p><div class="section" title="25.1.1. Upsert functionality"><div class="titlepage"><div><div><h4 class="title"><a name="_upsert_functionality"></a>25.1.1. Upsert functionality</h4></div></div></div><p>MySQL JDBC Connector is supporting upsert functionality using argument
<code class="literal">--update-mode allowinsert</code>. To achieve that Sqoop is using MySQL clause INSERT INTO
&#8230; ON DUPLICATE KEY UPDATE. This clause do not allow user to specify which columns
should be used to distinct whether we should update existing row or add new row. Instead
this clause relies on table&#8217;s unique keys (primary key belongs to this set). MySQL
will try to insert new row and if the insertion fails with duplicate unique key error
it will update appropriate row instead. As a result, Sqoop is ignoring values specified
in parameter <code class="literal">--update-key</code>, however user needs to specify at least one valid column
to turn on update mode itself.</p></div></div><div class="section" title="25.2. MySQL Direct Connector"><div class="titlepage"><div><div><h3 class="title"><a name="_mysql_direct_connector"></a>25.2. MySQL Direct Connector</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_requirements">25.2.1. Requirements</a></span></dt><dt><span class="section"><a href="#_limitations_2">25.2.2. Limitations</a></span></dt><dt><span class="section"><a href="#_direct_mode_transactions">25.2.3. Direct-mode Transactions</a></span></dt></dl></div><p>MySQL Direct Connector allows faster import and export to/from MySQL using <code class="literal">mysqldump</code> and <code class="literal">mysqlimport</code> tools functionality
instead of SQL selects and inserts.</p><p>To use the MySQL Direct Connector, specify the <code class="literal">--direct</code> argument for your import or export job.</p><p>Example:</p><pre class="screen">$ sqoop import --connect jdbc:mysql://db.foo.com/corp --table EMPLOYEES \
    --direct</pre><p>Passing additional parameters to mysqldump:</p><pre class="screen">$ sqoop import --connect jdbc:mysql://server.foo.com/db --table bar \
    --direct -- --default-character-set=latin1</pre><div class="section" title="25.2.1. Requirements"><div class="titlepage"><div><div><h4 class="title"><a name="_requirements"></a>25.2.1. Requirements</h4></div></div></div><p>Utilities <code class="literal">mysqldump</code> and <code class="literal">mysqlimport</code> should be present in the shell path of the user running the Sqoop command on
all nodes. To validate SSH as this user to all nodes and execute these commands. If you get an error, so will Sqoop.</p></div><div class="section" title="25.2.2. Limitations"><div class="titlepage"><div><div><h4 class="title"><a name="_limitations_2"></a>25.2.2. Limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Currently the direct connector does not support import of large object columns (BLOB and CLOB).
</li><li class="listitem">
Importing to HBase and Accumulo is not supported
</li><li class="listitem">
Use of a staging table when exporting data is not supported
</li><li class="listitem">
Import of views is not supported
</li></ul></div></div><div class="section" title="25.2.3. Direct-mode Transactions"><div class="titlepage"><div><div><h4 class="title"><a name="_direct_mode_transactions"></a>25.2.3. Direct-mode Transactions</h4></div></div></div><p>For performance, each writer will commit the current transaction
approximately every 32 MB of exported data. You can control this
by specifying the following argument <span class="emphasis"><em>before</em></span> any tool-specific arguments: <code class="literal">-D
sqoop.mysql.export.checkpoint.bytes=size</code>, where <span class="emphasis"><em>size</em></span> is a value in
bytes. Set <span class="emphasis"><em>size</em></span> to 0 to disable intermediate checkpoints,
but individual files being exported will continue to be committed
independently of one another.</p><p>Sometimes you need to export large data with Sqoop to a live MySQL cluster that
is under a high load serving random queries from the users of your application.
While data consistency issues during the export can be easily solved with a
staging table, there is still a problem with the performance impact caused by
the heavy export.</p><p>First off, the resources of MySQL dedicated to the import process can affect
the performance of the live product, both on the master and on the slaves.
Second, even if the servers can handle the import with no significant
performance impact (mysqlimport should be relatively "cheap"), importing big
tables can cause serious replication lag in the cluster risking data
inconsistency.</p><p>With <code class="literal">-D sqoop.mysql.export.sleep.ms=time</code>, where <span class="emphasis"><em>time</em></span> is a value in
milliseconds, you can let the server relax between checkpoints and the replicas
catch up by pausing the export process after transferring the number of bytes
specified in <code class="literal">sqoop.mysql.export.checkpoint.bytes</code>. Experiment with different
settings of these two parameters to archieve an export pace that doesn&#8217;t
endanger the stability of your MySQL cluster.</p><div class="important" title="Important" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Important"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Important]" src="images/important.png"></td><th align="left">Important</th></tr><tr><td align="left" valign="top"><p>Note that any arguments to Sqoop that are of the form <code class="literal">-D
parameter=value</code> are Hadoop <span class="emphasis"><em>generic arguments</em></span> and must appear before
any tool-specific arguments (for example, <code class="literal">--connect</code>, <code class="literal">--table</code>, etc).
Don&#8217;t forget that these parameters are only supported with the <code class="literal">--direct</code>
flag set.</p></td></tr></table></div></div></div><div class="section" title="25.3. Microsoft SQL Connector"><div class="titlepage"><div><div><h3 class="title"><a name="_microsoft_sql_connector"></a>25.3. Microsoft SQL Connector</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_extra_arguments">25.3.1. Extra arguments</a></span></dt><dt><span class="section"><a href="#_allow_identity_inserts">25.3.2. Allow identity inserts</a></span></dt><dt><span class="section"><a href="#_non_resilient_operations">25.3.3. Non-resilient operations</a></span></dt><dt><span class="section"><a href="#_schema_support">25.3.4. Schema support</a></span></dt><dt><span class="section"><a href="#_table_hints">25.3.5. Table hints</a></span></dt></dl></div><div class="section" title="25.3.1. Extra arguments"><div class="titlepage"><div><div><h4 class="title"><a name="_extra_arguments"></a>25.3.1. Extra arguments</h4></div></div></div><p>List of all extra arguments supported by Microsoft SQL Connector is shown below:</p><div class="table"><a name="idp4610904"></a><p class="title"><b>Table 50. Supported Microsoft SQL Connector extra arguments:</b></p><div class="table-contents"><table summary="Supported Microsoft SQL Connector extra arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    +--identity-insert
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Set IDENTITY_INSERT to ON before                                          export insert.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--non-resilient</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Don&#8217;t attempt to recover failed                                          export operations.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--schema &lt;name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Scheme name that sqoop should use.                                          Default is "dbo".
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--table-hints &lt;hints&gt;</code>
    </td><td style="" align="left">
    Table hints that Sqoop should use for                                          data movement.
    </td></tr></tbody></table></div></div><br class="table-break"></div><div class="section" title="25.3.2. Allow identity inserts"><div class="titlepage"><div><div><h4 class="title"><a name="_allow_identity_inserts"></a>25.3.2. Allow identity inserts</h4></div></div></div><p>You can allow inserts on columns that have identity. For example:</p><pre class="screen">$ sqoop export ... --export-dir custom_dir --table custom_table -- --identity-insert</pre></div><div class="section" title="25.3.3. Non-resilient operations"><div class="titlepage"><div><div><h4 class="title"><a name="_non_resilient_operations"></a>25.3.3. Non-resilient operations</h4></div></div></div><p>You can override the default and not use resilient operations during export.
This will avoid retrying failed operations. For example:</p><pre class="screen">$ sqoop export ... --export-dir custom_dir --table custom_table -- --non-resilient</pre></div><div class="section" title="25.3.4. Schema support"><div class="titlepage"><div><div><h4 class="title"><a name="_schema_support"></a>25.3.4. Schema support</h4></div></div></div><p>If you need to work with tables that are located in non-default schemas, you can
specify schema names via the <code class="literal">--schema</code> argument. Custom schemas are supported for
both import and export jobs. For example:</p><pre class="screen">$ sqoop import ... --table custom_table -- --schema custom_schema</pre></div><div class="section" title="25.3.5. Table hints"><div class="titlepage"><div><div><h4 class="title"><a name="_table_hints"></a>25.3.5. Table hints</h4></div></div></div><p>Sqoop supports table hints in both import and export jobs. Table hints are used only
for queries that move data from/to Microsoft SQL Server, but they cannot be used for
meta data queries. You can specify a comma-separated list of table hints in the
<code class="literal">--table-hints</code> argument. For example:</p><pre class="screen">$ sqoop import ... --table custom_table -- --table-hints NOLOCK</pre></div></div><div class="section" title="25.4. PostgreSQL Connector"><div class="titlepage"><div><div><h3 class="title"><a name="_postgresql_connector"></a>25.4. PostgreSQL Connector</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_extra_arguments_2">25.4.1. Extra arguments</a></span></dt><dt><span class="section"><a href="#_schema_support_2">25.4.2. Schema support</a></span></dt></dl></div><div class="section" title="25.4.1. Extra arguments"><div class="titlepage"><div><div><h4 class="title"><a name="_extra_arguments_2"></a>25.4.1. Extra arguments</h4></div></div></div><p>List of all extra arguments supported by PostgreSQL Connector is shown below:</p><div class="table"><a name="idp4625512"></a><p class="title"><b>Table 51. Supported PostgreSQL extra arguments:</b></p><div class="table-contents"><table summary="Supported PostgreSQL extra arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--schema &lt;name&gt;</code>
    </td><td style="" align="left">
    Scheme name that sqoop should use.                                          Default is "public".
    </td></tr></tbody></table></div></div><br class="table-break"></div><div class="section" title="25.4.2. Schema support"><div class="titlepage"><div><div><h4 class="title"><a name="_schema_support_2"></a>25.4.2. Schema support</h4></div></div></div><p>If you need to work with table that is located in schema other than default one,
you need to specify extra argument <code class="literal">--schema</code>. Custom schemas are supported for
both import and export job (optional staging table however must be present in the
same schema as target table). Example invocation:</p><pre class="screen">$ sqoop import ... --table custom_table -- --schema custom_schema</pre></div></div><div class="section" title="25.5. PostgreSQL Direct Connector"><div class="titlepage"><div><div><h3 class="title"><a name="_postgresql_direct_connector"></a>25.5. PostgreSQL Direct Connector</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_requirements_2">25.5.1. Requirements</a></span></dt><dt><span class="section"><a href="#_limitations_3">25.5.2. Limitations</a></span></dt></dl></div><p>PostgreSQL Direct Connector allows faster import and export to/from PostgresSQL "COPY" command.</p><p>To use the PostgreSQL Direct Connector, specify the <code class="literal">--direct</code> argument for your import or export job.</p><p>When importing from PostgreSQL in conjunction with direct mode, you
can split the import into separate files after
individual files reach a certain size. This size limit is controlled
with the <code class="literal">--direct-split-size</code> argument.</p><p>The direct connector offers also additional extra arguments:</p><div class="table"><a name="idp4634120"></a><p class="title"><b>Table 52. Additional supported PostgreSQL extra arguments in direct mode:</b></p><div class="table-contents"><table summary="Additional supported PostgreSQL extra arguments in direct mode:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--boolean-true-string &lt;str&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    String that will be used to encode                                          <code class="literal">true</code> value of <code class="literal">boolean</code> columns.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Default is "TRUE".
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--boolean-false-string &lt;str&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    String that will be used to encode                                          <code class="literal">false</code> value of <code class="literal">boolean</code> columns.
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    </td><td style="" align="left">
    Default is "FALSE".
    </td></tr></tbody></table></div></div><br class="table-break"><div class="section" title="25.5.1. Requirements"><div class="titlepage"><div><div><h4 class="title"><a name="_requirements_2"></a>25.5.1. Requirements</h4></div></div></div><p>Utility <code class="literal">psql</code> should be present in the shell path of the user running the Sqoop command on
all nodes. To validate SSH as this user to all nodes and execute these commands. If you get an error, so will Sqoop.</p></div><div class="section" title="25.5.2. Limitations"><div class="titlepage"><div><div><h4 class="title"><a name="_limitations_3"></a>25.5.2. Limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Currently the direct connector does not support import of large object columns (BLOB and CLOB).
</li><li class="listitem">
Importing to HBase and Accumulo is not supported
</li><li class="listitem">
Import of views is not supported
</li></ul></div></div></div><div class="section" title="25.6. pg_bulkload connector"><div class="titlepage"><div><div><h3 class="title"><a name="_pg_bulkload_connector"></a>25.6. pg_bulkload connector</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_purpose_16">25.6.1. Purpose</a></span></dt><dt><span class="section"><a href="#_requirements_3">25.6.2. Requirements</a></span></dt><dt><span class="section"><a href="#_syntax_16">25.6.3. Syntax</a></span></dt><dt><span class="section"><a href="#_data_staging">25.6.4. Data Staging</a></span></dt></dl></div><div class="section" title="25.6.1. Purpose"><div class="titlepage"><div><div><h4 class="title"><a name="_purpose_16"></a>25.6.1. Purpose</h4></div></div></div><p>pg_bulkload connector is a direct connector for exporting data into PostgreSQL.
This connector uses
<a class="ulink" href="http://pgbulkload.projects.postgresql.org/index.html" target="_top">pg_bulkload</a>.
Users benefit from functionality of pg_bulkload such as
fast exports bypassing shared bufferes and WAL,
flexible error records handling,
and ETL feature with filter functions.</p></div><div class="section" title="25.6.2. Requirements"><div class="titlepage"><div><div><h4 class="title"><a name="_requirements_3"></a>25.6.2. Requirements</h4></div></div></div><p>pg_bulkload connector requires following conditions for export job execution:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
The <a class="ulink" href="http://pgbulkload.projects.postgresql.org/index.html" target="_top">pg_bulkload</a>
  must be installed on DB server and all slave nodes.
  RPM for RedHat or CentOS is available in then
  <a class="ulink" href="http://pgfoundry.org/frs/?group_id=1000261" target="_top">download page</a>.
</li><li class="listitem">
The <a class="ulink" href="http://jdbc.postgresql.org/index.html" target="_top">PostgreSQL JDBC</a>
  is required on client node.
</li><li class="listitem">
Superuser role of PostgreSQL database is required for execution of pg_bulkload.
</li></ul></div></div><div class="section" title="25.6.3. Syntax"><div class="titlepage"><div><div><h4 class="title"><a name="_syntax_16"></a>25.6.3. Syntax</h4></div></div></div><p>Use <code class="literal">--connection-manager</code> option to specify connection manager classname.</p><pre class="screen">$ sqoop export (generic-args) --connection-manager org.apache.sqoop.manager.PGBulkloadManager (export-args)
$ sqoop-export (generic-args) --connection-manager org.apache.sqoop.manager.PGBulkloadManager (export-args)</pre><p>This connector supports export arguments shown below.</p><div class="table"><a name="idp4653216"></a><p class="title"><b>Table 53. Supported export control arguments:</b></p><div class="table-contents"><table summary="Supported export control arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--export-dir &lt;dir&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    HDFS source path for the export
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">-m,--num-mappers &lt;n&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Use <span class="emphasis"><em>n</em></span> map tasks to export in                                         parallel
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--table &lt;table-name&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Table to populate
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--input-null-string &lt;null-string&gt;</code>
    </td><td style="" align="left">
    The string to be interpreted as                                         null for string columns
    </td></tr></tbody></table></div></div><br class="table-break"><p>There are additional configuration for pg_bulkload execution
specified via Hadoop Configuration properties
which can be given with <code class="literal">-D &lt;property=value&gt;</code> option.
Because Hadoop Configuration properties are generic arguments of the sqoop,
it must preceed any export control arguments.</p><div class="table"><a name="idp4661640"></a><p class="title"><b>Table 54. Supported export control properties:</b></p><div class="table-contents"><table summary="Supported export control properties:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Property
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    mapred.reduce.tasks
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Number of reduce tasks for staging.                                The defalt value is 1.                                Each tasks do staging in a single transaction.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    pgbulkload.bin
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Path of the pg_bulkoad binary                                installed on each slave nodes.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    pgbulkload.check.constraints
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify whether CHECK constraints are checked                                during the loading.                                The default value is YES.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    pgbulkload.parse.errors
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    The maximum mumber of ingored records                                that cause errors during parsing,                                encoding, filtering, constraints checking,                                and data type conversion.                                Error records are recorded                                in the PARSE BADFILE.                                 The default value is INFINITE.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    pgbulkload.duplicate.errors
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Number of ingored records                                that violate unique constraints.                                Duplicated records are recorded in the                                DUPLICATE BADFILE on DB server.                                The default value is INFINITE.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    pgbulkload.filter
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Specify the filter function                                to convert each row in the input file.                                 See the pg_bulkload documentation to know                                how to write FILTER functions.
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    pgbulkload.clear.staging.table
    </td><td style="" align="left">
    Indicates that any data present in                               the staging table can be dropped.
    </td></tr></tbody></table></div></div><br class="table-break"><p>Here is a example of complete command line.</p><pre class="screen">$ sqoop export \
    -Dmapred.reduce.tasks=2
    -Dpgbulkload.bin="/usr/local/bin/pg_bulkload" \
    -Dpgbulkload.input.field.delim=$'\t' \
    -Dpgbulkload.check.constraints="YES" \
    -Dpgbulkload.parse.errors="INFINITE" \
    -Dpgbulkload.duplicate.errors="INFINITE" \
    --connect jdbc:postgresql://pgsql.example.net:5432/sqooptest \
    --connection-manager org.apache.sqoop.manager.PGBulkloadManager \
    --table test --username sqooptest --export-dir=/test -m 2</pre></div><div class="section" title="25.6.4. Data Staging"><div class="titlepage"><div><div><h4 class="title"><a name="_data_staging"></a>25.6.4. Data Staging</h4></div></div></div><p>Each map tasks of pg_bulkload connector&#8217;s export job create
their own staging table on the fly.
The Name of staging tables is decided based on the destination table
and the task attempt ids.
For example, the name of staging table for the "test" table is like
<code class="literal">test_attempt_1345021837431_0001_m_000000_0</code> .</p><p>Staging tables are automatically dropped if tasks successfully complete
or map tasks fail.
When reduce task fails,
staging table for the task are left for manual retry and
users must take care of it.</p></div></div><div class="section" title="25.7. Netezza Connector"><div class="titlepage"><div><div><h3 class="title"><a name="_netezza_connector"></a>25.7. Netezza Connector</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_extra_arguments_3">25.7.1. Extra arguments</a></span></dt><dt><span class="section"><a href="#_direct_mode">25.7.2. Direct Mode</a></span></dt><dt><span class="section"><a href="#_null_string_handling">25.7.3. Null string handling</a></span></dt></dl></div><div class="section" title="25.7.1. Extra arguments"><div class="titlepage"><div><div><h4 class="title"><a name="_extra_arguments_3"></a>25.7.1. Extra arguments</h4></div></div></div><p>List of all extra arguments supported by Netezza Connector is shown below:</p><div class="table"><a name="idp4677112"></a><p class="title"><b>Table 55. Supported Netezza extra arguments:</b></p><div class="table-contents"><table summary="Supported Netezza extra arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--partitioned-access</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Whether each mapper acts on a subset                                      of data slices of a table or all                                      Default is "false" for standard mode                                      and "true" for direct mode.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--max-errors</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Applicable only in direct mode.                                      This option specifies the error threshold                                      per mapper while transferring data. If                                      the number of errors encountered exceed                                      this threshold then the job will fail.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Default value is 1.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--log-dir</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Applicable only in direct mode.                                      Specifies the directory where Netezza                                      external table operation logs are stored                                      on the hadoop filesystem.  Logs are                                      stored under this directory with one                                      directory for the job and sub-directories                                      for each task number and attempt.                                      Default value is the user home directory.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--trunc-string</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Applicable only in direct mode.                                      Specifies whether the system                                       truncates strings to the declared                                      storage and loads the data. By default                                      truncation of strings is reported as an                                      error.
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--ctrl-chars</code>
    </td><td style="" align="left">
    Applicable only in direct mode.                                      Specifies whether control characters                                       (ASCII chars 1 - 31) can be allowed                                       to be part of char/nchar/varchar/nvarchar                                      columns.  Default is false.
    </td></tr></tbody></table></div></div><br class="table-break"></div><div class="section" title="25.7.2. Direct Mode"><div class="titlepage"><div><div><h4 class="title"><a name="_direct_mode"></a>25.7.2. Direct Mode</h4></div></div></div><p>Netezza connector supports an optimized data transfer facility using the
Netezza external tables feature.  Each map tasks of Netezza connector&#8217;s import
job will work on a subset of the Netezza partitions and transparently create
and use an external table to transport data.  Similarly, export jobs will use
the external table to push data fast onto the NZ system.   Direct mode does
not support staging tables, upsert options etc.</p><p>Here is an example of complete command line for import using the Netezza
external table feature.</p><pre class="screen">$ sqoop import \
    --direct \
    --connect jdbc:netezza://nzhost:5480/sqoop \
    --table nztable \
    --username nzuser \
    --password nzpass \
    --target-dir hdfsdir</pre><p>Here is an example of complete command line for export with tab as the field
terminator character.</p><pre class="screen">$ sqoop export \
    --direct \
    --connect jdbc:netezza://nzhost:5480/sqoop \
    --table nztable \
    --username nzuser \
    --password nzpass \
    --export-dir hdfsdir \
    --input-fields-terminated-by "\t"</pre></div><div class="section" title="25.7.3. Null string handling"><div class="titlepage"><div><div><h4 class="title"><a name="_null_string_handling"></a>25.7.3. Null string handling</h4></div></div></div><p>Netezza direct connector supports the null-string features of Sqoop.  The null
string values are converted to appropriate external table options during export
and import operations.</p><div class="table"><a name="idp4693352"></a><p class="title"><b>Table 56. Supported export control arguments:</b></p><div class="table-contents"><table summary="Supported export control arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--input-null-string &lt;null-string&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    The string to be interpreted as                                         null for string columns.
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--input-null-non-string &lt;null-string&gt;</code>
    </td><td style="" align="left">
    The string to be interpreted as                                         null for non string columns.
    </td></tr></tbody></table></div></div><br class="table-break"><p>In the case of Netezza direct mode connector, both the arguments must be
left to the default values or explicitly set to the same value.  Furthermore
the null string value is restricted to 0-4 utf8 characters.</p><p>On export, for non-string columns, if the chosen null value is a valid
representation in the column domain, then the column might not be loaded as
null.  For example, if the null string value is specified as "1", then on
export, any occurrence of "1" in the input file will be loaded as value 1
instead of NULL for int columns.</p><p>It is suggested that the null value be specified as empty string for
performance and consistency.</p><div class="table"><a name="idp4700520"></a><p class="title"><b>Table 57. Supported import control arguments:</b></p><div class="table-contents"><table summary="Supported import control arguments:" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Argument
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--null-string &lt;null-string&gt;</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    The string to be interpreted as                                         null for string columns.
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">--null-non-string &lt;null-string&gt;</code>
    </td><td style="" align="left">
    The string to be interpreted as                                         null for non string columns.
    </td></tr></tbody></table></div></div><br class="table-break"><p>In the case of Netezza direct mode connector, both the arguments must be
left to the default values or explicitly set to the same value.  Furthermore
the null string value is restricted to 0-4 utf8 characters.</p><p>On import, for non-string columns, the chosen null value in current
implementations the null value representation is ignored for non character
columns.  For example, if the null string value is specified as "\N", then on
import, any occurrence of NULL for non-char columns in the table will be
imported as an empty string instead of <span class="emphasis"><em>\N</em></span>, the chosen null string
representation.</p><p>It is suggested that the null value be specified as empty string for
performance and consistency.</p></div></div><div class="section" title="25.8. Data Connector for Oracle and Hadoop"><div class="titlepage"><div><div><h3 class="title"><a name="_data_connector_for_oracle_and_hadoop"></a>25.8. Data Connector for Oracle and Hadoop</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_about">25.8.1. About</a></span></dt><dd><dl><dt><span class="section"><a href="#_jobs">25.8.1.1. Jobs</a></span></dt><dt><span class="section"><a href="#_how_the_standard_oracle_manager_works_for_imports">25.8.1.2. How The Standard Oracle Manager Works for Imports</a></span></dt><dt><span class="section"><a href="#_how_the_data_connector_for_oracle_and_hadoop_works_for_imports">25.8.1.3. How The Data Connector for Oracle and Hadoop Works for Imports</a></span></dt><dt><span class="section"><a href="#_data_connector_for_oracle_and_hadoop_exports">25.8.1.4. Data Connector for Oracle and Hadoop Exports</a></span></dt></dl></dd><dt><span class="section"><a href="#_requirements_4">25.8.2. Requirements</a></span></dt><dd><dl><dt><span class="section"><a href="#_ensure_the_oracle_database_jdbc_driver_is_setup_correctly">25.8.2.1. Ensure The Oracle Database JDBC Driver Is Setup Correctly</a></span></dt><dt><span class="section"><a href="#_oracle_roles_and_privileges">25.8.2.2. Oracle Roles and Privileges</a></span></dt><dt><span class="section"><a href="#_additional_oracle_roles_and_privileges_required_for_export">25.8.2.3. Additional Oracle Roles And Privileges Required for Export</a></span></dt><dt><span class="section"><a href="#_supported_data_types">25.8.2.4. Supported Data Types</a></span></dt></dl></dd><dt><span class="section"><a href="#_execute_sqoop_with_data_connector_for_oracle_and_hadoop">25.8.3. Execute Sqoop With Data Connector for Oracle and Hadoop</a></span></dt><dd><dl><dt><span class="section"><a href="#_connect_to_oracle_oracle_rac">25.8.3.1. Connect to Oracle / Oracle RAC</a></span></dt><dt><span class="section"><a href="#_connect_to_an_oracle_database_instance">25.8.3.2. Connect to An Oracle Database Instance</a></span></dt><dt><span class="section"><a href="#_connect_to_an_oracle_rac">25.8.3.3. Connect to An Oracle RAC</a></span></dt><dt><span class="section"><a href="#_login_to_the_oracle_instance">25.8.3.4. Login to The Oracle Instance</a></span></dt><dt><span class="section"><a href="#_kill_data_connector_for_oracle_and_hadoop_jobs">25.8.3.5. Kill Data Connector for Oracle and Hadoop Jobs</a></span></dt></dl></dd><dt><span class="section"><a href="#_import_data_from_oracle">25.8.4. Import Data from Oracle</a></span></dt><dd><dl><dt><span class="section"><a href="#_match_hadoop_files_to_oracle_table_partitions">25.8.4.1. Match Hadoop Files to Oracle Table Partitions</a></span></dt><dt><span class="section"><a href="#_specify_the_partitions_to_import">25.8.4.2. Specify The Partitions To Import</a></span></dt><dt><span class="section"><a href="#_consistent_read_all_mappers_read_from_the_same_point_in_time">25.8.4.3. Consistent Read: All Mappers Read From The Same Point In Time</a></span></dt></dl></dd><dt><span class="section"><a href="#_export_data_into_oracle">25.8.5. Export Data into Oracle</a></span></dt><dd><dl><dt><span class="section"><a href="#_insert_export">25.8.5.1. Insert-Export</a></span></dt><dt><span class="section"><a href="#_update_export">25.8.5.2. Update-Export</a></span></dt><dt><span class="section"><a href="#_merge_export">25.8.5.3. Merge-Export</a></span></dt><dt><span class="section"><a href="#_create_oracle_tables">25.8.5.4. Create Oracle Tables</a></span></dt><dt><span class="section"><a href="#_nologging">25.8.5.5. NOLOGGING</a></span></dt><dt><span class="section"><a href="#_partitioning">25.8.5.6. Partitioning</a></span></dt><dt><span class="section"><a href="#_match_rows_via_multiple_columns">25.8.5.7. Match Rows Via Multiple Columns</a></span></dt><dt><span class="section"><a href="#_storage_clauses">25.8.5.8. Storage Clauses</a></span></dt></dl></dd><dt><span class="section"><a href="#_manage_date_and_timestamp_data_types">25.8.6. Manage Date And Timestamp Data Types</a></span></dt><dd><dl><dt><span class="section"><a href="#_import_date_and_timestamp_data_types_from_oracle">25.8.6.1. Import Date And Timestamp Data Types from Oracle</a></span></dt><dt><span class="section"><a href="#_the_data_connector_for_oracle_and_hadoop_does_not_apply_a_time_zone_to_date_timestamp_data_types">25.8.6.2. The Data Connector for Oracle and Hadoop Does Not Apply A Time Zone to DATE / TIMESTAMP Data Types</a></span></dt><dt><span class="section"><a href="#_the_data_connector_for_oracle_and_hadoop_retains_time_zone_information_in_timezone_data_types">25.8.6.3. The Data Connector for Oracle and Hadoop Retains Time Zone Information in TIMEZONE Data Types</a></span></dt><dt><span class="section"><a href="#_data_connector_for_oracle_and_hadoop_explicitly_states_time_zone_for_local_timezone_data_types">25.8.6.4. Data Connector for Oracle and Hadoop Explicitly States Time Zone for LOCAL TIMEZONE Data Types</a></span></dt><dt><span class="section"><a href="#_java_sql_timestamp">25.8.6.5. java.sql.Timestamp</a></span></dt><dt><span class="section"><a href="#_export_date_and_timestamp_data_types_into_oracle">25.8.6.6. Export Date And Timestamp Data Types into Oracle</a></span></dt></dl></dd><dt><span class="section"><a href="#_configure_the_data_connector_for_oracle_and_hadoop">25.8.7. Configure The Data Connector for Oracle and Hadoop</a></span></dt><dd><dl><dt><span class="section"><a href="#_oraoop_site_template_xml">25.8.7.1. oraoop-site-template.xml</a></span></dt><dt><span class="section"><a href="#_oraoop_oracle_session_initialization_statements">25.8.7.2. oraoop.oracle.session.initialization.statements</a></span></dt><dt><span class="section"><a href="#_oraoop_table_import_where_clause_location">25.8.7.3. oraoop.table.import.where.clause.location</a></span></dt><dt><span class="section"><a href="#_oracle_row_fetch_size">25.8.7.4. oracle.row.fetch.size</a></span></dt><dt><span class="section"><a href="#_oraoop_import_hint">25.8.7.5. oraoop.import.hint</a></span></dt><dt><span class="section"><a href="#_oraoop_oracle_append_values_hint_usage">25.8.7.6. oraoop.oracle.append.values.hint.usage</a></span></dt><dt><span class="section"><a href="#_mapred_map_tasks_speculative_execution">25.8.7.7. mapred.map.tasks.speculative.execution</a></span></dt><dt><span class="section"><a href="#_oraoop_block_allocation">25.8.7.8. oraoop.block.allocation</a></span></dt><dt><span class="section"><a href="#_oraoop_import_omit_lobs_and_long">25.8.7.9. oraoop.import.omit.lobs.and.long</a></span></dt><dt><span class="section"><a href="#_oraoop_locations">25.8.7.10. oraoop.locations</a></span></dt><dt><span class="section"><a href="#_sqoop_connection_factories">25.8.7.11. sqoop.connection.factories</a></span></dt><dt><span class="section"><a href="#_expressions_in_oraoop_site_xml">25.8.7.12. Expressions in oraoop-site.xml</a></span></dt></dl></dd><dt><span class="section"><a href="#_troubleshooting_the_data_connector_for_oracle_and_hadoop">25.8.8. Troubleshooting The Data Connector for Oracle and Hadoop</a></span></dt><dd><dl><dt><span class="section"><a href="#_quote_oracle_owners_and_tables">25.8.8.1. Quote Oracle Owners And Tables</a></span></dt><dt><span class="section"><a href="#_quote_oracle_columns">25.8.8.2. Quote Oracle Columns</a></span></dt><dt><span class="section"><a href="#_confirm_the_data_connector_for_oracle_and_hadoop_can_initialize_the_oracle_session">25.8.8.3. Confirm The Data Connector for Oracle and Hadoop Can Initialize The Oracle Session</a></span></dt><dt><span class="section"><a href="#_check_the_sqoop_debug_logs_for_error_messages">25.8.8.4. Check The Sqoop Debug Logs for Error Messages</a></span></dt><dt><span class="section"><a href="#_export_check_tables_are_compatible">25.8.8.5. Export: Check Tables Are Compatible</a></span></dt><dt><span class="section"><a href="#_export_parallelization">25.8.8.6. Export: Parallelization</a></span></dt><dt><span class="section"><a href="#_export_check_oraoop_oracle_append_values_hint_usage">25.8.8.7. Export: Check oraoop.oracle.append.values.hint.usage</a></span></dt><dt><span class="section"><a href="#_turn_on_verbose">25.8.8.8. Turn On Verbose</a></span></dt></dl></dd></dl></div><div class="section" title="25.8.1. About"><div class="titlepage"><div><div><h4 class="title"><a name="_about"></a>25.8.1. About</h4></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_jobs">25.8.1.1. Jobs</a></span></dt><dt><span class="section"><a href="#_how_the_standard_oracle_manager_works_for_imports">25.8.1.2. How The Standard Oracle Manager Works for Imports</a></span></dt><dt><span class="section"><a href="#_how_the_data_connector_for_oracle_and_hadoop_works_for_imports">25.8.1.3. How The Data Connector for Oracle and Hadoop Works for Imports</a></span></dt><dt><span class="section"><a href="#_data_connector_for_oracle_and_hadoop_exports">25.8.1.4. Data Connector for Oracle and Hadoop Exports</a></span></dt></dl></div><p>The Data Connector for Oracle and Hadoop is now included in Sqoop.</p><p>It can be enabled by specifying the <code class="literal">--direct</code> argument for your import or
export job.</p><div class="section" title="25.8.1.1. Jobs"><div class="titlepage"><div><div><h5 class="title"><a name="_jobs"></a>25.8.1.1. Jobs</h5></div></div></div><p>The Data Connector for Oracle and Hadoop inspects each Sqoop job and assumes
responsibility for the ones it can perform better than the Oracle manager built
into Sqoop.</p><p>Data Connector for Oracle and Hadoop accepts responsibility for the following
Sqoop Job types:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<span class="strong"><strong>Import</strong></span> jobs that are <span class="strong"><strong>Non-Incremental</strong></span>.
</li><li class="listitem">
<span class="strong"><strong>Export</strong></span> jobs
</li><li class="listitem">
Data Connector for Oracle and Hadoop does not accept responsibility for other
Sqoop job types. For example Data Connector for Oracle and Hadoop does not
accept <span class="strong"><strong>eval</strong></span> jobs etc.
</li></ul></div><p>Data Connector for Oracle and Hadoop accepts responsibility for those Sqoop Jobs
with the following attributes:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Oracle-related
</li><li class="listitem"><p class="simpara">
Table-Based - Jobs where the table argument is used and the specified object
is a table.
</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>Data Connector for Oracle and Hadoop does not process index-organized
tables unless the table is partitioned and <code class="literal">oraoop.chunk.method</code> is set
to <code class="literal">PARTITION</code></p></td></tr></table></div></li><li class="listitem">
There are at least 2 mappers &#8212; Jobs where the Sqoop command-line does not
include: <code class="literal">--num-mappers 1</code>
</li></ul></div></div><div class="section" title="25.8.1.2. How The Standard Oracle Manager Works for Imports"><div class="titlepage"><div><div><h5 class="title"><a name="_how_the_standard_oracle_manager_works_for_imports"></a>25.8.1.2. How The Standard Oracle Manager Works for Imports</h5></div></div></div><p>The Oracle manager built into Sqoop uses a range-based query for each mapper.
Each mapper executes a query of the form:</p><pre class="screen">SELECT * FROM sometable WHERE id &gt;= lo AND id &lt; hi</pre><p>The <span class="strong"><strong>lo</strong></span> and <span class="strong"><strong>hi</strong></span> values are based on the number of mappers and the minimum and
maximum values of the data in the column the table is being split by.</p><p>If no suitable index exists on the table then these queries result in full
table-scans within Oracle. Even with a suitable index, multiple mappers may
fetch data stored within the same Oracle blocks, resulting in redundant IO
calls.</p></div><div class="section" title="25.8.1.3. How The Data Connector for Oracle and Hadoop Works for Imports"><div class="titlepage"><div><div><h5 class="title"><a name="_how_the_data_connector_for_oracle_and_hadoop_works_for_imports"></a>25.8.1.3. How The Data Connector for Oracle and Hadoop Works for Imports</h5></div></div></div><p>The Data Connector for Oracle and Hadoop generates queries for the mappers of
the form:</p><pre class="screen">SELECT *
  FROM sometable
 WHERE rowid &gt;= dbms_rowid.rowid_create(1, 893, 1, 279, 0) AND
       rowid &lt;= dbms_rowid.rowid_create(1, 893, 1, 286, 32767)</pre><p>The Data Connector for Oracle and Hadoop queries ensure that:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
No two mappers read data from the same Oracle block. This minimizes
redundant IO.
</li><li class="listitem">
The table does not require indexes.
</li><li class="listitem">
The Sqoop command line does not need to specify a <code class="literal">--split-by</code> column.
</li></ul></div></div><div class="section" title="25.8.1.4. Data Connector for Oracle and Hadoop Exports"><div class="titlepage"><div><div><h5 class="title"><a name="_data_connector_for_oracle_and_hadoop_exports"></a>25.8.1.4. Data Connector for Oracle and Hadoop Exports</h5></div></div></div><p>Benefits of the Data Connector for Oracle and Hadoop:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<span class="strong"><strong>Merge-Export facility</strong></span> - Update Oracle tables by modifying changed rows AND
inserting rows from the HDFS file that did not previously exist in the Oracle
table. The Connector for Oracle and Hadoop&#8217;s Merge-Export is unique - there is
no Sqoop equivalent.
</li><li class="listitem">
<span class="strong"><strong>Lower impact on the Oracle database</strong></span> - Update the rows in the Oracle table
that have changed, not all rows in the Oracle table. This has performance
benefits and reduces the impact of the query on Oracle (for example, the Oracle
redo logs).
</li><li class="listitem">
<span class="strong"><strong>Improved performance</strong></span> - With partitioned tables, mappers utilize temporary
Oracle tables which allow parallel inserts and direct path writes.
</li></ul></div></div></div><div class="section" title="25.8.2. Requirements"><div class="titlepage"><div><div><h4 class="title"><a name="_requirements_4"></a>25.8.2. Requirements</h4></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_ensure_the_oracle_database_jdbc_driver_is_setup_correctly">25.8.2.1. Ensure The Oracle Database JDBC Driver Is Setup Correctly</a></span></dt><dt><span class="section"><a href="#_oracle_roles_and_privileges">25.8.2.2. Oracle Roles and Privileges</a></span></dt><dt><span class="section"><a href="#_additional_oracle_roles_and_privileges_required_for_export">25.8.2.3. Additional Oracle Roles And Privileges Required for Export</a></span></dt><dt><span class="section"><a href="#_supported_data_types">25.8.2.4. Supported Data Types</a></span></dt></dl></div><div class="section" title="25.8.2.1. Ensure The Oracle Database JDBC Driver Is Setup Correctly"><div class="titlepage"><div><div><h5 class="title"><a name="_ensure_the_oracle_database_jdbc_driver_is_setup_correctly"></a>25.8.2.1. Ensure The Oracle Database JDBC Driver Is Setup Correctly</h5></div></div></div><p>You may want to ensure the Oracle Database 11g Release 2 JDBC driver is setup
correctly on your system. This driver is required for Sqoop to work with Oracle.</p><p>The Oracle Database 11g Release 2 JDBC driver file is <code class="literal">ojdbc6.jar</code> (3.2Mb).</p><p>If this file is not on your system then download it from:
<a class="ulink" href="http://www.oracle.com/technetwork/database/features/jdbc/index-091264.html" target="_top">http://www.oracle.com/technetwork/database/features/jdbc/index-091264.html</a></p><p>This file should be put into the <code class="literal">$SQOOP_HOME/lib</code> directory.</p></div><div class="section" title="25.8.2.2. Oracle Roles and Privileges"><div class="titlepage"><div><div><h5 class="title"><a name="_oracle_roles_and_privileges"></a>25.8.2.2. Oracle Roles and Privileges</h5></div></div></div><p>The Oracle user for The Data Connector for Oracle and Hadoop requires the
following roles and privileges:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<code class="literal">create session</code>
</li></ul></div><p>In addition, the user must have the select any dictionary privilege or
select_catalog_role role or all of the following object privileges:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<code class="literal">select on v_$instance</code>
</li><li class="listitem">
<code class="literal">select on dba_tables</code>
</li><li class="listitem">
<code class="literal">select on dba_tab_columns</code>
</li><li class="listitem">
<code class="literal">select on dba_objects</code>
</li><li class="listitem">
<code class="literal">select on dba_extents</code>
</li><li class="listitem">
<code class="literal">select on dba_segments</code> &#8212; Required for Sqoop imports only
</li><li class="listitem">
<code class="literal">select on dba_constraints</code> &#8212; Required for Sqoop imports only
</li><li class="listitem">
<code class="literal">select on v_$database</code> &#8212; Required for Sqoop imports only
</li><li class="listitem">
<code class="literal">select on v_$parameter</code> &#8212; Required for Sqoop imports only
</li></ul></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>The user also requires the alter session privilege to make use of session
tracing functionality. See "oraoop.oracle.session.initialization.statements"
for more information.</p></td></tr></table></div></div><div class="section" title="25.8.2.3. Additional Oracle Roles And Privileges Required for Export"><div class="titlepage"><div><div><h5 class="title"><a name="_additional_oracle_roles_and_privileges_required_for_export"></a>25.8.2.3. Additional Oracle Roles And Privileges Required for Export</h5></div></div></div><p>The Oracle user for Data Connector for Oracle and Hadoop requires:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
Quota on the tablespace in which the Oracle export tables are located.
</p><p class="simpara">An example Oracle command to achieve this is</p><pre class="screen">alter user username quota unlimited on tablespace</pre></li><li class="listitem"><p class="simpara">
The following privileges:
</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Type of Export
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Privileges Required
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    All Export
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">create table</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">select on dba_tab_partitions</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">select on dba_tab_subpartitions</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">select on dba_indexes</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">select on dba_ind_columns</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Insert-Export with a template table into another schema
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">select any table</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">create any table</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">insert any table</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">alter any table</code> (partitioning)
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Insert-Export without a template table into another schema
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">select,insert on table</code> (no partitioning)
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">select,alter on table</code> (partitioning)
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Update-Export into another schema
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">select,update on table</code> (no partitioning)
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">select,delete,alter,insert on table</code> (partitioning)
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Merge-Export into another schema
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">select,insert,update on table</code> (no partitioning)
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    </td><td style="" align="left">
    <code class="literal">select,insert,delete,alter on table</code> (partitioning)
    </td></tr></tbody></table></div></li></ul></div></div><div class="section" title="25.8.2.4. Supported Data Types"><div class="titlepage"><div><div><h5 class="title"><a name="_supported_data_types"></a>25.8.2.4. Supported Data Types</h5></div></div></div><p>The following Oracle data types are supported by the Data Connector for
Oracle and Hadoop:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    BINARY_DOUBLE
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    NCLOB
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    BINARY_FLOAT
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    NUMBER
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    BLOB
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    NVARCHAR2
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    CHAR
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    RAW
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    CLOB
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    ROWID
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    DATE
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    TIMESTAMP
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    FLOAT
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    TIMESTAMP WITH TIME ZONE
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    INTERVAL DAY TO SECOND
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    TIMESTAMP WITH LOCAL TIME ZONE
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    INTERVAL YEAR TO MONTH
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    URITYPE
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    LONG
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    VARCHAR2
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    NCHAR
    </td><td style="" align="left">
    </td></tr></tbody></table></div><p>All other Oracle column types are NOT supported. Example Oracle column types NOT
supported by Data Connector for Oracle and Hadoop include:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    All of the ANY types
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    BFILE
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    All of the MEDIA types
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    LONG RAW
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    All of the SPATIAL types
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    MLSLABEL
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Any type referred to as UNDEFINED
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    UROWID
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    All custom (user-defined) URI types
    </td><td style="" align="left">
    XMLTYPE
    </td></tr></tbody></table></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>Data types RAW, LONG and LOB (BLOB, CLOB and NCLOB) are supported for
Data Connector for Oracle and Hadoop imports. They are not supported for Data
Connector for Oracle and Hadoop exports.</p></td></tr></table></div></div></div><div class="section" title="25.8.3. Execute Sqoop With Data Connector for Oracle and Hadoop"><div class="titlepage"><div><div><h4 class="title"><a name="_execute_sqoop_with_data_connector_for_oracle_and_hadoop"></a>25.8.3. Execute Sqoop With Data Connector for Oracle and Hadoop</h4></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_connect_to_oracle_oracle_rac">25.8.3.1. Connect to Oracle / Oracle RAC</a></span></dt><dt><span class="section"><a href="#_connect_to_an_oracle_database_instance">25.8.3.2. Connect to An Oracle Database Instance</a></span></dt><dt><span class="section"><a href="#_connect_to_an_oracle_rac">25.8.3.3. Connect to An Oracle RAC</a></span></dt><dt><span class="section"><a href="#_login_to_the_oracle_instance">25.8.3.4. Login to The Oracle Instance</a></span></dt><dt><span class="section"><a href="#_kill_data_connector_for_oracle_and_hadoop_jobs">25.8.3.5. Kill Data Connector for Oracle and Hadoop Jobs</a></span></dt></dl></div><div class="section" title="25.8.3.1. Connect to Oracle / Oracle RAC"><div class="titlepage"><div><div><h5 class="title"><a name="_connect_to_oracle_oracle_rac"></a>25.8.3.1. Connect to Oracle / Oracle RAC</h5></div></div></div><p>The Sqoop <code class="literal">--connect</code> parameter defines the Oracle instance or Oracle RAC to
connect to. It is required with all Sqoop import and export commands.</p><p>Data Connector for Oracle and Hadoop expects the associated connection string
to be of a specific format dependent on whether the Oracle SID, Service
or TNS name is defined.  The TNS name based URL scheme can be used to enable
authentication using Oracle wallets.</p><p><code class="literal">--connect jdbc:oracle:thin:@OracleServer:OraclePort:OracleSID</code></p><p><code class="literal">--connect jdbc:oracle:thin:@//OracleServer:OraclePort/OracleService</code></p><p><code class="literal">--connect jdbc:oracle:thin:@TNSName</code></p></div><div class="section" title="25.8.3.2. Connect to An Oracle Database Instance"><div class="titlepage"><div><div><h5 class="title"><a name="_connect_to_an_oracle_database_instance"></a>25.8.3.2. Connect to An Oracle Database Instance</h5></div></div></div><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Parameter / Component
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">jdbc:oracle:thin</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    The Data Connector for Oracle and                                              Hadoop requires the connection string                                              starts with jdbc:oracle.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    The Data Connector for Oracle and                                              Hadoop has been tested with the thin                                              driver however it should work equally                                              well with other drivers such as OCI.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">OracleServer</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    The host name of the Oracle server.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">OraclePort</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    The port to connect to the Oracle server.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">OracleSID</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    The Oracle instance.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">OracleService</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    The Oracle Service.
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">TNSName</code>
    </td><td style="" align="left">
    The TNS name for the entry describing                                              the connection to the Oracle server.
    </td></tr></tbody></table></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>The Hadoop mappers connect to the Oracle database using a dynamically
generated JDBC URL. This is designed to improve performance however it can be
disabled by specifying:</p><p><code class="literal">-D oraoop.jdbc.url.verbatim=true</code></p></td></tr></table></div></div><div class="section" title="25.8.3.3. Connect to An Oracle RAC"><div class="titlepage"><div><div><h5 class="title"><a name="_connect_to_an_oracle_rac"></a>25.8.3.3. Connect to An Oracle RAC</h5></div></div></div><p>Use the <code class="literal">--connect</code> parameter as above. The connection string should point to
one instance of the Oracle RAC. The listener of the host of this Oracle
instance will locate the other instances of the Oracle RAC.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>To improve performance, The Data Connector for Oracle and Hadoop
identifies the active instances of the Oracle RAC and connects each Hadoop
mapper to them in a roundrobin manner.</p></td></tr></table></div><p>If services are defined for this Oracle RAC then use the following parameter
to specify the service name:</p><p><code class="literal">-D oraoop.oracle.rac.service.name=ServiceName</code></p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Parameter / Component
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">OracleServer:OraclePort:OracleInstance</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    Name one instance of the Oracle RAC.                                                                      The Data Connector for Oracle and                                                                      Hadoop assumes the same port number for                                                                      all instances of the Oracle RAC.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    The listener of the host of this Oracle                                                                      instance is used to locate other                                                                      instances of the Oracle RAC. For more                                                                      information enter this command on the                                                                      host command line:
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">lsnrctl status</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">-D oraoop.oracle.rac.service.name=ServiceName</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    The service to connect to in the Oracle RAC.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    A connection is made to all instances                                                                      of the Oracle RAC associated with the                                                                      service given by <code class="literal">ServiceName</code>.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    If omitted, a connection is made to all                                                                      instances of the Oracle RAC.
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    The listener of the host of this Oracle                                                                      instance needs to know the <code class="literal">ServiceName</code>                                                                      and all instances of the Oracle RAC. For                                                                      more information enter this command on                                                                      the host command line:
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    </td><td style="" align="left">
    <code class="literal">lsnrctl status</code>
    </td></tr></tbody></table></div></div><div class="section" title="25.8.3.4. Login to The Oracle Instance"><div class="titlepage"><div><div><h5 class="title"><a name="_login_to_the_oracle_instance"></a>25.8.3.4. Login to The Oracle Instance</h5></div></div></div><p>Login to the Oracle instance on the Sqoop command line:</p><p><code class="literal">--connect jdbc:oracle:thin:@OracleServer:OraclePort:OracleInstance --username
UserName -P</code></p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Parameter / Component
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Description
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">--username UserName</code>
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    The username to login to the Oracle                                         instance (SID).
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    <code class="literal">-P</code>
    </td><td style="" align="left">
    You will be prompted for the password                                         to login to the Oracle instance.
    </td></tr></tbody></table></div></div><div class="section" title="25.8.3.5. Kill Data Connector for Oracle and Hadoop Jobs"><div class="titlepage"><div><div><h5 class="title"><a name="_kill_data_connector_for_oracle_and_hadoop_jobs"></a>25.8.3.5. Kill Data Connector for Oracle and Hadoop Jobs</h5></div></div></div><p>Use the Hadoop Job Tracker to kill the Sqoop job, just as you would kill any
other Map-Reduce job.</p><p>$ <code class="literal">hadoop job -kill jobid</code></p><p>To allow an Oracle DBA to kill a Data Connector for Oracle and Hadoop
job (via killing the sessions in Oracle) you need to prevent Map-Reduce from
re-attempting failed jobs. This is done via the following Sqoop
command-line switch:</p><p><code class="literal">-D mapred.map.max.attempts=1</code></p><p>This sends instructions similar to the following to the console:</p><pre class="screen">14/07/07 15:24:51 INFO oracle.OraOopManagerFactory:
Note: This Data Connector for Oracle and Hadoop job can be killed via Oracle
by executing the following statement:
  begin
    for row in (select sid,serial# from v$session where
                module='Data Connector for Oracle and Hadoop' and
                action='import 20140707152451EST') loop
      execute immediate 'alter system kill session ''' || row.sid ||
                        ',' || row.serial# || '''';
    end loop;
  end;</pre></div></div><div class="section" title="25.8.4. Import Data from Oracle"><div class="titlepage"><div><div><h4 class="title"><a name="_import_data_from_oracle"></a>25.8.4. Import Data from Oracle</h4></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_match_hadoop_files_to_oracle_table_partitions">25.8.4.1. Match Hadoop Files to Oracle Table Partitions</a></span></dt><dt><span class="section"><a href="#_specify_the_partitions_to_import">25.8.4.2. Specify The Partitions To Import</a></span></dt><dt><span class="section"><a href="#_consistent_read_all_mappers_read_from_the_same_point_in_time">25.8.4.3. Consistent Read: All Mappers Read From The Same Point In Time</a></span></dt></dl></div><p>Execute Sqoop. Following is an example command:</p><p>$ <code class="literal">sqoop import --direct --connect &#8230; --table OracleTableName</code></p><p>If The Data Connector for Oracle and Hadoop accepts the job then the following
text is output:</p><pre class="screen">**************************************************
*** Using Data Connector for Oracle and Hadoop ***
**************************************************</pre><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
More information is available on the <code class="literal">--connect</code> parameter. See "Connect to
Oracle / Oracle RAC" for more information.
</li><li class="listitem"><p class="simpara">
If Java runs out of memory the workaround is to specify each mapper&#8217;s
JVM memory allocation. Add the following parameter for example to allocate 4GB:
</p><p class="simpara"><code class="literal">-Dmapred.child.java.opts=-Xmx4000M</code></p></li><li class="listitem"><p class="simpara">
An Oracle optimizer hint is included in the SELECT statement by default.
See "oraoop.import.hint" for more information.
</p><p class="simpara">You can alter the hint on the command line as follows:</p><p class="simpara"><code class="literal">-Doraoop.import.hint="NO_INDEX(t)"</code></p><p class="simpara">You can turn off the hint on the command line as follows (notice the space
between the double quotes):</p><p class="simpara"><code class="literal">-Doraoop.import.hint=" "</code></p></li></ul></div></td></tr></table></div><div class="section" title="25.8.4.1. Match Hadoop Files to Oracle Table Partitions"><div class="titlepage"><div><div><h5 class="title"><a name="_match_hadoop_files_to_oracle_table_partitions"></a>25.8.4.1. Match Hadoop Files to Oracle Table Partitions</h5></div></div></div><p><code class="literal">-Doraoop.chunk.method={ROWID|PARTITION}</code></p><p>To import data from a partitioned table in such a way that the resulting HDFS
folder structure in Hadoop will match the table&#8217;s partitions, set the chunk
method to PARTITION. The alternative (default) chunk method is ROWID.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
For the number of Hadoop files to match the number of Oracle partitions,
set the number of mappers to be greater than or equal to the number of
partitions.
</li><li class="listitem">
If the table is not partitioned then value PARTITION will lead to an error.
</li></ul></div></td></tr></table></div></div><div class="section" title="25.8.4.2. Specify The Partitions To Import"><div class="titlepage"><div><div><h5 class="title"><a name="_specify_the_partitions_to_import"></a>25.8.4.2. Specify The Partitions To Import</h5></div></div></div><p><code class="literal">-Doraoop.import.partitions=PartitionA,PartitionB --table OracleTableName</code></p><p>Imports <code class="literal">PartitionA</code> and <code class="literal">PartitionB</code> of <code class="literal">OracleTableName</code>.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
You can enclose an individual partition name in double quotes to retain the
letter case or if the name has special characters.
</p><p class="simpara"><code class="literal">-Doraoop.import.partitions='"PartitionA",PartitionB' --table OracleTableName</code></p><p class="simpara">If the partition name is not double quoted then its name will be automatically
converted to upper case, PARTITIONB for above.</p><p class="simpara">When using double quotes the entire list of partition names must be enclosed in
single quotes.</p><p class="simpara">If the last partition name in the list is double quoted then there must be a
comma at the end of the list.</p><p class="simpara"><code class="literal">-Doraoop.import.partitions='"PartitionA","PartitionB",' --table
OracleTableName</code></p></li><li class="listitem">
Name each partition to be included. There is no facility to provide a range of
partition names.
</li><li class="listitem">
There is no facility to define sub partitions. The entire partition is
included/excluded as per the filter.
</li></ul></div></td></tr></table></div></div><div class="section" title="25.8.4.3. Consistent Read: All Mappers Read From The Same Point In Time"><div class="titlepage"><div><div><h5 class="title"><a name="_consistent_read_all_mappers_read_from_the_same_point_in_time"></a>25.8.4.3. Consistent Read: All Mappers Read From The Same Point In Time</h5></div></div></div><p><code class="literal">-Doraoop.import.consistent.read={true|false}</code></p><p>When set to <code class="literal">false</code> (by default) each mapper runs a select query. This will
return potentially inconsistent data if there are a lot of DML operations on
the table at the time of import.</p><p>Set to <code class="literal">true</code> to ensure all mappers read from the same point in time. The
System Change Number (SCN) is passed down to all mappers, which use the Oracle
Flashback Query to query the table as at that SCN.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Values <code class="literal">true</code> | <code class="literal">false</code> are case sensitive.
</li><li class="listitem"><p class="simpara">
By default the SCN is taken from V$database. You can specify the SCN in the
following command
</p><p class="simpara"><code class="literal">-Doraoop.import.consistent.read.scn=12345</code></p></li></ul></div></td></tr></table></div></div></div><div class="section" title="25.8.5. Export Data into Oracle"><div class="titlepage"><div><div><h4 class="title"><a name="_export_data_into_oracle"></a>25.8.5. Export Data into Oracle</h4></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_insert_export">25.8.5.1. Insert-Export</a></span></dt><dt><span class="section"><a href="#_update_export">25.8.5.2. Update-Export</a></span></dt><dt><span class="section"><a href="#_merge_export">25.8.5.3. Merge-Export</a></span></dt><dt><span class="section"><a href="#_create_oracle_tables">25.8.5.4. Create Oracle Tables</a></span></dt><dt><span class="section"><a href="#_nologging">25.8.5.5. NOLOGGING</a></span></dt><dt><span class="section"><a href="#_partitioning">25.8.5.6. Partitioning</a></span></dt><dt><span class="section"><a href="#_match_rows_via_multiple_columns">25.8.5.7. Match Rows Via Multiple Columns</a></span></dt><dt><span class="section"><a href="#_storage_clauses">25.8.5.8. Storage Clauses</a></span></dt></dl></div><p>Execute Sqoop. Following is an example command:</p><p>$ <code class="literal">sqoop export --direct --connect &#8230; --table OracleTableName --export-dir
/user/username/tablename</code></p><p>The Data Connector for Oracle and Hadoop accepts all jobs that export data to
Oracle. You can verify The Data Connector for Oracle and Hadoop is in use by
checking the following text is output:</p><pre class="screen">**************************************************
*** Using Data Connector for Oracle and Hadoop ***
**************************************************</pre><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<code class="literal">OracleTableName</code> is the Oracle table the data will export into.
</li><li class="listitem">
<code class="literal">OracleTableName</code> can be in a schema other than that for the connecting user.
Prefix the table name with the schema, for example <code class="literal">SchemaName.OracleTableName</code>.
</li><li class="listitem">
Hadoop tables are picked up from the <code class="literal">/user/username/tablename</code> directory.
</li><li class="listitem">
The export will fail if the Hadoop file contains any fields of a data type
not supported by The Data Connector for Oracle and Hadoop. See
"Supported Data Types" for more information.
</li><li class="listitem">
The export will fail if the column definitions in the Hadoop table do not
exactly match the column definitions in the Oracle table.
</li><li class="listitem">
The Data Connector for Oracle and Hadoop indicates if it finds temporary
tables that it created more than a day ago that still exist. Usually these
tables can be dropped. The only circumstance when these tables should not be
dropped is when an The Data Connector for Oracle and Hadoop job has been
running for more than 24 hours and is still running.
</li><li class="listitem">
More information is available on the <code class="literal">--connect</code> parameter. See
"Connect to Oracle / Oracle RAC" for more information.
</li></ul></div></td></tr></table></div><div class="section" title="25.8.5.1. Insert-Export"><div class="titlepage"><div><div><h5 class="title"><a name="_insert_export"></a>25.8.5.1. Insert-Export</h5></div></div></div><p>Appends data to <code class="literal">OracleTableName</code>. It does not modify existing data in
<code class="literal">OracleTableName</code>.</p><p>Insert-Export is the default method, executed in the absence of the
<code class="literal">--update-key parameter</code>. All rows in the HDFS file in
<code class="literal">/user/UserName/TableName</code> are inserted into <code class="literal">OracleTableName</code>. No
change is made to pre-existing data in <code class="literal">OracleTableName</code>.</p><p>$ <code class="literal">sqoop export --direct --connect &#8230; --table OracleTableName --export-dir
/user/username/tablename</code></p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
If <code class="literal">OracleTableName</code> was previously created by The Data Connector for Oracle
and Hadoop with partitions then this export will create a new partition for the
data being inserted.
</li><li class="listitem">
When creating <code class="literal">OracleTableName</code> specify a template. See
"Create Oracle Tables" for more information.
</li></ul></div></td></tr></table></div></div><div class="section" title="25.8.5.2. Update-Export"><div class="titlepage"><div><div><h5 class="title"><a name="_update_export"></a>25.8.5.2. Update-Export</h5></div></div></div><p><code class="literal">--update-key OBJECT</code></p><p>Updates existing rows in <code class="literal">OracleTableName</code>.</p><p>Rows in the HDFS file in <code class="literal">/user/UserName/TableName</code> are matched to rows in
<code class="literal">OracleTableName</code> by the <code class="literal">OBJECT</code> column. Rows that match are copied from the
HDFS file to the Oracle table. No action is taken on rows that do not match.</p><p>$ <code class="literal">sqoop export --direct --connect &#8230; --update-key OBJECT --table
OracleTableName --export-dir /user/username/tablename</code></p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
If <code class="literal">OracleTableName</code> was previously created by The Data Connector for Oracle
and Hadoop with partitions then this export will create a new partition for the
data being inserted. Updated rows will be moved to the new partition that was
created for the export.
</li><li class="listitem">
For performance reasons it is strongly recommended that where more than a few
rows are involved column <code class="literal">OBJECT</code> be an index column of <code class="literal">OracleTableName</code>.
</li><li class="listitem">
Ensure the column name defined with <code class="literal">--update-key OBJECT</code> is specified in the
correct letter case. Sqoop will show an error if the letter case is incorrect.
</li><li class="listitem">
It is possible to match rows via multiple columns. See "Match Rows Via
Multiple Columns" for more information.
</li></ul></div></td></tr></table></div></div><div class="section" title="25.8.5.3. Merge-Export"><div class="titlepage"><div><div><h5 class="title"><a name="_merge_export"></a>25.8.5.3. Merge-Export</h5></div></div></div><p><code class="literal">--update-key OBJECT -Doraoop.export.merge=true</code></p><p>Updates existing rows in <code class="literal">OracleTableName</code>. Copies across rows from the HDFS
file that do not exist within the Oracle table.</p><p>Rows in the HDFS file in <code class="literal">/user/UserName/TableName</code> are matched to rows in
<code class="literal">OracleTableName</code> by the <code class="literal">OBJECT</code> column. Rows that match are copied from the
HDFS file to the Oracle table. Rows in the HDFS file that do not exist in
<code class="literal">OracleTableName</code> are added to <code class="literal">OracleTableName</code>.</p><p>$ <code class="literal">sqoop export --direct --connect &#8230; --update-key OBJECT
-Doraoop.export.merge=true --table OracleTableName --export-dir
/user/username/tablename</code></p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Merge-Export is unique to The Data Connector for Oracle and Hadoop. It is
not a standard Sqoop feature.
</li><li class="listitem">
If <code class="literal">OracleTableName</code> was previously created by The Data Connector for Oracle
and Hadoop with partitions, then this export will create a new partition for
the data being inserted. Updated rows will be moved to the new partition that
was created for the export.
</li><li class="listitem">
For performance reasons it is strongly recommended that where more than a
few rows are involved column <code class="literal">OBJECT</code> be an index column of <code class="literal">OracleTableName</code>.
</li><li class="listitem">
Ensure the column name defined with <code class="literal">--update-key OBJECT</code> is specified in the
correct letter case. Sqoop will show an error if the letter case is incorrect.
</li><li class="listitem">
It is possible to match rows via multiple columns. See "Match Rows Via
Multiple Columns" for more information.
</li></ul></div></td></tr></table></div></div><div class="section" title="25.8.5.4. Create Oracle Tables"><div class="titlepage"><div><div><h5 class="title"><a name="_create_oracle_tables"></a>25.8.5.4. Create Oracle Tables</h5></div></div></div><p><code class="literal">-Doraoop.template.table=TemplateTableName</code></p><p>Creates <code class="literal">OracleTableName</code> by replicating the structure and data types of
<code class="literal">TemplateTableName</code>. <code class="literal">TemplateTableName</code> is a table that exists in Oracle prior
to executing the Sqoop command.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
The export will fail if the Hadoop file contains any fields of a data type
not supported by The Data Connector for Oracle and Hadoop. See "Supported
Data Types" for more information.
</li><li class="listitem">
The export will fail if the column definitions in the Hadoop table do not
exactly match the column definitions in the Oracle table.
</li><li class="listitem">
This parameter is specific to creating an Oracle table. The export will fail
if <code class="literal">OracleTableName</code> already exists in Oracle.
</li></ul></div></td></tr></table></div><p>Example command:</p><p>$ <code class="literal">sqoop export --direct --connect.. --table OracleTableName --export-dir
/user/username/tablename -Doraoop.template.table=TemplateTableName</code></p></div><div class="section" title="25.8.5.5. NOLOGGING"><div class="titlepage"><div><div><h5 class="title"><a name="_nologging"></a>25.8.5.5. NOLOGGING</h5></div></div></div><p><code class="literal">-Doraoop.nologging=true</code></p><p>Assigns the NOLOGGING option to <code class="literal">OracleTableName</code>.</p><p>NOLOGGING may enhance performance but you will be unable to backup the table.</p></div><div class="section" title="25.8.5.6. Partitioning"><div class="titlepage"><div><div><h5 class="title"><a name="_partitioning"></a>25.8.5.6. Partitioning</h5></div></div></div><p><code class="literal">-Doraoop.partitioned=true</code></p><p>Partitions the table with the following benefits:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
The speed of the export is improved by allowing each mapper to insert data
into a separate Oracle table using direct path writes. (An alter table exchange
subpartition SQL statement is subsequently executed to swap the data into the
export table.)
</li><li class="listitem">
You can selectively query or delete the data inserted by each Sqoop export
job. For example, you can delete old data by dropping old partitions from
the table.
</li></ul></div><p>The partition value is the SYSDATE of when Sqoop export job was performed.</p><p>The partitioned table created by The Data Connector for Oracle and Hadoop
includes the following columns that don&#8217;t exist in the template table:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<code class="literal">oraoop_export_sysdate</code> - This is the Oracle SYSDATE when the Sqoop export
job was performed. The created table will be partitioned by this column.
</li><li class="listitem">
<code class="literal">oraoop_mapper_id</code> - This is the id of the Hadoop mapper that was used to
process the rows from the HDFS file. Each partition is subpartitioned by this
column. This column exists merely to facilitate the exchange subpartition
mechanism that is performed by each mapper during the export process.
</li><li class="listitem">
<code class="literal">oraoop_mapper_row</code> - A unique row id within the mapper / partition.
</li></ul></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>If a unique row id is required for the table it can be formed by a
combination of oraoop_export_sysdate, oraoop_mapper_id and oraoop_mapper_row.</p></td></tr></table></div></div><div class="section" title="25.8.5.7. Match Rows Via Multiple Columns"><div class="titlepage"><div><div><h5 class="title"><a name="_match_rows_via_multiple_columns"></a>25.8.5.7. Match Rows Via Multiple Columns</h5></div></div></div><p><code class="literal">-Doraoop.update.key.extra.columns="ColumnA,ColumnB"</code></p><p>Used with Update-Export and Merge-Export to match on more than one column. The
first column to be matched on is <code class="literal">--update-key OBJECT</code>. To match on additional
columns, specify those columns on this parameter.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Letter case for the column names on this parameter is not important.
</li><li class="listitem">
All columns used for matching should be indexed. The first three items on the
index should be <code class="literal">ColumnA</code>, <code class="literal">ColumnB</code> and the column specified on
<code class="literal">--update-key</code> - but the order in which the columns are specified is not
important.
</li></ul></div></td></tr></table></div></div><div class="section" title="25.8.5.8. Storage Clauses"><div class="titlepage"><div><div><h5 class="title"><a name="_storage_clauses"></a>25.8.5.8. Storage Clauses</h5></div></div></div><p><code class="literal">-Doraoop.temporary.table.storage.clause="StorageClause"</code></p><p><code class="literal">-Doraoop.table.storage.clause="StorageClause"</code></p><p>Use to customize storage with Oracle clauses as in TABLESPACE or COMPRESS</p><p><code class="literal">-Doraoop.table.storage.clause</code> applies to the export table that is created
from <code class="literal">-Doraoop.template.table</code>. See "Create Oracle Tables" for more
information. <code class="literal">-Doraoop.temporary.table.storage.clause</code> applies to all other
working tables that are created during the export process and then dropped at
the end of the export job.</p></div></div><div class="section" title="25.8.6. Manage Date And Timestamp Data Types"><div class="titlepage"><div><div><h4 class="title"><a name="_manage_date_and_timestamp_data_types"></a>25.8.6. Manage Date And Timestamp Data Types</h4></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_import_date_and_timestamp_data_types_from_oracle">25.8.6.1. Import Date And Timestamp Data Types from Oracle</a></span></dt><dt><span class="section"><a href="#_the_data_connector_for_oracle_and_hadoop_does_not_apply_a_time_zone_to_date_timestamp_data_types">25.8.6.2. The Data Connector for Oracle and Hadoop Does Not Apply A Time Zone to DATE / TIMESTAMP Data Types</a></span></dt><dt><span class="section"><a href="#_the_data_connector_for_oracle_and_hadoop_retains_time_zone_information_in_timezone_data_types">25.8.6.3. The Data Connector for Oracle and Hadoop Retains Time Zone Information in TIMEZONE Data Types</a></span></dt><dt><span class="section"><a href="#_data_connector_for_oracle_and_hadoop_explicitly_states_time_zone_for_local_timezone_data_types">25.8.6.4. Data Connector for Oracle and Hadoop Explicitly States Time Zone for LOCAL TIMEZONE Data Types</a></span></dt><dt><span class="section"><a href="#_java_sql_timestamp">25.8.6.5. java.sql.Timestamp</a></span></dt><dt><span class="section"><a href="#_export_date_and_timestamp_data_types_into_oracle">25.8.6.6. Export Date And Timestamp Data Types into Oracle</a></span></dt></dl></div><div class="section" title="25.8.6.1. Import Date And Timestamp Data Types from Oracle"><div class="titlepage"><div><div><h5 class="title"><a name="_import_date_and_timestamp_data_types_from_oracle"></a>25.8.6.1. Import Date And Timestamp Data Types from Oracle</h5></div></div></div><p>This section lists known differences in the data obtained by performing an
Data Connector for Oracle and Hadoop import of an Oracle table versus a native
Sqoop import of the same table.</p></div><div class="section" title="25.8.6.2. The Data Connector for Oracle and Hadoop Does Not Apply A Time Zone to DATE / TIMESTAMP Data Types"><div class="titlepage"><div><div><h5 class="title"><a name="_the_data_connector_for_oracle_and_hadoop_does_not_apply_a_time_zone_to_date_timestamp_data_types"></a>25.8.6.2. The Data Connector for Oracle and Hadoop Does Not Apply A Time Zone to DATE / TIMESTAMP Data Types</h5></div></div></div><p>Data stored in a DATE or TIMESTAMP column of an Oracle table is not associated
with a time zone. Sqoop without the Data Connector for Oracle and Hadoop
inappropriately applies time zone information to this data.</p><p>Take for example the following timestamp in an Oracle DATE or TIMESTAMP column:
<code class="literal">2am on 3rd October, 2010</code>.</p><p>Request Sqoop without the Data Connector for Oracle and Hadoop import this data
using a system located in Melbourne Australia. The data is adjusted to Melbourne
Daylight Saving Time. The data is imported into Hadoop as:
<code class="literal">3am on 3rd October, 2010.</code></p><p>The Data Connector for Oracle and Hadoop does not apply time zone information to
these Oracle data-types. Even from a system located in Melbourne Australia, The
Data Connector for Oracle and Hadoop ensures the Oracle and Hadoop timestamps
match. The Data Connector for Oracle and Hadoop correctly imports this
timestamp as:
<code class="literal">2am on 3rd October, 2010</code>.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>In order for The Data Connector for Oracle and Hadoop to ensure data
accuracy, Oracle DATE and TIMESTAMP values must be represented by a String,
even when <code class="literal">--as-sequencefile</code> is used on the Sqoop command-line to produce a
binary file in Hadoop.</p></td></tr></table></div></div><div class="section" title="25.8.6.3. The Data Connector for Oracle and Hadoop Retains Time Zone Information in TIMEZONE Data Types"><div class="titlepage"><div><div><h5 class="title"><a name="_the_data_connector_for_oracle_and_hadoop_retains_time_zone_information_in_timezone_data_types"></a>25.8.6.3. The Data Connector for Oracle and Hadoop Retains Time Zone Information in TIMEZONE Data Types</h5></div></div></div><p>Data stored in a TIMESTAMP WITH TIME ZONE column of an Oracle table is
associated with a time zone. This data consists of two distinct parts: when the
event occurred and where the event occurred.</p><p>When Sqoop without The Data Connector for Oracle and Hadoop is used to import
data it converts the timestamp to the time zone of the system running Sqoop and
omits the component of the data that specifies where the event occurred.</p><p>Take for example the following timestamps (with time zone) in an Oracle
TIMESTAMP WITH TIME ZONE column:</p><pre class="screen">2:59:00 am on 4th April, 2010. Australia/Melbourne
2:59:00 am on 4th April, 2010. America/New York</pre><p>Request Sqoop without The Data Connector for Oracle and Hadoop import this data
using a system located in Melbourne Australia. From the data imported into
Hadoop we know when the events occurred, assuming we know the Sqoop command was
run from a system located in the Australia/Melbourne time zone, but we have lost
the information regarding where the event occurred.</p><pre class="screen">2010-04-04 02:59:00.0
2010-04-04 16:59:00.0</pre><p>Sqoop with The Data Connector for Oracle and Hadoop imports the example
timestamps as follows. The Data Connector for Oracle and Hadoop retains the
time zone portion of the data.</p><pre class="screen">2010-04-04 02:59:00.0 Australia/Melbourne
2010-04-04 02:59:00.0 America/New_York</pre></div><div class="section" title="25.8.6.4. Data Connector for Oracle and Hadoop Explicitly States Time Zone for LOCAL TIMEZONE Data Types"><div class="titlepage"><div><div><h5 class="title"><a name="_data_connector_for_oracle_and_hadoop_explicitly_states_time_zone_for_local_timezone_data_types"></a>25.8.6.4. Data Connector for Oracle and Hadoop Explicitly States Time Zone for LOCAL TIMEZONE Data Types</h5></div></div></div><p>Data stored in a TIMESTAMP WITH LOCAL TIME ZONE column of an Oracle table is
associated with a time zone. Multiple end-users in differing time zones
(locales) will each have that data expressed as a timestamp within their
respective locale.</p><p>When Sqoop without the Data Connector for Oracle and Hadoop is used to import
data it converts the timestamp to the time zone of the system running Sqoop and
omits the component of the data that specifies location.</p><p>Take for example the following two timestamps (with time zone) in an Oracle
TIMESTAMP WITH LOCAL TIME ZONE column:</p><pre class="screen">2:59:00 am on 4th April, 2010. Australia/Melbourne
2:59:00 am on 4th April, 2010. America/New York</pre><p>Request Sqoop without the Data Connector for Oracle and Hadoop import this data
using a system located in Melbourne Australia. The timestamps are imported
correctly but the local time zone has to be guessed. If multiple systems in
different locale were executing the Sqoop import it would be very difficult to
diagnose the cause of the data corruption.</p><pre class="screen">2010-04-04 02:59:00.0
2010-04-04 16:59:00.0</pre><p>Sqoop with the Data Connector for Oracle and Hadoop explicitly states the time
zone portion of the data imported into Hadoop. The local time zone is GMT by
default. You can set the local time zone with parameter:</p><p><code class="literal">-Doracle.sessionTimeZone=Australia/Melbourne</code></p><p>The Data Connector for Oracle and Hadoop would import these two timestamps as:</p><pre class="screen">2010-04-04 02:59:00.0 Australia/Melbourne
2010-04-04 16:59:00.0 Australia/Melbourne</pre></div><div class="section" title="25.8.6.5. java.sql.Timestamp"><div class="titlepage"><div><div><h5 class="title"><a name="_java_sql_timestamp"></a>25.8.6.5. java.sql.Timestamp</h5></div></div></div><p>To use Sqoop&#8217;s handling of date and timestamp data types when importing data
from Oracle use the following parameter:</p><p><code class="literal">-Doraoop.timestamp.string=false</code></p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>Sqoop&#8217;s handling of date and timestamp data types does not store the
timezone. However, some developers may prefer Sqoop&#8217;s handling as the Data
Connector for Oracle and Hadoop converts date and timestamp data types to
string. This may not work for some developers as the string will require
parsing later in the workflow.</p></td></tr></table></div></div><div class="section" title="25.8.6.6. Export Date And Timestamp Data Types into Oracle"><div class="titlepage"><div><div><h5 class="title"><a name="_export_date_and_timestamp_data_types_into_oracle"></a>25.8.6.6. Export Date And Timestamp Data Types into Oracle</h5></div></div></div><p>Ensure the data in the HDFS file fits the required format exactly before using
Sqoop to export the data into Oracle.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
The Sqoop export command will fail if the data is not in the required format.
</li><li class="listitem">
ff = Fractional second
</li><li class="listitem">
TZR = Time Zone Region
</li></ul></div></td></tr></table></div><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left"><col align="left"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    Oracle Data Type
    </th><th style="border-bottom: 0.5pt solid ; " align="left">
    Required Format of The Data in the HDFS File
    </th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    DATE
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">yyyy-mm-dd hh24:mi:ss</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    TIMESTAMP
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">yyyy-mm-dd hh24:mi:ss.ff</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left">
    TIMESTAMPTZ
    </td><td style="border-bottom: 0.5pt solid ; " align="left">
    <code class="literal">yyyy-mm-dd hh24:mi:ss.ff TZR</code>
    </td></tr><tr><td style="border-right: 0.5pt solid ; " align="left">
    TIMESTAMPLTZ
    </td><td style="" align="left">
    <code class="literal">yyyy-mm-dd hh24:mi:ss.ff TZR</code>
    </td></tr></tbody></table></div></div></div><div class="section" title="25.8.7. Configure The Data Connector for Oracle and Hadoop"><div class="titlepage"><div><div><h4 class="title"><a name="_configure_the_data_connector_for_oracle_and_hadoop"></a>25.8.7. Configure The Data Connector for Oracle and Hadoop</h4></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_oraoop_site_template_xml">25.8.7.1. oraoop-site-template.xml</a></span></dt><dt><span class="section"><a href="#_oraoop_oracle_session_initialization_statements">25.8.7.2. oraoop.oracle.session.initialization.statements</a></span></dt><dt><span class="section"><a href="#_oraoop_table_import_where_clause_location">25.8.7.3. oraoop.table.import.where.clause.location</a></span></dt><dt><span class="section"><a href="#_oracle_row_fetch_size">25.8.7.4. oracle.row.fetch.size</a></span></dt><dt><span class="section"><a href="#_oraoop_import_hint">25.8.7.5. oraoop.import.hint</a></span></dt><dt><span class="section"><a href="#_oraoop_oracle_append_values_hint_usage">25.8.7.6. oraoop.oracle.append.values.hint.usage</a></span></dt><dt><span class="section"><a href="#_mapred_map_tasks_speculative_execution">25.8.7.7. mapred.map.tasks.speculative.execution</a></span></dt><dt><span class="section"><a href="#_oraoop_block_allocation">25.8.7.8. oraoop.block.allocation</a></span></dt><dt><span class="section"><a href="#_oraoop_import_omit_lobs_and_long">25.8.7.9. oraoop.import.omit.lobs.and.long</a></span></dt><dt><span class="section"><a href="#_oraoop_locations">25.8.7.10. oraoop.locations</a></span></dt><dt><span class="section"><a href="#_sqoop_connection_factories">25.8.7.11. sqoop.connection.factories</a></span></dt><dt><span class="section"><a href="#_expressions_in_oraoop_site_xml">25.8.7.12. Expressions in oraoop-site.xml</a></span></dt></dl></div><div class="section" title="25.8.7.1. oraoop-site-template.xml"><div class="titlepage"><div><div><h5 class="title"><a name="_oraoop_site_template_xml"></a>25.8.7.1. oraoop-site-template.xml</h5></div></div></div><p>The oraoop-site-template.xml file is supplied with the Data Connector for
Oracle and Hadoop. It contains a number of ALTER SESSION statements that are
used to initialize the Oracle sessions created by the Data Connector for Oracle
and Hadoop.</p><p>If you need to customize these initializations to your environment then:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
Find <code class="literal">oraoop-site-template.xml</code> in the Sqoop configuration directory.
</li><li class="listitem">
Copy <code class="literal">oraoop-site-template.xml</code> to <code class="literal">oraoop-site.xml</code>.
</li><li class="listitem">
Edit the <code class="literal">ALTER SESSION</code> statements in <code class="literal">oraoop-site.xml</code>.
</li></ol></div></div><div class="section" title="25.8.7.2. oraoop.oracle.session.initialization.statements"><div class="titlepage"><div><div><h5 class="title"><a name="_oraoop_oracle_session_initialization_statements"></a>25.8.7.2. oraoop.oracle.session.initialization.statements</h5></div></div></div><p>The value of this property is a semicolon-delimited list of Oracle SQL
statements. These statements are executed, in order, for each Oracle session
created by the Data Connector for Oracle and Hadoop.</p><p>The default statements include:</p><div class="variablelist"><dl><dt><span class="term">
<code class="literal">alter session set time_zone = '{oracle.sessionTimeZone|GMT}';</code>
</span></dt><dd><p class="simpara">
This statement initializes the timezone of the JDBC client. This ensures that
data from columns of type TIMESTAMP WITH LOCAL TIMEZONE are correctly adjusted
into the timezone of the client and not kept in the timezone of the Oracle
database.
</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
There is an explanation to the text within the curly-braces. See
"Expressions in oraoop-site.xml" for more information..
</li><li class="listitem">
A list of the time zones supported by your Oracle database is available by
executing the following query: <code class="literal">SELECT TZNAME FROM V$TIMEZONE_NAMES;</code>
</li></ul></div></td></tr></table></div></dd><dt><span class="term">
<code class="literal">alter session disable parallel query;</code>
</span></dt><dd><p class="simpara">
This statement instructs Oracle to not parallelize SQL statements executed by
the Data Connector for Oracle and Hadoop sessions. This Oracle feature is
disabled because the Map/Reduce job launched by Sqoop is the mechanism used
for parallelization.
</p><p class="simpara">It is recommended that you not enable parallel query because it can have an
adverse effect the load on the Oracle instance and on the balance between
the Data Connector for Oracle and Hadoop mappers.</p><p class="simpara">Some export operations are performed in parallel where deemed appropriate by
the Data Connector for Oracle and Hadoop. See "Parallelization" for
more information.</p></dd><dt><span class="term">
<code class="literal">alter session set "_serial_direct_read"=true;</code>
</span></dt><dd>
This statement instructs Oracle to bypass the buffer cache. This is used to
prevent Oracle from filling its buffers with the data being read by the Data
Connector for Oracle and Hadoop, therefore diminishing its capacity to cache
higher prioritized data. Hence, this statement is intended to minimize the
Data Connector for Oracle and Hadoop&#8217;s impact on the immediate future
performance of the Oracle database.
</dd><dt><span class="term">
<code class="literal">--alter session set events '10046 trace name context forever, level 8';</code>
</span></dt><dd>
This statement has been commented-out. To allow tracing, remove the comment
token "--" from the start of the line.
</dd></dl></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
These statements are placed on separate lines for readability. They do not
need to be placed on separate lines.
</li><li class="listitem">
A statement can be commented-out via the standard Oracle double-hyphen
token: "--". The comment takes effect until the next semicolon.
</li></ul></div></td></tr></table></div></div><div class="section" title="25.8.7.3. oraoop.table.import.where.clause.location"><div class="titlepage"><div><div><h5 class="title"><a name="_oraoop_table_import_where_clause_location"></a>25.8.7.3. oraoop.table.import.where.clause.location</h5></div></div></div><div class="variablelist"><dl><dt><span class="term">
SUBSPLIT (default)
</span></dt><dd><p class="simpara">
When set to this value, the where clause is applied to each subquery used to
retrieve data from the Oracle table.
</p><p class="simpara">A Sqoop command like:</p><p class="simpara"><code class="literal">sqoop import -D oraoop.table.import.where.clause.location=SUBSPLIT --table
JUNK --where "owner like 'G%'"</code></p><p class="simpara">Generates SQL query of the form:</p><pre class="screen">SELECT OWNER,OBJECT_NAME
  FROM JUNK
 WHERE ((rowid &gt;=
           dbms_rowid.rowid_create(1, 113320, 1024, 4223664, 0)
         AND rowid &lt;=
             dbms_rowid.rowid_create(1, 113320, 1024, 4223671, 32767)))
   AND (owner like 'G%')
UNION ALL
SELECT OWNER,OBJECT_NAME
  FROM JUNK
 WHERE ((rowid &gt;=
           dbms_rowid.rowid_create(1, 113320, 1024, 4223672, 0)
         AND rowid &lt;=
           dbms_rowid.rowid_create(1, 113320, 1024, 4223679, 32767)))
   AND (owner like 'G%')</pre></dd><dt><span class="term">
SPLIT
</span></dt><dd><p class="simpara">
When set to this value, the where clause is applied to the entire SQL
statement used by each split/mapper.
</p><p class="simpara">A Sqoop command like:</p><p class="simpara"><code class="literal">sqoop import -D oraoop.table.import.where.clause.location=SPLIT --table
JUNK --where "rownum &#8656; 10"</code></p><p class="simpara">Generates SQL query of the form:</p><pre class="screen">SELECT OWNER,OBJECT_NAME
  FROM (
        SELECT OWNER,OBJECT_NAME
          FROM JUNK
         WHERE ((rowid &gt;=
                   dbms_rowid.rowid_create(1, 113320, 1024, 4223664, 0)
                 AND rowid &lt;=
                   dbms_rowid.rowid_create(1, 113320, 1024, 4223671, 32767)))
        UNION ALL
        SELECT OWNER,OBJECT_NAME
          FROM JUNK
         WHERE ((rowid &gt;=
                   dbms_rowid.rowid_create(1, 113320, 1024, 4223672, 0)
                 AND rowid &lt;=
                   dbms_rowid.rowid_create(1, 113320, 1024, 4223679,32767)))
       )
 WHERE rownum &lt;= 10</pre><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
In this example, there are up to 10 rows imported per mapper.
</li><li class="listitem">
The SPLIT clause may result in greater overhead than the SUBSPLIT
clause because the UNION statements need to be fully materialized
before the data can be streamed to the mappers. However, you may
wish to use SPLIT in the case where you want to limit the total
number of rows processed by each mapper.
</li></ul></div></td></tr></table></div></dd></dl></div></div><div class="section" title="25.8.7.4. oracle.row.fetch.size"><div class="titlepage"><div><div><h5 class="title"><a name="_oracle_row_fetch_size"></a>25.8.7.4. oracle.row.fetch.size</h5></div></div></div><p>The value of this property is an integer specifying the number of rows the
Oracle JDBC driver should fetch in each network round-trip to the database.
The default value is 5000.</p><p>If you alter this setting, confirmation of the
change is displayed in the logs of the mappers during the Map-Reduce job.</p></div><div class="section" title="25.8.7.5. oraoop.import.hint"><div class="titlepage"><div><div><h5 class="title"><a name="_oraoop_import_hint"></a>25.8.7.5. oraoop.import.hint</h5></div></div></div><p>The Oracle optimizer hint is added to the SELECT statement for IMPORT jobs
as follows:</p><pre class="screen">SELECT /*+ NO_INDEX(t) */ * FROM employees;</pre><p>The default hint is <code class="literal">NO_INDEX(t)</code></p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
The hint can be added to the command line. See "Import Data from Oracle" for
more information.
</li><li class="listitem">
See the Oracle Database Performance Tuning Guide (Using Optimizer Hints)
for more information on Oracle optimizer hints.
</li><li class="listitem"><p class="simpara">
To turn the hint off, insert a space between the &lt;value&gt; elements.
</p><pre class="screen">&lt;property&gt;
  &lt;name&gt;oraoop.import.hint&lt;/name&gt;
  &lt;value&gt; &lt;/value&gt;
&lt;/property&gt;</pre></li></ul></div></td></tr></table></div></div><div class="section" title="25.8.7.6. oraoop.oracle.append.values.hint.usage"><div class="titlepage"><div><div><h5 class="title"><a name="_oraoop_oracle_append_values_hint_usage"></a>25.8.7.6. oraoop.oracle.append.values.hint.usage</h5></div></div></div><p>The value of this property is one of: AUTO / ON / OFF.</p><div class="variablelist"><dl><dt><span class="term">
AUTO
</span></dt><dd><p class="simpara">
AUTO is the default value.
</p><p class="simpara">Currently AUTO is equivalent to OFF.</p></dd><dt><span class="term">
ON
</span></dt><dd>
During export the Data Connector for Oracle and Hadoop uses direct path
writes to populate the target Oracle table, bypassing the buffer cache.
Oracle only allows a single session to perform direct writes against a specific
table at any time, so this has the effect of serializing the writes to the
table. This may reduce throughput, especially if the number of mappers is high.
However, for databases where DBWR is very busy, or where the IO bandwidth to
the underlying table is narrow (table resides on a single disk spindle for
instance), then setting <code class="literal">oraoop.oracle.append.values.hint.usage</code> to ON may
reduce the load on the Oracle database and possibly increase throughput.
</dd><dt><span class="term">
OFF
</span></dt><dd>
During export the Data Connector for Oracle and Hadoop does not use the
<code class="literal">APPEND_VALUES</code> Oracle hint.
</dd></dl></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>This parameter is only effective on Oracle 11g Release 2 and above.</p></td></tr></table></div></div><div class="section" title="25.8.7.7. mapred.map.tasks.speculative.execution"><div class="titlepage"><div><div><h5 class="title"><a name="_mapred_map_tasks_speculative_execution"></a>25.8.7.7. mapred.map.tasks.speculative.execution</h5></div></div></div><p>By default speculative execution is disabled for the Data Connector for
Oracle and Hadoop. This avoids placing redundant load on the Oracle database.</p><p>If Speculative execution is enabled, then Hadoop may initiate multiple mappers
to read the same blocks of data, increasing the overall load on the database.</p></div><div class="section" title="25.8.7.8. oraoop.block.allocation"><div class="titlepage"><div><div><h5 class="title"><a name="_oraoop_block_allocation"></a>25.8.7.8. oraoop.block.allocation</h5></div></div></div><p>This setting determines how Oracle&#8217;s data-blocks are assigned to Map-Reduce mappers.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>Applicable to import. Not applicable to export.</p></td></tr></table></div><div class="variablelist"><dl><dt><span class="term">
ROUNDROBIN (default)
</span></dt><dd><p class="simpara">
Each chunk of Oracle blocks is allocated to the mappers in a roundrobin
manner. This helps prevent one of the mappers from being
allocated a large proportion of typically small-sized blocks from the
start of Oracle data-files. In doing so it also helps prevent one of the
other mappers from being allocated a large proportion of typically
larger-sized blocks from the end of the Oracle data-files.
</p><p class="simpara">Use this method to help ensure all the mappers are allocated a similar
amount of work.</p></dd><dt><span class="term">
RANDOM
</span></dt><dd>
The list of Oracle blocks is randomized before being allocated to the
mappers via a round-robin approach. This has the benefit of increasing
the chance that, at any given instant in time, each mapper is reading
from a different Oracle data-file. If the Oracle data-files are located on
separate spindles, this should increase the overall IO throughput.
</dd><dt><span class="term">
SEQUENTIAL
</span></dt><dd><p class="simpara">
Each chunk of Oracle blocks is allocated to the mappers sequentially.
This produces the tendency for each mapper to sequentially read a large,
contiguous proportion of an Oracle data-file. It is unlikely for the
performance of this method to exceed that of the round-robin method
and it is more likely to allocate a large difference in the work between
the mappers.
</p><p class="simpara">Use of this method is generally not recommended.</p></dd></dl></div></div><div class="section" title="25.8.7.9. oraoop.import.omit.lobs.and.long"><div class="titlepage"><div><div><h5 class="title"><a name="_oraoop_import_omit_lobs_and_long"></a>25.8.7.9. oraoop.import.omit.lobs.and.long</h5></div></div></div><p>This setting can be used to omit all LOB columns (BLOB, CLOB and NCLOB) and LONG
column from an Oracle table being imported. This is advantageous in
troubleshooting, as it provides a convenient way to exclude all LOB-based data
from the import.</p></div><div class="section" title="25.8.7.10. oraoop.locations"><div class="titlepage"><div><div><h5 class="title"><a name="_oraoop_locations"></a>25.8.7.10. oraoop.locations</h5></div></div></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>Applicable to import. Not applicable to export.</p></td></tr></table></div><p>By default, four mappers are used for a Sqoop import job. The number of mappers
can be altered via the Sqoop <code class="literal">--num-mappers</code> parameter.</p><p>If the data-nodes in your Hadoop cluster have 4 task-slots (that is they are
4-CPU core machines) it is likely for all four mappers to execute on the
same machine. Therefore, IO may be concentrated between the Oracle database
and a single machine.</p><p>This setting allows you to control which DataNodes in your Hadoop cluster each
mapper executes on. By assigning each mapper to a separate machine you may
improve the overall IO performance for the job. This will also have the
side-effect of the imported data being more diluted across the machines in
the cluster. (HDFS replication will dilute the data across the cluster anyway.)</p><p>Specify the machine names as a comma separated list. The locations are
allocated to each of the mappers in a round-robin manner.</p><p>If using EC2, specify the internal name of the machines. Here is an example
of using this parameter from the Sqoop command-line:</p><p>$ <code class="literal">sqoop import -D
oraoop.locations=ip-10-250-23-225.ec2.internal,ip-10-250-107-32.ec2.internal,ip-10-250-207-2.ec2.internal,ip-10-250-27-114.ec2.internal
--direct --connect&#8230;</code></p></div><div class="section" title="25.8.7.11. sqoop.connection.factories"><div class="titlepage"><div><div><h5 class="title"><a name="_sqoop_connection_factories"></a>25.8.7.11. sqoop.connection.factories</h5></div></div></div><p>This setting determines behavior if the Data Connector for Oracle and Hadoop
cannot accept the job. By default Sqoop accepts the jobs that the Data Connector
for Oracle and Hadoop rejects.</p><p>Set the value to <code class="literal">org.apache.sqoop.manager.oracle.OraOopManagerFactory</code> when you
want the job to fail if the Data Connector for Oracle and Hadoop cannot
accept the job.</p></div><div class="section" title="25.8.7.12. Expressions in oraoop-site.xml"><div class="titlepage"><div><div><h5 class="title"><a name="_expressions_in_oraoop_site_xml"></a>25.8.7.12. Expressions in oraoop-site.xml</h5></div></div></div><p>Text contained within curly-braces { and } are expressions to be evaluated
prior to the SQL statement being executed. The expression contains the name
of the configuration property optionally followed by a default value to use
if the property has not been set. A pipe | character is used to delimit the
property name and the default value.</p><p>For example:</p><div class="variablelist"><dl><dt><span class="term">
When this Sqoop command is executed
</span></dt><dd>
$ <code class="literal">sqoop import -D oracle.sessionTimeZone=US/Hawaii --direct --connect</code>
</dd><dt><span class="term">
The statement within oraoop-site.xml
</span></dt><dd>
<code class="literal">alter session set time_zone ='{oracle.sessionTimeZone|GMT}';</code>
</dd><dt><span class="term">
Becomes
</span></dt><dd>
<code class="literal">alter session set time_zone = 'US/Hawaii'</code>
</dd><dt><span class="term">
If the oracle.sessionTimeZone property had not been set, then this statement would use the specified default value and would become
</span></dt><dd>
<code class="literal">alter session set time_zone = 'GMT'</code>
</dd></dl></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>The <code class="literal">oracle.sessionTimeZone</code> property can be specified within the
<code class="literal">sqoop-site.xml</code> file if you want this setting to be used all the time.</p></td></tr></table></div></div></div><div class="section" title="25.8.8. Troubleshooting The Data Connector for Oracle and Hadoop"><div class="titlepage"><div><div><h4 class="title"><a name="_troubleshooting_the_data_connector_for_oracle_and_hadoop"></a>25.8.8. Troubleshooting The Data Connector for Oracle and Hadoop</h4></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_quote_oracle_owners_and_tables">25.8.8.1. Quote Oracle Owners And Tables</a></span></dt><dt><span class="section"><a href="#_quote_oracle_columns">25.8.8.2. Quote Oracle Columns</a></span></dt><dt><span class="section"><a href="#_confirm_the_data_connector_for_oracle_and_hadoop_can_initialize_the_oracle_session">25.8.8.3. Confirm The Data Connector for Oracle and Hadoop Can Initialize The Oracle Session</a></span></dt><dt><span class="section"><a href="#_check_the_sqoop_debug_logs_for_error_messages">25.8.8.4. Check The Sqoop Debug Logs for Error Messages</a></span></dt><dt><span class="section"><a href="#_export_check_tables_are_compatible">25.8.8.5. Export: Check Tables Are Compatible</a></span></dt><dt><span class="section"><a href="#_export_parallelization">25.8.8.6. Export: Parallelization</a></span></dt><dt><span class="section"><a href="#_export_check_oraoop_oracle_append_values_hint_usage">25.8.8.7. Export: Check oraoop.oracle.append.values.hint.usage</a></span></dt><dt><span class="section"><a href="#_turn_on_verbose">25.8.8.8. Turn On Verbose</a></span></dt></dl></div><div class="section" title="25.8.8.1. Quote Oracle Owners And Tables"><div class="titlepage"><div><div><h5 class="title"><a name="_quote_oracle_owners_and_tables"></a>25.8.8.1. Quote Oracle Owners And Tables</h5></div></div></div><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col><col></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><div class="literallayout"><p>If the owner of the Oracle table needs to be<br>
quoted, use:</p></div></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><div class="literallayout"><p>$ <code class="literal">sqoop import &#8230; --table<br>
"\"\"Scott\".customers\""</code><br>
<br>
This is the equivalent of:<br>
"Scott".customers</p></div></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><div class="literallayout"><p>If the Oracle table needs to be quoted, use:</p></div></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><div class="literallayout"><p>$ <code class="literal">sqoop import &#8230; --table<br>
"\"scott.\"Customers\"\""</code><br>
<br>
This is the equivalent of:<br>
scott."Customers"</p></div></td></tr><tr><td style="border-right: 0.5pt solid ; " align="left" valign="top"><div class="literallayout"><p>If both the owner of the Oracle table and the<br>
table itself needs to be quoted, use:</p></div></td><td style="" align="left" valign="top"><div class="literallayout"><p>$ <code class="literal">sqoop import &#8230; --table<br>
"\"\"Scott\".\"Customers\"\""</code><br>
<br>
This is the equivalent of:<br>
"Scott"."Customers"</p></div></td></tr></tbody></table></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
The HDFS output directory is called something like:
/user/username/"Scott"."Customers"
</li><li class="listitem">
If a table name contains a $ character, it may need to be escaped within your
Unix shell. For example, the dr$object table in the ctxsys schema would be
referred to as: $ <code class="literal">sqoop import &#8230; --table "ctxsys.dr\$object"</code>
</li></ul></div></td></tr></table></div></div><div class="section" title="25.8.8.2. Quote Oracle Columns"><div class="titlepage"><div><div><h5 class="title"><a name="_quote_oracle_columns"></a>25.8.8.2. Quote Oracle Columns</h5></div></div></div><div class="variablelist"><dl><dt><span class="term">
If a column name of an Oracle table needs to be quoted, use
</span></dt><dd><p class="simpara">
$ <code class="literal">sqoop import &#8230; --table customers --columns "\"\"first name\"\""</code>
</p><p class="simpara">This is the equivalent of: <code class="literal">select "first name" from customers</code></p></dd></dl></div></div><div class="section" title="25.8.8.3. Confirm The Data Connector for Oracle and Hadoop Can Initialize The Oracle Session"><div class="titlepage"><div><div><h5 class="title"><a name="_confirm_the_data_connector_for_oracle_and_hadoop_can_initialize_the_oracle_session"></a>25.8.8.3. Confirm The Data Connector for Oracle and Hadoop Can Initialize The Oracle Session</h5></div></div></div><p>If the Sqoop output includes feedback such as the following then the
configuration properties contained within <code class="literal">oraoop-site-template.xml</code> and
<code class="literal">oraoop-site.xml</code> have been loaded by Hadoop and can be accessed by the Data
Connector for Oracle and Hadoop.</p><p><code class="literal">14/07/08 15:21:13 INFO oracle.OracleConnectionFactory:
Initializing Oracle session with SQL</code></p></div><div class="section" title="25.8.8.4. Check The Sqoop Debug Logs for Error Messages"><div class="titlepage"><div><div><h5 class="title"><a name="_check_the_sqoop_debug_logs_for_error_messages"></a>25.8.8.4. Check The Sqoop Debug Logs for Error Messages</h5></div></div></div><p>For more information about any errors encountered during the Sqoop import,
refer to the log files generated by each of the (by default 4) mappers that
performed the import.</p><p>The logs can be obtained via your Map-Reduce Job Tracker&#8217;s web page.</p><p>Include these log files with any requests you make for assistance on the Sqoop
User Group web site.</p></div><div class="section" title="25.8.8.5. Export: Check Tables Are Compatible"><div class="titlepage"><div><div><h5 class="title"><a name="_export_check_tables_are_compatible"></a>25.8.8.5. Export: Check Tables Are Compatible</h5></div></div></div><p>Check tables particularly in the case of a parsing error.</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Ensure the fields contained with the HDFS file and the columns within the
Oracle table are identical. If they are not identical, the Java code
dynamically generated by Sqoop to parse the HDFS file will throw an error when
reading the file &#8211; causing the export to fail. When creating a table in Oracle
ensure the definitions for the table template are identical to the definitions
for the HDFS file.
</li><li class="listitem">
Ensure the data types in the table are supported. See "Supported Data Types"
for more information.
</li><li class="listitem">
Are date and time zone based data types used? See "Export Date And Timestamp
Data Types into Oracle" for more information.
</li></ul></div></div><div class="section" title="25.8.8.6. Export: Parallelization"><div class="titlepage"><div><div><h5 class="title"><a name="_export_parallelization"></a>25.8.8.6. Export: Parallelization</h5></div></div></div><p><code class="literal">-D oraoop.export.oracle.parallelization.enabled=false</code></p><p>If you see a parallelization error you may decide to disable parallelization
on Oracle queries.</p></div><div class="section" title="25.8.8.7. Export: Check oraoop.oracle.append.values.hint.usage"><div class="titlepage"><div><div><h5 class="title"><a name="_export_check_oraoop_oracle_append_values_hint_usage"></a>25.8.8.7. Export: Check oraoop.oracle.append.values.hint.usage</h5></div></div></div><p>The oraoop.oracle.append.values.hint.usage parameter should not be set to ON
if the Oracle table contains either a BINARY_DOUBLE or BINARY_FLOAT column and
the HDFS file being exported contains a NULL value in either of these column
types. Doing so will result in the error: <code class="literal">ORA-12838: cannot read/modify an
object after modifying it in parallel</code>.</p></div><div class="section" title="25.8.8.8. Turn On Verbose"><div class="titlepage"><div><div><h5 class="title"><a name="_turn_on_verbose"></a>25.8.8.8. Turn On Verbose</h5></div></div></div><p>Turn on verbose on the Sqoop command line.</p><p><code class="literal">--verbose</code></p><p>Check Sqoop stdout (standard output) and the mapper logs for information as to
where the problem may be.</p></div></div></div></div><div class="section" title="26. Getting Support"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_getting_support"></a>26. Getting Support</h2></div></div></div><p>Some general information is available at the
<a class="ulink" href="http://sqoop.apache.org/" target="_top">http://sqoop.apache.org/</a></p><p>Report bugs in Sqoop to the issue tracker at
<a class="ulink" href="https://issues.apache.org/jira/browse/SQOOP" target="_top">https://issues.apache.org/jira/browse/SQOOP</a>.</p><p>Questions and discussion regarding the usage of Sqoop should be directed to the
<a class="ulink" href="http://sqoop.apache.org/mail-lists.html" target="_top">sqoop-user mailing list</a>.</p><p>Before contacting either forum, run your Sqoop job with the
<code class="literal">--verbose</code> flag to acquire as much debugging information as
possible. Also report the string returned by <code class="literal">sqoop version</code> as
well as the version of Hadoop you are running (<code class="literal">hadoop version</code>).</p></div><div class="section" title="27. Troubleshooting"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_troubleshooting"></a>27. Troubleshooting</h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_general_troubleshooting_process">27.1. General Troubleshooting Process</a></span></dt><dt><span class="section"><a href="#_specific_troubleshooting_tips">27.2. Specific Troubleshooting Tips</a></span></dt><dd><dl><dt><span class="section"><a href="#_oracle_connection_reset_errors">27.2.1. Oracle: Connection Reset Errors</a></span></dt><dt><span class="section"><a href="#_oracle_case_sensitive_catalog_query_errors">27.2.2. Oracle: Case-Sensitive Catalog Query Errors</a></span></dt><dt><span class="section"><a href="#_mysql_connection_failure">27.2.3. MySQL: Connection Failure</a></span></dt><dt><span class="section"><a href="#_oracle_ora_00933_error_sql_command_not_properly_ended">27.2.4. Oracle: ORA-00933 error (SQL command not properly ended)</a></span></dt><dt><span class="section"><a href="#_mysql_import_of_tinyint_1_from_mysql_behaves_strangely">27.2.5. MySQL: Import of TINYINT(1) from MySQL behaves strangely</a></span></dt></dl></dd></dl></div><div class="section" title="27.1. General Troubleshooting Process"><div class="titlepage"><div><div><h3 class="title"><a name="_general_troubleshooting_process"></a>27.1. General Troubleshooting Process</h3></div></div></div><p>The following steps should be followed to troubleshoot any failure that you
encounter while running Sqoop.</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Turn on verbose output by executing the same command again and specifying
  the <code class="literal">--verbose</code> option. This produces more debug output on the console
  which can be inspected to identify any obvious errors.
</li><li class="listitem">
Look at the task logs from Hadoop to see if there are any specific failures
  recorded there. It is possible that the failure that occurs while task
  execution is not relayed correctly to the console.
</li><li class="listitem">
Make sure that the necessary input files or input/output tables are present
  and can be accessed by the user that Sqoop is executing as or connecting to
  the database as. It is possible that the necessary files or tables are present
  but the specific user that Sqoop connects as does not have the necessary
  permissions to access these files.
</li><li class="listitem">
If you are doing a compound action such as populating a Hive table or
  partition, try breaking the job into two separate actions to see where the
  problem really occurs. For example if an import that creates and populates a
  Hive table is failing, you can break it down into two steps - first for doing
  the import alone, and the second to create a Hive table without the import
  using the <code class="literal">create-hive-table</code> tool. While this does not address the original
  use-case of populating the Hive table, it does help narrow down the problem
  to either regular import or during the creation and population of Hive table.
</li><li class="listitem">
Search the mailing lists archives and JIRA for keywords relating to the
  problem. It is possible that you may find a solution discussed there that
  will help you solve or work-around your problem.
</li></ul></div></div><div class="section" title="27.2. Specific Troubleshooting Tips"><div class="titlepage"><div><div><h3 class="title"><a name="_specific_troubleshooting_tips"></a>27.2. Specific Troubleshooting Tips</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_oracle_connection_reset_errors">27.2.1. Oracle: Connection Reset Errors</a></span></dt><dt><span class="section"><a href="#_oracle_case_sensitive_catalog_query_errors">27.2.2. Oracle: Case-Sensitive Catalog Query Errors</a></span></dt><dt><span class="section"><a href="#_mysql_connection_failure">27.2.3. MySQL: Connection Failure</a></span></dt><dt><span class="section"><a href="#_oracle_ora_00933_error_sql_command_not_properly_ended">27.2.4. Oracle: ORA-00933 error (SQL command not properly ended)</a></span></dt><dt><span class="section"><a href="#_mysql_import_of_tinyint_1_from_mysql_behaves_strangely">27.2.5. MySQL: Import of TINYINT(1) from MySQL behaves strangely</a></span></dt></dl></div><div class="section" title="27.2.1. Oracle: Connection Reset Errors"><div class="titlepage"><div><div><h4 class="title"><a name="_oracle_connection_reset_errors"></a>27.2.1. Oracle: Connection Reset Errors</h4></div></div></div><p>Problem: When using the default Sqoop connector for Oracle, some data does
get transferred, but during the map-reduce job a lot of errors are reported
as below:</p><pre class="screen">11/05/26 16:23:47 INFO mapred.JobClient: Task Id : attempt_201105261333_0002_m_000002_0, Status : FAILED
java.lang.RuntimeException: java.lang.RuntimeException: java.sql.SQLRecoverableException: IO Error: Connection reset
at com.cloudera.sqoop.mapreduce.db.DBInputFormat.setConf(DBInputFormat.java:164)
at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62)
at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:605)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:322)
at org.apache.hadoop.mapred.Child$4.run(Child.java:268)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1115)
at org.apache.hadoop.mapred.Child.main(Child.java:262)
Caused by: java.lang.RuntimeException: java.sql.SQLRecoverableException: IO Error: Connection reset
at com.cloudera.sqoop.mapreduce.db.DBInputFormat.getConnection(DBInputFormat.java:190)
at com.cloudera.sqoop.mapreduce.db.DBInputFormat.setConf(DBInputFormat.java:159)
... 9 more
Caused by: java.sql.SQLRecoverableException: IO Error: Connection reset
at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:428)
at oracle.jdbc.driver.PhysicalConnection.&lt;init&gt;(PhysicalConnection.java:536)
at oracle.jdbc.driver.T4CConnection.&lt;init&gt;(T4CConnection.java:228)
at oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:32)
at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:521)
at java.sql.DriverManager.getConnection(DriverManager.java:582)
at java.sql.DriverManager.getConnection(DriverManager.java:185)
at com.cloudera.sqoop.mapreduce.db.DBConfiguration.getConnection(DBConfiguration.java:152)
at com.cloudera.sqoop.mapreduce.db.DBInputFormat.getConnection(DBInputFormat.java:184)
... 10 more
Caused by: java.net.SocketException: Connection reset
at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:96)
at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
at oracle.net.ns.DataPacket.send(DataPacket.java:199)
at oracle.net.ns.NetOutputStream.flush(NetOutputStream.java:211)
at oracle.net.ns.NetInputStream.getNextPacket(NetInputStream.java:227)
at oracle.net.ns.NetInputStream.read(NetInputStream.java:175)
at oracle.net.ns.NetInputStream.read(NetInputStream.java:100)
at oracle.net.ns.NetInputStream.read(NetInputStream.java:85)
at oracle.jdbc.driver.T4CSocketInputStreamWrapper.readNextPacket(T4CSocketInputStreamWrapper.java:123)
at oracle.jdbc.driver.T4CSocketInputStreamWrapper.read(T4CSocketInputStreamWrapper.java:79)
at oracle.jdbc.driver.T4CMAREngine.unmarshalUB1(T4CMAREngine.java:1122)
at oracle.jdbc.driver.T4CMAREngine.unmarshalSB1(T4CMAREngine.java:1099)
at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:288)
at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)
at oracle.jdbc.driver.T4CTTIoauthenticate.doOAUTH(T4CTTIoauthenticate.java:366)
at oracle.jdbc.driver.T4CTTIoauthenticate.doOAUTH(T4CTTIoauthenticate.java:752)
at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:366)
... 18 more</pre><p>Solution: This problem occurs primarily due to the lack of a fast random
number generation device on the host where the map tasks execute. On
typical Linux systems this can be addressed by setting the following
property in the <code class="literal">java.security</code> file:</p><pre class="screen">securerandom.source=file:/dev/../dev/urandom</pre><p>The <code class="literal">java.security</code> file can be found under <code class="literal">$JAVA_HOME/jre/lib/security</code>
directory. Alternatively, this property can also be specified on the
command line via:</p><pre class="screen">-D mapred.child.java.opts="-Djava.security.egd=file:/dev/../dev/urandom"</pre><p>Please note that it&#8217;s very important to specify this weird path <code class="literal">/dev/../dev/urandom</code>
as it is due to a Java bug
<a class="ulink" href="http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6202721" target="_top">6202721</a>,
or <code class="literal">/dev/urandom</code> will be ignored and substituted by <code class="literal">/dev/random</code>.</p></div><div class="section" title="27.2.2. Oracle: Case-Sensitive Catalog Query Errors"><div class="titlepage"><div><div><h4 class="title"><a name="_oracle_case_sensitive_catalog_query_errors"></a>27.2.2. Oracle: Case-Sensitive Catalog Query Errors</h4></div></div></div><p>Problem: While working with Oracle you may encounter problems when Sqoop can
not figure out column names. This happens because the catalog queries that
Sqoop uses for Oracle expect the correct case to be specified for the
user name and table name.</p><p>One example, using --hive-import and resulting in a NullPointerException:</p><pre class="screen">1/09/21 17:18:49 INFO manager.OracleManager: Time zone has been set to
GMT
11/09/21 17:18:49 DEBUG manager.SqlManager: Using fetchSize for next
query: 1000
11/09/21 17:18:49 INFO manager.SqlManager: Executing SQL statement:
SELECT t.* FROM addlabel_pris t WHERE 1=0
11/09/21 17:18:49 DEBUG manager.OracleManager$ConnCache: Caching
released connection for jdbc:oracle:thin:
11/09/21 17:18:49 ERROR sqoop.Sqoop: Got exception running Sqoop:
java.lang.NullPointerException
java.lang.NullPointerException
at com.cloudera.sqoop.hive.TableDefWriter.getCreateTableStmt(TableDefWriter.java:148)
at com.cloudera.sqoop.hive.HiveImport.importTable(HiveImport.java:187)
at com.cloudera.sqoop.tool.ImportTool.importTable(ImportTool.java:362)
at com.cloudera.sqoop.tool.ImportTool.run(ImportTool.java:423)
at com.cloudera.sqoop.Sqoop.run(Sqoop.java:144)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
at com.cloudera.sqoop.Sqoop.runSqoop(Sqoop.java:180)
at com.cloudera.sqoop.Sqoop.runTool(Sqoop.java:219)
at com.cloudera.sqoop.Sqoop.runTool(Sqoop.java:228)
at com.cloudera.sqoop.Sqoop.main(Sqoop.java:237)</pre><div class="orderedlist" title="Solution:"><p class="title"><b>Solution:</b></p><ol class="orderedlist" type="1"><li class="listitem">
Specify the user name, which Sqoop is connecting as, in upper case (unless
it was created with mixed/lower case within quotes).
</li><li class="listitem">
Specify the table name, which you are working with, in upper case (unless
it was created with mixed/lower case within quotes).
</li></ol></div></div><div class="section" title="27.2.3. MySQL: Connection Failure"><div class="titlepage"><div><div><h4 class="title"><a name="_mysql_connection_failure"></a>27.2.3. MySQL: Connection Failure</h4></div></div></div><p>Problem: While importing a MySQL table into Sqoop, if you do not have
the necessary permissions to access your MySQL database over the network,
you may get the below connection failure.</p><pre class="screen">Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</pre><p>Solution: First, verify that you can connect to the database from the node where
you are running Sqoop:</p><pre class="screen">$ mysql --host=&lt;IP Address&gt; --database=test --user=&lt;username&gt; --password=&lt;password&gt;</pre><p>If this works, it rules out any problem with the client network configuration
or security/authentication configuration.</p><p>Add the network port for the server to your my.cnf file <code class="literal">/etc/my.cnf</code>:</p><pre class="screen">[mysqld]
port = xxxx</pre><p>Set up a user account to connect via Sqoop.
Grant permissions to the user to access the database over the network:
(1.) Log into MySQL as root <code class="literal">mysql -u root -p&lt;ThisIsMyPassword&gt;</code>.
(2.) Issue the following command:</p><pre class="screen">mysql&gt; grant all privileges on test.* to 'testuser'@'%' identified by 'testpassword'</pre><p>Note that doing this will enable the testuser to connect to the
MySQL server from any IP address. While this will work, it is not
advisable for a production environment. We advise consulting with your
DBA to grant the necessary privileges based on the setup topology.</p><p>If the database server&#8217;s IP address changes, unless it is bound to
a static hostname in your server, the connect string passed into Sqoop
will also need to be changed.</p></div><div class="section" title="27.2.4. Oracle: ORA-00933 error (SQL command not properly ended)"><div class="titlepage"><div><div><h4 class="title"><a name="_oracle_ora_00933_error_sql_command_not_properly_ended"></a>27.2.4. Oracle: ORA-00933 error (SQL command not properly ended)</h4></div></div></div><p>Problem: While working with Oracle you may encounter the below problem
when the Sqoop command explicitly specifies the --driver
&lt;driver name&gt; option. When the driver option is included in
the Sqoop command, the built-in connection manager selection defaults to the
generic connection manager, which causes this issue with Oracle. If the
driver option is not specified, the built-in connection manager selection
mechanism selects the Oracle specific connection manager which generates
valid SQL for Oracle and uses the driver "oracle.jdbc.OracleDriver".</p><pre class="screen">ERROR manager.SqlManager: Error executing statement:
java.sql.SQLSyntaxErrorException: ORA-00933: SQL command not properly ended</pre><p>Solution: Omit the option --driver oracle.jdbc.driver.OracleDriver and then
re-run the Sqoop command.</p></div><div class="section" title="27.2.5. MySQL: Import of TINYINT(1) from MySQL behaves strangely"><div class="titlepage"><div><div><h4 class="title"><a name="_mysql_import_of_tinyint_1_from_mysql_behaves_strangely"></a>27.2.5. MySQL: Import of TINYINT(1) from MySQL behaves strangely</h4></div></div></div><p>Problem: Sqoop is treating TINYINT(1) columns as booleans, which is for example
causing issues with HIVE import. This is because by default the MySQL JDBC connector
maps the TINYINT(1) to java.sql.Types.BIT, which Sqoop by default maps to Boolean.</p><p>Solution: A more clean solution is to force MySQL JDBC Connector to stop
converting TINYINT(1) to java.sql.Types.BIT by adding <code class="literal">tinyInt1isBit=false</code> into your
JDBC path (to create something like <code class="literal">jdbc:mysql://localhost/test?tinyInt1isBit=false</code>).
Another solution would be to explicitly override the column mapping for the datatype
TINYINT(1) column. For example, if the column name is foo, then pass the following
option to Sqoop during import: --map-column-hive foo=tinyint. In the case of non-Hive
imports to HDFS, use --map-column-java foo=integer.</p></div></div></div></div><div class="footer-text"><span align="center"><a href="index.html"><img src="images/home.png" alt="Documentation Home"></a></span><br>
  This document was built from Sqoop source available at
  <a href="https://git-wip-us.apache.org/repos/asf?p=sqoop.git">https://git-wip-us.apache.org/repos/asf?p=sqoop.git</a>.
  </div></body></html>
